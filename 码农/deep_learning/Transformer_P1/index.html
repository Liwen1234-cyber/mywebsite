<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-码农/deep_learning/Transformer_P1" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Transformer_P1_Encoder | Coisini</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://doc.minddiy.top/码农/deep_learning/Transformer_P1/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Transformer_P1_Encoder | Coisini"><meta data-rh="true" name="description" content="变形金刚的英文就是Transformer,那Transformer也跟我们之后会,提到的BERT有非常强烈的关係,所以这边有一个BERT探出头来,代表说Transformer跟BERT,是很有关係的"><meta data-rh="true" property="og:description" content="变形金刚的英文就是Transformer,那Transformer也跟我们之后会,提到的BERT有非常强烈的关係,所以这边有一个BERT探出头来,代表说Transformer跟BERT,是很有关係的"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://doc.minddiy.top/码农/deep_learning/Transformer_P1/"><link data-rh="true" rel="alternate" href="https://doc.minddiy.top/码农/deep_learning/Transformer_P1/" hreflang="en"><link data-rh="true" rel="alternate" href="https://doc.minddiy.top/码农/deep_learning/Transformer_P1/" hreflang="x-default"><meta name="google-site-verification" content="1FUPX6Qo4y3ecU623ShEurhgnjhSTjK49rRMhEDlzFA">
<link rel="stylesheet" href="/katex/katex.min.css">
<script src="/js/matomo.js" async defer="defer"></script><link rel="stylesheet" href="/assets/css/styles.79037026.css">
<script src="/assets/js/runtime~main.468f2b27.js" defer="defer"></script>
<script src="/assets/js/main.4763ab3e.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Chialisp Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="Chialisp Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Coisini</b></a></div><div class="navbar__items navbar__items--right"><a href="https://minddiy.top" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Main site<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input" value=""><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><main class="docMainContainer_TBSr docMainContainerEnhanced_lQrH"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Transformer_P1_Encoder</h1></header>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429093519335-6ae36961b530ee117a863cc2a6c22a1f.png" width="622" height="470" class="img_ev3q"></p>
<p>变形金刚的英文就是Transformer,那Transformer也跟我们之后会,提到的BERT有非常强烈的关係,所以这边有一个BERT探出头来,代表说Transformer跟BERT,是很有关係的</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="sequence-to-sequence-seq2seq">Sequence-to-sequence (Seq2seq)<a href="#sequence-to-sequence-seq2seq" class="hash-link" aria-label="Direct link to Sequence-to-sequence (Seq2seq)" title="Direct link to Sequence-to-sequence (Seq2seq)">​</a></h2>
<p>Transformer就是一个,==Sequence-to-sequence==的model,他的缩写,我们会写做==Seq2seq==,那Sequence-to-sequence的model,又是什麼呢</p>
<p>我们之前在讲input a sequence的,case的时候,我们说input是一个sequence,那output有几种可能</p>
<ul>
<li>一种是input跟output的长度一样,这个是在作业二的时候做的</li>
<li>有一个case是output指,output一个东西,这个是在作业四的时候做的</li>
<li>那接来作业五的case是,我们不知道应该要output多长,由机器自己决定output的长度,即Seq2seq</li>
</ul>
<ol>
<li>
<p>举例来说,Seq2seq一个很好的应用就是 <strong>语音辨识</strong></p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429093940488-5714f91af3dce1869404057be2248694.png" width="518" height="114" class="img_ev3q"></p>
<p>在做语音辨识的时候,输入是声音讯号,声音讯号其实就是一串的vector,输出是语音辨识的结果,也就是输出的这段	声音讯号,所对应的文字</p>
<p>我们这边用圈圈来代表文字,每一个圈圈就代表,比如说中文裡面的一个方块子,今天<strong>输入跟输出的长度,当然是有一些关係,但是却没有绝对的关係</strong>，输入的声音讯号,他的长度是大T,我们并没有办法知道说,根据大T输出的这个长度N一定是多少。</p>
<p><strong>输出的长度由机器自己决定</strong>,由机器自己去听这段声音讯号的内容,自己决定他应该要输出几个文字,他输出的语音辨识结果,输出的句子裡面应该包含几个字,由机器自己来决定,这个是语音辨识</p>
</li>
<li>
<p>还有很多其他的例子,比如说作业五我们会做机器翻译</p>
<p><img decoding="async" loading="lazy" src="data:image/png;base64,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" width="524" height="110" class="img_ev3q"></p>
<p>让机器读一个语言的句子,输出另外一个语言的句子,那在做机器翻译的时候,<strong>输入的文字的长度是N,输出的句子的长度是N&#x27;,那N跟N&#x27;之间的关係,也要由机器自己来决定</strong></p>
<p>输入机器学习这个句子,输出是machine learning,输入是有四个字,输出有两个英文的词汇,但是并不是所有中文跟英文的关係,都是输出就是输入的二分之一,到底输入一段句子,输出英文的句子要多长,由机器自己决定</p>
</li>
<li>
<p>甚至可以做更复杂的问题,比如说做语音翻译</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429100418127-ba36299d85c1abccd241ea27b775a7e8.png" width="544" height="122" class="img_ev3q"></p>
<p>语音翻译就是,你对机器说一句话,比如说machine learning,他输出的不是英文,他直接<strong>把他听到的英文的声音讯号翻译成中文文字</strong></p>
<p>你对他说machine learning,他输出的是机器学习</p>
<p>為什麼我们要做,Speech Translation这样的任务,為什麼我们不直接先做一个语音辨识,再做一个机器翻译,把语音辨识系统跟机器翻译系统,接起来 就直接是语音翻译？</p>
<p>因為<strong>世界上有很多语言,他根本连文字都没有</strong>,世界上有超过七千种语言,那其实在这七千种语言,有超过半数其实是没有文字的,对这些没有文字的语言而言,你要做语音辨识,可能根本就没有办法,因為他没有文字,所以你根本就没有办法做语音辨识,但我们有没有可能对这些语言,做语音翻译,直接把它翻译成,我们有办法阅读的文字</p>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="hokkien闽南语台语">Hokkien（闽南语、台语）<a href="#hokkien闽南语台语" class="hash-link" aria-label="Direct link to Hokkien（闽南语、台语）" title="Direct link to Hokkien（闽南语、台语）">​</a></h3>
<p> 一个很好的例子也许就是,台语的语音辨识,但我不会说台语没有文字,很多人觉得台语是有文字的,但台语的文字并没有那麼普及,现在听说小学都有教台语的文字了,但台语的文字,并不是一般人能够看得懂的</p>
<p>如果你做语音辨识,你给机器一段台语,然后它可能输出是母汤,你根本就不知道,这段话在说什麼。</p>
<img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/unclestrong/DeepLearning_LHY21_Notes/master/Notes_pic/image-20210429102344921.png" alt="image-20210429102344921" style="zoom:67%" class="img_ev3q">
<p>所以我们期待说机器也许可以做语音的翻译,对它讲一句台语,它直接输出的是同样意思的,中文的句子,那这样一般人就可以看懂</p>
<img decoding="async" loading="lazy" src="https://github.com/unclestrong/DeepLearning_LHY21_Notes/blob/master/Notes_pic/image-20210429102425315.png?raw=true" alt="image-20210429102425315" style="zoom:67%" class="img_ev3q">
<p>我们可以训练一个类神经网路,这个类神经网路听某一种语言,的声音讯号,输出是另外一种语言的文字。</p>
<p>今天你要训练一个neural network,你就需要有input跟output的配合,你需要有台语的声音讯号,跟中文文字的对应关係,那这样的资料是比较容易收集的。比如说YouTube上面,有很多的乡土剧</p>
<img decoding="async" loading="lazy" src="https://github.com/unclestrong/DeepLearning_LHY21_Notes/blob/master/Notes_pic/image-20210429102635671.png?raw=true" alt="image-20210429102635671" style="zoom:67%" class="img_ev3q">
<p>乡土剧就是,台语语音 中文字幕,所以你只要它的台语语音载下来,中文字幕载下来,你就有台语声音讯号,跟中文之间的对应关係,你就可以硬train一个模型,然后叫机器直接做台语的语音辨识,输入台语 输出中文</p>
<p>那你可能会觉得这个想法很狂,而且好像听起来有很多很多的问题,那我们实验室就载了,一千五百个小时的乡土剧的资料,然后 就真的拿来训练一个,语音辨识系统</p>
<p>你可能会觉得说,这听起来有很多的问题</p>
<ul>
<li>乡土剧有很多杂讯,有很多的音乐,不要管它这样子</li>
<li>乡土剧的字幕,不一定跟声音有对起来,就不要管它这样子</li>
<li>台语还有一些,比如说台罗拼音,台语也是有类似音标这种东西,也许我们可以先辨识成音标,当作一个中介,然后在从音标转成中文,也没有这样做</li>
</ul>
<img decoding="async" loading="lazy" src="https://github.com/unclestrong/DeepLearning_LHY21_Notes/blob/master/Notes_pic/image-20210429103037724.png?raw=true" alt="image-20210429103037724" style="zoom:67%" class="img_ev3q">
<p>直接训练一个模型,输入是声音讯号,输出直接就是中文的文字,这种没有想太多 直接资料倒进去,就训练一个模型的行為,就叫作==硬train一发==</p>
<p>那你可能会想说,这样子硬train一发到底能不能够,做一个台语语音辨识系统呢,其实 还真的是有可能的,以下是一些真正的结果</p>
<p>机器在听的一千五百个小时的,乡土剧以后,你可以对它输入一句台语,然后他就输出一句中文的文字,以下是真正的例子</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429104632912-3051438b82715d751b1a8cdf57f0a558.png" width="597" height="429" class="img_ev3q"></p>
<p>机器听到的声音是这样子的</p>
<ul>
<li>你的身体撑不住(台语),那机器输出是什麼呢,它的输出是 你的身体撑不住,这个声音讯号是你的身体撑不住(台语),但机器并不是输出无勘,而是它就输出撑不住</li>
<li>或者是机器听到的,是这样的声音讯号,没事你為什麼要请假(台语),没事你為什麼要请假,机器听到没事(台语),它并不是输出 没代没誌,它是输出 没事,这样听到四个音节没代没誌(台语),但它知道说台语的没代没誌(台语),翻成中文 也许应该输出 没事,所以机器的输出是,没事你為什麼要请假</li>
<li>但机器其实也是蛮容易犯错的,底下特别找机个犯错的例子,给你听一下,你听听这一段声音讯号,不会腻吗(台语),他说不会腻吗(台语),我自己听到的时候我觉得,我跟机器的答案是一样的,就是说<strong>要生了吗</strong>,但其实这句话,正确的答案就是,不会腻吗(台语),不会腻吗</li>
<li>当然机器在倒装,你知道有时候你从台语,转成中文句子需要倒装,在倒装的部分感觉就没有太学起来,举例来说它听到这样的句子,我有跟厂长拜託(台语),他说我有跟厂长拜託(台语),那机器的输出是,我有帮厂长拜託,但是你知道说这句话,其实是倒装,我有跟厂长拜託(台语),是我拜託厂长,但机器对於它来说,如果台语跟中文的关係需要倒装的话,看起来学习起来还是有一点困难</li>
</ul>
<p>这个例子想要告诉你说,直接台语声音讯号转繁体中文,不是没有可能,是有可能可以做得到的,那其实台湾有很多人都在做,台语的语音辨识,如果你想要知道更多有关,台语语音辨识的事情的话,可以看一下下面这个<a href="https://sites.google.com/speech.ntut.edu.tw/fsw/home/challenge-2020" target="_blank" rel="noopener noreferrer">网站</a></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="text-to-speech-tts-synthesis">Text-to-Speech (TTS) Synthesis<a href="#text-to-speech-tts-synthesis" class="hash-link" aria-label="Direct link to Text-to-Speech (TTS) Synthesis" title="Direct link to Text-to-Speech (TTS) Synthesis">​</a></h4>
<p>台语语音辨识反过来,就是台语的<strong>语音合成</strong>,我们如果是一个模型,输入台语声音 输出中文的文字,那就是语音辨识,反过来 输入文字 输出声音讯号,就是语音合成</p>
<p>这边就是demo一下台语的语音合成,这个资料用的是,台湾 媠声(台语)的资料,来找GOOGLE台湾媠声(台语),就可以找到这个资料集,裡面就是台语的声音讯号,听起来像是这个样子</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429105644179-90082a350515e74c29669eb74e5c132e.png" width="566" height="414" class="img_ev3q"></p>
<p>比如说你跟它说,欢迎来到台湾台大语音处理实验室</p>
<p>不过这边是需要跟大家说明一下,现在还没有真的做End to End的模型,<strong>这边模型还是分成两阶</strong>,他会先把中文的文字,转成台语的台罗拼音,就像是台语的KK音标,在把台语的KK音标转成声音讯号,不过从台语的KK音标,转成声音讯号这一段,就是一个像是Transformer的network,其实是一个叫做echotron的model,它本质上就是一个Seq2Seq model,大概长的是这个样子</p>
<p>所以你输入文字,欢迎来到台大语音处理实验室,机器的输出是这个样子的,欢迎来到台大(台语),语音处理实验室(台语),或是你对他说这一句中文,然后他输出的台语是这个样子,最近肺炎真严重(台语),要记得戴口罩 勤洗手(台语),有病就要看医生(台语)</p>
<p>所以你真的是可以,合出台语的声音讯号的,就用我们在这一门课裡面学到的,Transformer或者是Seq2Seq的model</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="seq2seq-for-chatbot">Seq2seq for Chatbot<a href="#seq2seq-for-chatbot" class="hash-link" aria-label="Direct link to Seq2seq for Chatbot" title="Direct link to Seq2seq for Chatbot">​</a></h3>
<p>刚才讲的是跟语音比较有关的,那在<strong>文字</strong>上,也会很广泛的使用了Seq2Seq model</p>
<p>举例来说你可以用Seq2Seq model,来训练一个聊天机器人</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429110313820-59982441c575123ae302e7f1b5f751c4.png" width="583" height="336" class="img_ev3q"></p>
<p>聊天机器人就是你对它说一句话,它要给你一个回应,<strong>输入输出都是文字</strong>,文字就是一个vector Sequence,所以你完全可以用Seq2Seq 的model,来做一个聊天机器人</p>
<p>你就要<strong>收集大量人的对话</strong>,像这种对话你可以收集,电视剧 电影的台词 等等,你可以收集到,一堆人跟人之间的对话</p>
<p>假设在对话裡面有出现,某一个人说Hi,和另外一个人说,Hello How are you today,那你就可以教机器说,看到输入是Hi,那你的输出就要跟,Hello how are you today,越接近越好</p>
<p>那就可以训练一个Seq2Seq model,那跟它说一句话,它就会给你一个回应</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="question-answering-qa">Question Answering (QA)<a href="#question-answering-qa" class="hash-link" aria-label="Direct link to Question Answering (QA)" title="Direct link to Question Answering (QA)">​</a></h3>
<p>那事实上Seq2Seq model,在NLP的领域,在natural language processing的领域的使用,是比你想像的更為广泛,其实很多<strong>natural language processing的任务,都可以想成是==question answering,QA==的任务</strong></p>
<p>Question Answering,就是给机器读一段文字,然后你问机器一个问题,希望他可以给你一个正确的答案</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429163745845-11d38190a305d779f4c505cf400d3ebe.png" width="602" height="415" class="img_ev3q"></p>
<ul>
<li>假设你今天想做的是翻译,那机器读的文章就是一个英文句子,<strong>问题</strong>就是这个句子的德文翻译是什麼,然后输出的<strong>答案</strong>就是德文</li>
<li>或者是你想要叫机器自动作摘要,摘要就是给机器读一篇长的文章,叫他把长的文章的重点节录出来,那你就是给机器一段文字,<strong>问题</strong>是这段文字的摘要是什麼,然后期待他<strong>答案</strong>可以输出一个摘要</li>
<li>或者是你想要叫机器做Sentiment analysis,Sentiment analysis就是机器要自动判断一个句子,是正面的还是负面的；假设你有做了一个產品,然后上线以后,你想要知道网友的评价,但是你又不可能一直找人家ptt上面,把 每一篇文章都读过,所以就做一个Sentiment analysis model,看到有一篇文章裡面,有提到你的產品,然后就把这篇文章丢到,你的model裡面,去判断这篇文章,是正面还是负面。你就给机器要判断正面还负面的文章,<strong>问题</strong>就是这个句子,是正面还是负面的,然后希望机器可以告诉你<strong>答案</strong></li>
</ul>
<p>所以各式各样的NLP的问题,往往都可以看作是QA的问题,而<strong>QA的问题,就可以用Seq2Seq model来解</strong></p>
<p>具体来说就是有一个Seq2Seq model输入,就是有问题跟文章把它接在一起,输出就是问题的答案,就结束了,你的问题加文章合起来,是一段很长的文字,答案是一段文字</p>
<p>Seq2Seq model只要是<strong>输入一段文字</strong>,<strong>输出一段文字</strong>,只要是<strong>输入一个Sequence</strong>,<strong>输出一个Sequence</strong>就可以解,所以你可以把QA的问题,硬是用Seq2Seq model解,叫它读一篇文章读一个问题,然后就直接输出答案,所以各式各样NLP的任务,其实都有机会使用Seq2Seq model</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429201442155-6978182c02e2102cc9a0442fc4336829.png" width="551" height="385" class="img_ev3q"></p>
<p>必须要强调一下,对多数NLP的任务,或对多数的语音相关的任务而言,往往為这些任务<strong>客製化模型,你会得到更好的结果</strong></p>
<p>但是各个任务客製化的模型,就不是我们这一门课的重点了,如果你对人类语言处理,包括语音 包括自然语言处理,这些相关的任务有兴趣的话呢,可以参考一下以下课程网页的[连结](Source webpage: <a href="https://speech.ee.ntu.edu.tw/~hylee/dlhlp/2020-spring.html),%E5%B0%B1%E6%98%AF%E5%8E%BB%E5%B9%B4%E4%B8%8A%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0,%E4%B8%8E%E4%BA%BA%E7%B1%BB%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86,%E8%BF%99%E9%97%A8%E8%AF%BE%E7%9A%84%E5%86%85%E5%AE%B9%E8%A3%A1%E9%9D%A2%E5%B0%B1%E4%BC%9A%E6%95%99%E4%BD%A0,%E5%90%84%E5%BC%8F%E5%90%84%E6%A0%B7%E7%9A%84%E4%BB%BB%E5%8A%A1%E6%9C%80%E5%A5%BD%E7%9A%84%E6%A8%A1%E5%9E%8B,%E5%BA%94%E8%AF%A5%E6%98%AF%E4%BB%80%E9%BA%BC" target="_blank" rel="noopener noreferrer">https://speech.ee.ntu.edu.tw/~hylee/dlhlp/2020-spring.html),就是去年上的深度学习,与人类语言处理,这门课的内容裡面就会教你,各式各样的任务最好的模型,应该是什麼</a></p>
<p>举例来说在做语音辨识,我们刚才讲的是一个Seq2Seq model,输入一段声音讯号,直接输出文字,今天啊 Google的 pixel4,Google官方告诉你说,Google pixel4也是用,N to N的Neural network,pixel4裡面就是,有一个Neural network,输入声音讯号,输出就直接是文字</p>
<p>但他其实用的不是Seq2Seq model,他用的是一个叫做,RNN transducer的 model,像这些模型他就是為了,语音的某些特性所设计,这样其实可以表现得更好,至於每一个任务,有什麼样客製化的模型,这个就是另外一门课的主题,就不是我们今天想要探讨的重点</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="seq2seq-for-syntactic-parsing">Seq2seq for Syntactic Parsing<a href="#seq2seq-for-syntactic-parsing" class="hash-link" aria-label="Direct link to Seq2seq for Syntactic Parsing" title="Direct link to Seq2seq for Syntactic Parsing">​</a></h3>
<p>在语音还有自然语言处理上的应用,其实有很多应用,你<strong>不觉得他是一个Seq2Seq model的问题,但你都可以硬用Seq2Seq model的问题硬解他</strong></p>
<p>举例来说<strong>文法剖析</strong>,给机器一段文字,比如Deep learning is very powerful</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429202108751-d41e52ae9f333620da2746acae742bbe.png" width="549" height="285" class="img_ev3q"></p>
<p>机器要做的事情是產生,一个<strong>文法的剖析树</strong> 告诉我们,deep加learning合起来,是一个名词片语,very加powerful合起来,是一个形容词片语,形容词片语加is以后会  变成,一个动词片语,动词片语加名词片语合起来,是一个句子</p>
<p>那今天文法剖析要做的事情,就是產生这样子的一个Syntactic tree,所以在文法剖析的任务裡面,假设你想要deep learning解的话,输入是一段文字,他是一个Sequence,但输出看起来不像是一个Sequence,输出是一个树状的结构,但<strong>事实上一个树状的结构,可以硬是把他看作是一个Sequence</strong></p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429202417748-2c34611d995031ae138ffdfba13d5bf3.png" width="528" height="404" class="img_ev3q"></p>
<p>这个树状结构可以对应到一个,这样子的Sequence,从这个Sequence裡面,你也可以看出</p>
<ul>
<li>这个树状的结构有一个S，有一个左括号,有一个右括号</li>
<li>S裡面有一个noun phrase,有一个左括号跟右括号</li>
<li>NP裡面有一个左括号跟右括号,NP裡面有is</li>
<li>然后有这个形容词片语,他有一个左括号右括号</li>
</ul>
<p>这一个<strong>Sequence就代表了这一个tree 的structure</strong>,你先把tree 的structure,转成一个Sequence以后,你就可以用Seq2Seq model硬解他</p>
<p>train一个Seq2Seq model,读这个句子,然后直接输入这一串文字,再把这串文字转成一个树状的结构,你就可以硬是用Seq2Seq model,来做文法剖析这件事,这个概念听起来非常的狂,但这是真的可以做得到的,</p>
<p>你可以读一篇文章叫做,grammar as a Foreign Language</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429203059333-e0a486c8b8796a0cc6574d256664f2b1.png" width="573" height="410" class="img_ev3q"></p>
<p>这篇文章其实不是太新的文章,你会发现她放在arxiv上面的时间,是14年的年底,所以其实也是一个,上古神兽等级的文章,这篇文章问世的时候,那个时候Seq2Seq model还不流行,那时候Seq2Seq model,主要只有被用在翻译上,所以这篇文章的title才会取说,grammar as a Foreign Language</p>
<p>他把文法剖析这件事情,当作是一个翻译的问题,把文法当作是另外一种语言,直接套用当时人们认為,只能用在翻译上的模型硬做,结果他得到state of the art的结果</p>
<p>我(李宏毅老师)其实在国际会议的时候,有遇过这个第一作者Oriol Vlnyals,那个时候Seq2Seq model,还是个非常潮的东西,那个时候在我的认知裡面,我觉得这个模型,应该是挺难train的,我问他说,train Seq2Seq model有没有什麼tips,没想到你做个文法剖析,用Seq2Seq model,居然可以硬做到state of the art,这应该有什麼很厉害的tips吧</p>
<p>他说什麼没有什麼tips,他说我连Adam都没有用,我直接gradient descent,就train起来了,我第一次train就成功了,只是我要冲到state of the art,还是稍微调了一下参数而已,我也不知道是真的还假的啦,不过今天Seq2Seq model,真的是已经被很广泛地,应用在各式各样的应用上了</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="multi-label-classification">multi-label classification<a href="#multi-label-classification" class="hash-link" aria-label="Direct link to multi-label classification" title="Direct link to multi-label classification">​</a></h3>
<p>还有一些任务可以用seq2seq&#x27;s model,举例来说 ==multi-label的classification==</p>
<p>==multi-class==的classification,跟==multi-label==的classification,听起来名字很像,但他们其实是不一样的事情,multi-class的classification意思是说,我们有不只一个class机器要做的事情,是从数个class裡面,选择某一个class出来</p>
<p>但是multi-label的classification,意思是说<strong>同一个东西,它可以属於多个class</strong>,举例来说 你在做文章分类的时候</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429203936697-3776726a5754b00ea13401cf8b05dd5d.png" width="532" height="299" class="img_ev3q"></p>
<p>可能这篇文章 属於class 1跟3,这篇文章属於class 3 9 17等等,你可能会说,这  种multi-label classification的问题,能不能<strong>直接把它当作一个multi-class classification的问题来解</strong></p>
<p>举例来说,我把这些文章丢到一个classifier裡面</p>
<ul>
<li>本来classifier只会输出一个答案,输出分数最高的那个答案</li>
<li>我现在就输出分数最高的前三名,看看能不能解,multi-label的classification的问题</li>
</ul>
<p>但<strong>这种方法可能是行不通的</strong>,因為每一篇文章对应的class的数目,根本不一样 有些东西 有些文章,对应的class的数目,是两个 有的是一个 有的是三个</p>
<p>所以 如果你说 我直接取一个threshold,我直接取分数最高的前三名,class file output分数最高的前三名,来当作我的输出 显然,不一定能够得到好的结果 那怎麼办呢</p>
<p>这边可以用seq2seq硬做,<strong>输入一篇文章</strong> <strong>输出就是class</strong> 就结束了,机器自己决定 它要输出几个class</p>
<p>我们说seq2seq model,就是由机器自己决定输出几个东西,输出的output sequence的长度是多少,既然 你没有办法决定class的数目,那就让机器帮你决定,每篇文章 要属於多少个class</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="seq2seq-for-object-detection">Seq2seq for Object Detection<a href="#seq2seq-for-object-detection" class="hash-link" aria-label="Direct link to Seq2seq for Object Detection" title="Direct link to Seq2seq for Object Detection">​</a></h3>
<p>或者是object detection,这个看起来跟seq2seq model,应该八竿子打不著的问题,它也可以用seq2seq&#x27;s model硬解</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429204527490-80d70cd27126384c858fce14e80ccd5e.png" width="584" height="417" class="img_ev3q"></p>
<p>object detection就是给机器一张图片,然后它把图片裡面的物件框出来,把它框出说 这个是斑马 这个也是斑马,但这种问题 可以用seq2seq&#x27;s硬做,至於怎麼做 我们这边就不细 讲,我在这边放一个文献,放一个连结给大家参考,讲这麼多就是要告诉你说,seq2seq&#x27;s model 它是一个,很powerful的model,它是一个很有用的model</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="encoder-decoder">Encoder-Decoder<a href="#encoder-decoder" class="hash-link" aria-label="Direct link to Encoder-Decoder" title="Direct link to Encoder-Decoder">​</a></h2>
<p>我们现在就是要来学,怎麼做seq2seq这件事,一般的seq2seq&#x27;s model,它裡面会分成两块 一块是Encoder,另外一块是Decoder</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429205404198-246aa9fcad577178fe32f188224374ea.png" width="409" height="232" class="img_ev3q"></p>
<p>你input一个sequence有Encoder,负责处理这个sequence,再把处理好的结果丢给Decoder,由Decoder决定,它要输出什麼样的sequence,等一下 我们都还会再细讲,Encoder跟 Decoder内部的架构</p>
<p>seq2seq model的起源,其实非常的早 在14年的9月,就有一篇seq2seq&#x27;s model,用在翻译的文章 被放到Arxiv上</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429205447815-aa8e6e933e768aca285a6c8bc506bc0d.png" width="724" height="260" class="img_ev3q"></p>
<p>可以想像当时的seq2seq&#x27;s model,看起来还是比较阳春的,今天讲到seq2seq&#x27;s model的时候,大家第一个会浮现在脑中的,可能都是我们今天的主角,也就是transformer</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429205517760-e9c4d89d1b35e34908b5694d8e03d417.png" width="392" height="633" class="img_ev3q"></p>
<p>它有一个Encoder架构,有一个Decoder架构,它裡面有很多花花绿绿的block,等一下就会讲一下,这裡面每一个花花绿绿的block,分别在做的事情是什麼</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="encoder">Encoder<a href="#encoder" class="hash-link" aria-label="Direct link to Encoder" title="Direct link to Encoder">​</a></h3>
<p>seq2seq model ==Encoder==要做的事情,就是<strong>给一排向量，输出另外一排向量</strong></p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429205911444-211dde211a8bb0ee8688a7ff65395d72.png" width="528" height="361" class="img_ev3q"></p>
<p>给一排向量、输出一排向量这件事情,很多模型都可以做到,可能第一个想到的是,我们刚刚讲完的self-attention,其实不只self-attention,RNN CNN 其实也都能够做到,input一排向量,output另外一个同样长度的向量</p>
<p>在transformer裡面,transformer的Encoder,用的就是self-attention,这边看起来有点复杂,我们用另外一张图,来仔细地解释一下,这个Encoder的架构,等一下再来跟原始的transformer的,论文裡面的图进行比对,</p>
<p>现在的Encoder裡面,会<strong>分成很多很多的block</strong></p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429210126607-364aec6725859c0cd2772fcae3df6009.png" width="272" height="424" class="img_ev3q"></p>
<p>每一个block都是输入一排向量,输出一排向量,你输入一排向量 第一个block,第一个block输出另外一排向量,再输给另外一个block,到最后一个block,会输出最终的vector sequence,<strong>每一个block 其实,并不是neural network的一层</strong></p>
<p><strong>每一个block裡面做的事情,是好几个layer在做的事情</strong>,在transformer的Encoder裡面,每一个block做的事情,大概是这样子的</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429210257652-649b5c3b3c81b8f1520eabe1a8191f6f.png" width="599" height="446" class="img_ev3q"></p>
<ul>
<li>先做一个self-attention,input一排vector以后,做self-attention,考虑整个sequence的资讯，Output另外一排vector.</li>
<li>接下来这一排vector,会再丢到fully connected的feed forward network裡面,再output另外一排vector,这一排vector就是block的输出</li>
</ul>
<p>事实上在原来的<strong>transformer裡面,它做的事情是更复杂的</strong></p>
<p>在之前self-attention的时候,我们说 输入一排vector,就输出一排vector,这边的每一个vector,它是考虑了 所有的input以后,所得到的结果</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429210831750-3a60f5467160536954ee54638d240c8f.png" width="830" height="384" class="img_ev3q"></p>
<p>在transformer裡面,它加入了一个设计,我们<strong>不只是输出这个vector</strong>,我们还要<strong>把这个vector加上它的input</strong>,它要把input拉过来 直接加给输出,得到新的output</p>
<p>也就是说,这边假设这个vector叫做<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">a</span></span></span></span>,这个vector叫做<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span> 你要把<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span>当作是新的输出</p>
<p>这样子的network架构,叫做==residual connection==,那其实这种residual connection,在deep learning的领域用的是非常的广泛,之后如果我们有时间的话,再来详细介绍,為什麼要用residual connection</p>
<p>那你现在就先知道说,有一种network设计的架构,叫做<strong>residual connection,它会把input直接跟output加起来,得到新的vector</strong></p>
<p>得到residual的结果以后,再把它做一件事情叫做normalization,这边用的不是batch normalization,这边用的叫做==layer normalization==</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429211313025-434c844e9ac5463fa9cc2b8867e0a7cd.png" width="826" height="625" class="img_ev3q"></p>
<p>layer normalization做的事情,比bacth normalization更简单一点</p>
<p>输入一个向量 输出另外一个向量,不需要考虑batch,它会<strong>把输入的这个向量,计算它的mean跟standard deviation</strong></p>
<p>但是要注意一下,<strong>==batch normalization==是对不同example,不同feature的同一个dimension,去计算mean跟standard deviation</strong></p>
<p>但**==layer normalization==,它是对同一个feature,同一个example裡面,不同的dimension,去计算mean跟standard deviation**</p>
<p>计算出mean,跟standard deviation以后,就可以做一个normalize,我们把input 这个vector裡面每一个,dimension减掉mean,再除以standard deviation以后得到x&#x27;,就是layer normalization的输出</p>
<span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0489em;vertical-align:-0.247em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8019em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.9463em;vertical-align:-0.686em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">σ</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>
<p>得到layer normalization的输出以后,它的这个输出 才是FC network的输入</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210429211858981-a20bc4a871e6c8861bf64bb992589686.png" width="829" height="625" class="img_ev3q"></p>
<p>而<strong>FC network这边,也有residual的架构</strong>,所以 我们会把FC network的input,跟它的output加起来 做一下residual,得到新的输出</p>
<p>这个FC network做完residual以后,还不是结束 你要把residual的结果,<strong>再做一次layer normalization</strong>,得到的输出,才是residual network裡面,一个block的输出,所以这个是挺复杂的</p>
<p>所以我们这边讲的 这一个图,其实就是我们刚才讲的那件事情</p>
<img decoding="async" loading="lazy" src="https://github.com/unclestrong/DeepLearning_LHY21_Notes/blob/master/Notes_pic/image-20210429212721750.png?raw=true" alt="image-20210429212721750" style="zoom:50%" class="img_ev3q">
<ul>
<li>首先 你有self-attention,其实在input的地方,还有加上positional encoding,我们之前已经有讲过,如果你只光用self-attention,你没有未知的资讯,所以你需要加上positional的information,然后在这个图上,有特别画出positional的information</li>
<li>Multi-Head Attention,这个就是self-attention的block,这边有特别强调说,它是Multi-Head的self-attention</li>
<li>Add&amp;norm,就是residual加layer normalization,我们刚才有说self-attention,有加上residual的connection,加下来还要过layer normalization,这边这个图上的Add&amp;norm,就是residual加layer norm的意思</li>
<li>接下来,要过feed forward network</li>
<li>fc的feed forward network以后再做一次Add&amp;norm,再做一次residual加layer norm,才是一个block的输出,</li>
<li>然后这个block会重复n次,这个复杂的block,其实在之后会讲到的,一个非常重要的模型BERT裡面,会再用到 BERT,它其实就是transformer的encoder</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="to-learn-more">To Learn more<a href="#to-learn-more" class="hash-link" aria-label="Direct link to To Learn more" title="Direct link to To Learn more">​</a></h2>
<p>讲到这边 你心裡一定充满了问号,就是為什麼 transformer的encoder,要这样设计 不这样设计行不行?</p>
<p>行 不一定要这样设计,这个encoder的network架构,现在设计的方式,本文是按照原始的论文讲给你听的,但<strong>原始论文的设计 不代表它是最好的,最optimal的设计</strong></p>
<p><img decoding="async" loading="lazy" alt="image-20210429213356887" src="/assets/images/image-20210429213356887-194a761018985174c997c1734e6cfe68.png" width="571" height="325" class="img_ev3q"></p>
<ul>
<li>有一篇文章叫,<a href="https://arxiv.org/abs/2002.04745" target="_blank" rel="noopener noreferrer">on layer normalization in the transformer architecture</a>，它问的问题就是 為什麼,layer normalization是放在那个地方呢,為什麼我们是先做,residual再做layer normalization,能不能够把layer normalization,放到每一个block的input,也就是说 你做residual以后,再做layer normalization,再加进去 你可以看到说左边这个图,是原始的transformer,右边这个图是稍微把block,更换一下顺序以后的transformer,更换一下顺序以后 结果是会比较好的,这就代表说,原始的transformer 的架构,并不是一个最optimal的设计,你永远可以思考看看,有没有更好的设计方式</li>
<li>再来还有一个问题就是,為什麼是layer norm 為什麼是别的,不是别的,為什麼不做batch normalization,也许这篇paper可以回答你的问题,这篇paper是<a href="https://arxiv.org/abs/2003.07845" target="_blank" rel="noopener noreferrer">Power Norm：,Rethinking Batch Normalization In Transformers</a>,它首先告诉你说 為什麼,batch normalization不如,layer normalization,在Transformers裡面為什麼,batch normalization不如,layer normalization,接下来在说,它提出来一个power normalization,一听就是很power的意思,都可以比layer normalization,还要performance差不多或甚至好一点</li>
</ul></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#sequence-to-sequence-seq2seq" class="table-of-contents__link toc-highlight">Sequence-to-sequence (Seq2seq)</a><ul><li><a href="#hokkien闽南语台语" class="table-of-contents__link toc-highlight">Hokkien（闽南语、台语）</a></li><li><a href="#seq2seq-for-chatbot" class="table-of-contents__link toc-highlight">Seq2seq for Chatbot</a></li><li><a href="#question-answering-qa" class="table-of-contents__link toc-highlight">Question Answering (QA)</a></li><li><a href="#seq2seq-for-syntactic-parsing" class="table-of-contents__link toc-highlight">Seq2seq for Syntactic Parsing</a></li><li><a href="#multi-label-classification" class="table-of-contents__link toc-highlight">multi-label classification</a></li><li><a href="#seq2seq-for-object-detection" class="table-of-contents__link toc-highlight">Seq2seq for Object Detection</a></li></ul></li><li><a href="#encoder-decoder" class="table-of-contents__link toc-highlight">Encoder-Decoder</a><ul><li><a href="#encoder" class="table-of-contents__link toc-highlight">Encoder</a></li></ul></li><li><a href="#to-learn-more" class="table-of-contents__link toc-highlight">To Learn more</a></li></ul></div></div></div></div></main></div></div></div></div>
</body>
</html>