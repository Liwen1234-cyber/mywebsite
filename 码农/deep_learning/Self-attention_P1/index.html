<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-码农/deep_learning/Self-attention_P1" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Self-attention | Coisini</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://doc.minddiy.top/码农/deep_learning/Self-attention_P1/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Self-attention | Coisini"><meta data-rh="true" name="description" content="CNN以后,我们要讲另外一个常见的Network架构,这个架构叫做Self-Attention,而这个Self-Attention"><meta data-rh="true" property="og:description" content="CNN以后,我们要讲另外一个常见的Network架构,这个架构叫做Self-Attention,而这个Self-Attention"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://doc.minddiy.top/码农/deep_learning/Self-attention_P1/"><link data-rh="true" rel="alternate" href="https://doc.minddiy.top/码农/deep_learning/Self-attention_P1/" hreflang="en"><link data-rh="true" rel="alternate" href="https://doc.minddiy.top/码农/deep_learning/Self-attention_P1/" hreflang="x-default"><meta name="google-site-verification" content="1FUPX6Qo4y3ecU623ShEurhgnjhSTjK49rRMhEDlzFA">
<link rel="stylesheet" href="/katex/katex.min.css">
<script src="/js/matomo.js" async defer="defer"></script><link rel="stylesheet" href="/assets/css/styles.79037026.css">
<script src="/assets/js/runtime~main.468f2b27.js" defer="defer"></script>
<script src="/assets/js/main.4763ab3e.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Chialisp Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="Chialisp Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Coisini</b></a></div><div class="navbar__items navbar__items--right"><a href="https://minddiy.top" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Main site<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input" value=""><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><main class="docMainContainer_TBSr docMainContainerEnhanced_lQrH"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Self-attention</h1></header>
<p>CNN以后,我们要讲另外一个常见的Network架构,这个架构叫做Self-Attention,而这个Self-Attention</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="sophisticated-input">Sophisticated Input<a href="#sophisticated-input" class="hash-link" aria-label="Direct link to Sophisticated Input" title="Direct link to Sophisticated Input">​</a></h2>
<p>到目前為止,我们的Network的<strong>Input</strong>都是<strong>一个向量</strong>,不管是在预测这个,YouTube观看人数的问题上啊,还是影像处理上啊,我们的输入都可以看作是一个向量,然后我们的输出,可能是一个<strong>数值</strong>,这个是<strong>Regression</strong>,可能是一个<strong>类别</strong>,这是<strong>Classification</strong></p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210403221220604-18233e15bb74ee59ab655083f242c0e6.png" width="542" height="284" class="img_ev3q"></p>
<p>但假设我们遇到更復杂的问题呢,假设我们说<strong>输入是多个向量</strong>,而且这个<strong>输入的向量的数目是会改变的</strong>呢</p>
<p>我们刚才在讲影像辨识的时候,我还特别强调我们假设输入的<strong>影像大小都是一样</strong>的,那现在假设每次我们Model输入的Sequence的数目,Sequence的长度都不一样呢,那这个时候应该要怎麼处理？</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="vector-set-as-input">Vector Set as Input<a href="#vector-set-as-input" class="hash-link" aria-label="Direct link to Vector Set as Input" title="Direct link to Vector Set as Input">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="文字处理">文字处理<a href="#文字处理" class="hash-link" aria-label="Direct link to 文字处理" title="Direct link to 文字处理">​</a></h3>
<p>假设我们今天要Network的输入是一个句子,每一个句子的长度都不一样,每个句子裡面词汇的数目都不一样</p>
<p><img decoding="async" loading="lazy" alt="image-20210403221757797" src="data:image/png;base64,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" width="172" height="91" class="img_ev3q"></p>
<p>如果我们把一个<strong>句子裡面的每一个词汇,都描述成一个向量</strong>,那我们的Model的<strong>输入</strong>,就会<strong>是一个Vector Set</strong>,而且这个Vector Set的大小,每次都不一样,句子的长度不一样,那你的Vector Set的大小就不一样</p>
<p>那<strong>怎麼把一个词汇表示成一个向量</strong>,最简单的做法是==One-Hot的Encoding==</p>
<p><img decoding="async" loading="lazy" src="data:image/png;base64,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" width="290" height="247" class="img_ev3q"></p>
<p>你就开一个很长很长的向量,这个向量的长度跟世界上存在的词汇的数目是一样多的,每一个维度对应到一个词汇,Apple就是100,Bag就是010,Cat就是001,以此类推</p>
<p>但是这样子的表示方法有一个非常严重的问题,它假设所有的<strong>词汇彼此之间都是没有关係</strong>的,从这个向量裡面你看不到：Cat跟Dog都是动物所以他们比较接近,Cat跟Apple一个动物一个植物,所以他们比较不相像。这个向量裡面,没有任何语义的资讯</p>
<p>有另外一个方法叫做==Word Embedding==</p>
<p><img decoding="async" loading="lazy" src="data:image/png;base64,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" width="304" height="261" class="img_ev3q"></p>
<p>Word Embedding就是,我们会给每一个词汇一个向量,而这个<strong>向量是有语义的资讯的</strong></p>
<p>如果你把Word Embedding画出来的话,你会发现,所有的动物可能聚集成一团,所有的植物可能聚集成一团,所有的动词可能聚集成一团等等</p>
<p>Word Embedding，如果你有兴趣的话,可以看一下以下的录影<a href="https://youtu.be/X7PH3NuYW0Q,%E6%80%BB%E4%B9%8B%E4%BD%A0%E7%8E%B0%E5%9C%A8%E5%9C%A8%E7%BD%91%E8%B7%AF%E4%B8%8A,%E5%8F%AF%E4%BB%A5%E8%BD%BD%E5%88%B0%E4%B8%80%E7%A7%8D%E4%B8%9C%E8%A5%BF%E5%8F%AB%E5%81%9AWord" target="_blank" rel="noopener noreferrer">https://youtu.be/X7PH3NuYW0Q,总之你现在在网路上,可以载到一种东西叫做Word</a> Embedding,这个Word Embedding,会给每一个词汇一个向量,而<strong>一个句子就是一排长度不一的向量</strong></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="声音信号">声音信号<a href="#声音信号" class="hash-link" aria-label="Direct link to 声音信号" title="Direct link to 声音信号">​</a></h3>
<p>一段声音讯号其实是一排向量,怎麼说呢,我们会把一段声音讯号取一个范围,这个范围叫做一个==Window==</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404160614690-d768a0935969be8f237f3b1107fb13cb.png" width="697" height="418" class="img_ev3q"></p>
<p>把这个Window裡面的资讯描述成一个向量,这个向量就叫做一个==Frame==,在语音上,我们会把<strong>一个向量叫做一个Frame</strong>,通常这个Window的长度就是25个Millisecond</p>
<p>把这麼一个小段的声音讯号变成一个Frame,变成一个向量就有百百种做法,那这边就不细讲</p>
<p>一小段25个Millisecond裡面的语音讯号,為了要描述一整段的声音讯号,你会把这个<strong>Window往右移一点</strong>,通常移动的大小是10个Millisecond</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404161620047-a251efa121069d75222dc78cd3dd7fd4.png" width="699" height="484" class="img_ev3q"></p>
<p>一段声音讯号,你就是用一串向量来表示,而因為每一个Window啊,他们往右移都是移动10个Millisecond,所以一秒鐘的声音讯号有100个向量,所以一分鐘的声音讯号,就有这个100乘以60,就有6000个向量</p>
<p>所以语音其实很复杂的,一小段的声音讯号,它裡面包含的资讯量其实是非常可观的</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="图">图<a href="#图" class="hash-link" aria-label="Direct link to 图" title="Direct link to 图">​</a></h3>
<p>一个Graph 一个图,也是一堆向量,我们知道说Social Network就是一个Graph</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404161854708-051a798a7f79dc000443b93edc91260d.png" width="737" height="445" class="img_ev3q"></p>
<p>在Social Network上面<strong>每一个节点就是一个人</strong>,然后<strong>节点跟节点之间的edge就是他们两个的关系连接</strong>,比如说是不是朋友等等</p>
<p>而<strong>每一个节点可以看作是一个向量</strong>,你可以拿每一个人的,比如说他的Profile裡面的资讯啊,他的性别啊 他的年龄啊,他的工作啊 他讲过的话啊等等,把这些资讯用一个向量来表示</p>
<p>所以一个Social Network 一个Graph,你也可以看做是一堆的向量所组成的</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="分子信息">分子信息<a href="#分子信息" class="hash-link" aria-label="Direct link to 分子信息" title="Direct link to 分子信息">​</a></h3>
<p>一个分子,它也可以看作是一个Graph</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404162239665-764c223ad41cbab9cad8d952701c51cc.png" width="767" height="433" class="img_ev3q"></p>
<p>现在Drug Discovery的应用非常地受到重视,尤其是在Covid-19这一段时间,很多人都期待,也许用机器学习,可以在Drug Discovery上面做到什麼突破,那这个时候,你就需要把一个分子,当做是你的模型的输入</p>
<p><strong>一个分子可以看作是一个Graph</strong>,分子上面的每一个球,也就是<strong>每一个原子，可以表述成一个向量</strong></p>
<p>一个<strong>原子可以用One-Hot Vector</strong>来表示,氢就是1000,碳就是0100,然后这个氧就是0010,所以一个分子就是一个Graph,它就是一堆向量。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-output">What is the output?<a href="#what-is-the-output" class="hash-link" aria-label="Direct link to What is the output?" title="Direct link to What is the output?">​</a></h2>
<p>我们刚才已经看说输入是一堆向量,它可以是文字,可以是语音,可以是Graph,那这个时候,我们有可能有什麼样的输出呢,有三种可能性</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-每一个向量都有一个对应的label">1. 每一个向量都有一个对应的Label<a href="#1-每一个向量都有一个对应的label" class="hash-link" aria-label="Direct link to 1. 每一个向量都有一个对应的Label" title="Direct link to 1. 每一个向量都有一个对应的Label">​</a></h3>
<p>当你的模型,看到输入是四个向量的时候,它就要输出四个Label,而每一个Label,它可能是一个数值,那就是Regression的问题,如果每个Label是一个Class,那就是一个Classification的问题</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404165717142-b46435c1d0d4d6a9443538b11a9eedc0.png" width="774" height="527" class="img_ev3q"></p>
<ul>
<li>
<p>举例来说 在文字处理上,假设你今天要做的是==POS Tagging==,POS Tagging就是词性标註,你要让机器自动决定每一个词汇 它是什麼样的词性,它是名词 还是动词 还是形容词等等</p>
<p>这个任务啊,其实并没有很容易,举例来说,你现在看到一个句子,I saw a saw</p>
<p>这并不是打错,并不是“我看一个看”,而是“我看到一个锯子”,这个第二个saw当名词用的时候,它是锯子，那所以机器要知道,第一个saw是个动词,第二个saw虽然它也是个saw,但它是名词,但是每一个输入的词汇,都要有一个对应的输出的词性</p>
<p>这个任务就是,输入跟输出的长度是一样的Case,这个就是属於第一个类型的输出</p>
</li>
<li>
<p>那如果是语音的话,你可以想想看我们作业二就是这样子的任务</p>
<p>虽然我们作业二,没有给大家一个完整的Sequence,我们是把每一个每一个每一个Vector分开给大家了,但是串起来就是一段声音讯号裡面,有一串Vector,每一个Vector你都要决定,它是哪一个Phonetic，这是一个语音辨识的简化版</p>
</li>
<li>
<p>或者是如果是Social Network的话,就是给一个Graph</p>
<p>你的Model要决定每一个节点,它有什麼样的特性,比如说他会不会买某一个商品,这样我们才知道说,要不要推荐某一个商品给他,</p>
<p>所以以上就是举输入跟输出 数目一样的例子</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-一整个sequence只需要输出一个label">2. 一整个Sequence,只需要输出一个Label<a href="#2-一整个sequence只需要输出一个label" class="hash-link" aria-label="Direct link to 2. 一整个Sequence,只需要输出一个Label" title="Direct link to 2. 一整个Sequence,只需要输出一个Label">​</a></h3>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404165454032-1a851a22adc405ff38babe96fa646cf3.png" width="787" height="322" class="img_ev3q"></p>
<ul>
<li>
<p>举例来说,如果是文字的话,我们就说Sentiment Analysis</p>
<p>Sentiment Analysis就是给机器看一段话,它要<strong>决定说这段话是正面的还是负面的</strong></p>
<p>那你可以想像说这种应用很有用,假设你的公司开发了一个產品,这个產品上线了,你想要知道网友的评价怎麼样,但是你又不可能一则一则网友的留言都去分析,那也许你就可以用这种,Sentiment Analysis的技术,让机器自动去判读说,当一则贴文裡面有提到某个產品的时候,它是正面的 还是负面的,那你就可以知道你的產品,在网友心中的评价怎麼样,这个是Sentiment Analysis给一整个句子,只需要一个Label,那Positive或Negative,那这个就是第二类的输出</p>
</li>
<li>
<p>那如果是语音的例子的话呢,在作业四裡面我们会做语者辨认,机器要听一段声音,然后决定他是谁讲的</p>
</li>
<li>
<p>或者是如果是Graph的话呢,今天你可能想要给一个分子,然后要预测说这个分子,比如说它有没有毒性,或者是它的亲水性如何,那这就是给一个Graph 输出一个Label</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-机器要自己决定应该要输出多少个label">3. 机器要自己决定,应该要输出多少个Label<a href="#3-机器要自己决定应该要输出多少个label" class="hash-link" aria-label="Direct link to 3. 机器要自己决定,应该要输出多少个Label" title="Direct link to 3. 机器要自己决定,应该要输出多少个Label">​</a></h3>
<p>我们不知道应该输出多少个Label,机器要自己决定,应该要输出多少个Label,可能你输入是N个向量,输出可能是N&#x27;个Label,為什麼是N&#x27;,机器自己决定</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404170045293-afe2f324f317b311d71353553b7d57ef.png" width="787" height="206" class="img_ev3q"></p>
<p>这种任务又叫做==sequence to sequence==的任务,那我们在作业五会有sequence to sequence的作业,所以这个之后我们还会再讲</p>
<ul>
<li>翻译就是sequence to sequence的任务,因為输入输出是不同的语言,它们的词汇的数目本来就不会一样多</li>
<li>或者是语音辨识也是,真正的语音辨识也是一个sequence to sequence的任务,输入一句话,然后输出一段文字,这也是一个sequence to sequence的任务</li>
</ul>
<p>第二种类型有作业四,感兴趣可以去看看作业四  的程式，那因為上课时间有限,所以上课,我们今天就先只讲第一个类型,也就是输入跟输出数目一样多的状况</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="sequence-labeling">Sequence Labeling<a href="#sequence-labeling" class="hash-link" aria-label="Direct link to Sequence Labeling" title="Direct link to Sequence Labeling">​</a></h2>
<p>那这种输入跟输出数目一样多的状况又叫做==Sequence Labeling==,你要给Sequence裡面的每一个向量,都给它一个Label,那要怎麼解Sequence Labeling的问题呢</p>
<p>那直觉的想法就是我们就拿个<strong>Fully-Connected的Network</strong></p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404172216111-ada4a26a7fd4b288442f3a3e06434dd0.png" width="692" height="465" class="img_ev3q"></p>
<p>然后虽然这个输入是一个Sequence,但我们就各个击破,不要管它是不是一个Sequence,把每一个向量,分别输入到Fully-Connected的Network裡面</p>
<p>然后Fully-Connected的Network就会给我们输出,那现在看看,你要做的是Regression还是Classification,產生正确的对应的输出,就结束了,</p>
<p>那这麼做显然有<strong>非常大的瑕疵</strong>,假设今天是,词性标记的问题,你给机器一个句子,I saw a saw,对Fully-Connected Network来说,<strong>后面这一个saw跟前面这个saw完全一模一样</strong>,它们是同一个词汇啊</p>
<p>既然Fully-Connected的Network<strong>输入同一个词汇,它没有理由输出不同的东西</strong></p>
<p>但实际上,你期待第一个saw要输出动词,第二个saw要输出名词,但对Network来说它不可能做到,因為这两个saw 明明是一模一样的,你叫它一个要输出动词,一个要输出名词,它会非常地困惑,完全不知道要怎麼处理</p>
<p>所以怎麼办,有没有可能<strong>让Fully-Connected的Network,考虑更多的,比如说上下文的Context的资讯</strong>呢</p>
<p>这是有可能的,你就<strong>把前后几个向量都串起来,  一起丢到Fully-Connected的Network就结束了</strong></p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404172839097-d8d237249b37e94a31e4dfbe26703107.png" width="845" height="467" class="img_ev3q"></p>
<p>在作业二裡面,我们不是只看一个Frame,去判断这个Frame属於哪一个Phonetic,也就属於哪一个音标,而是看这个Frame的前面五个加后面五个,也就总共看十一个Frame,来决定它是哪一个音标</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404173219186-3baa6a99170fb543221fce433f011e45.png" width="796" height="514" class="img_ev3q"></p>
<p>所以我们可以给Fully-Connected的Network,一整个Window的资讯,让它可以考虑一些上下文的,跟我现在要考虑的这个向量,相邻的其他向量的资讯</p>
<p>但是<strong>这样子的方法还是有极限</strong>,作业二就算是给你Sequence的资讯,你考虑整个Sequence,你可能也很难再做的更好啦,作业二考虑前后五个Frame,其实就可以得到很不错的结果了,所以你要过Strong Baseline,重点并不在於考虑整个Sequence,你就不需要往那个方向想了,用助教现有给你的Data,你就可以轻易的过Strong Baseline,</p>
<p>但是真正的问题,但是如果今天我们有某一个任务,不是考虑一个Window就可以解决的,而是要<strong>考虑一整个Sequence才能够解决</strong>的话,那要怎麼办呢</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404173257899-c963ba2df1ea025334ec5c04cbe45359.png" width="823" height="514" class="img_ev3q"></p>
<p>那有人可能会想说这个很容易，我就<strong>把Window开大一点啊,大到可以把整个Sequence盖住就结束了</strong></p>
<p>但是，今天<strong>Sequence的长度是有长有短的</strong>,我们刚才有说,我们输入给我们的Model的Sequence的长度,每次可能都不一样</p>
<p>如果你今天说我真的要开一个Window,把整个Sequence盖住,那你可能要<strong>统计一下你 的训练资料</strong>,然后看看你的训练资料裡面,最长的Sequence有多长,然后开一个Window比最长的Sequence还要长,你才有可能把整个Sequence盖住</p>
<p>但是你开一个这麼大的Window,意味著说你的Fully-Connected的Network,它需要非常多的参数,那可能不只<strong>运算量很大,可能还容易Overfitting</strong></p>
<p>所以有没有更好的方法,来考虑整个Input Sequence的资讯呢,这就要用到我们接下来要跟大家介绍的,==Self-Attention==这个技术</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="self-attention-1">Self-Attention<a href="#self-attention-1" class="hash-link" aria-label="Direct link to Self-Attention" title="Direct link to Self-Attention">​</a></h2>
<p>Self-Attention的运作方式就是,<strong>Self-Attention会吃一整个Sequence的资讯</strong></p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404174805338-3a6382d83b5672342f5abd6097ff4fe2.png" width="717" height="567" class="img_ev3q"></p>
<p>然后你Input几个Vector,它就输出几个Vector,比如说你这边Input一个深蓝色的Vector,这边就给你一个另外一个Vector</p>
<p>这边给个浅蓝色,它就给你另外一个Vector,这边输入4个Vector,它就Output 4个Vector</p>
<p>那这4个Vector有什麼特别的地方呢,<strong>这4个Vector,他们都是考虑一整个Sequence以后才得到的</strong>,那等一下我会讲说Self-Attention,怎麼考虑一整个Sequence的资讯</p>
<p>所以这边每一个向量,我们特别给它一个黑色的框框代表说它不是一个普通的向量</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404193106925-0ee6e4fe2496d75b56ee38043c6ab3c7.png" width="750" height="323" class="img_ev3q"></p>
<p>如此一来你这个Fully-Connected的Network,它就不是只考虑一个非常小的范围,或一个小的Window,而是考虑整个Sequence的资讯,再来决定现在应该要输出什麼样的结果，这个就是Self-Attention。</p>
<p><strong>Self-Attention不是只能用一次,你可以叠加很多次</strong></p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404194146583-46de01744a5219e4a5ed223745d84db6.png" width="693" height="609" class="img_ev3q"></p>
<p>可以Self-Attention的输出,通过Fully-Connected Network以后,再做一次Self-Attention,Fully-Connected的Network,再过一次Self-Attention,再重新考虑一次整个Input Sequence的资讯,再丢到另外一个Fully-Connected的Network,最后再得到最终的结果</p>
<p>所以<strong>可以把Fully-Connected的Network,跟Self-Attention交替使用</strong></p>
<ul>
<li>Self-Attention处理整个Sequence的资讯</li>
<li>Fully-Connected的Network,专注於处理某一个位置的资讯</li>
<li>再用Self-Attention,再把整个Sequence资讯再处理一次</li>
<li>然后交替使用Self-Attention跟Fully-Connected</li>
</ul>
<p>有关Self-Attention,最知名的相关的文章,就是《Attention is all you need》.那在这篇Paper裡面呢,Google提出了==Transformer==这样的Network架构,那Transformer就是变形金刚,所以提到这个Network的时候呢,我们就会有变形金刚这个形象</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404201325728-8705b7eea0905b4268e37ae692459765.png" width="288" height="220" class="img_ev3q"></p>
<p>Transformer我们今天还不会讲到,但我们之后会讲到,Transformer裡面一个最重要的Module就是Self-Attention,它就是变形金刚的火种源</p>
<p>那这篇Paper最厉害的地方,就是它有一个<strong>霸气的名字Attention is all you need.</strong></p>
<p>那其实像Self-Attention这样的架构,最早我并不会说它是出现在《Attention is all you need》这样的Paper,因為其实很多更早的Paper,就有提出过类似的架构,只是不见得叫做Self-Attention,比如说叫做Self-Matching,或者是叫别的名字,不过呢是Attention is all you need.这篇Paper,把Self-Attention这个Module,把它发扬光大</p>
<p>那Self-Attention是怎麼运作的 呢</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="self-attention过程">Self-Attention过程<a href="#self-attention过程" class="hash-link" aria-label="Direct link to Self-Attention过程" title="Direct link to Self-Attention过程">​</a></h3>
<p>Self-Attention的Input,它就是一串的Vector,那<strong>这个Vector可能是你整个Network的Input,它也可能是某个Hidden Layer的Output</strong>,所以我们这边不是用<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span>来表示它,</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404202435331-3d51f11f2bf598b37b9c73da89005511.png" width="787" height="518" class="img_ev3q"></p>
<p>我们用<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">a</span></span></span></span>来表示它，代表它有可能是前面已经做过一些处理,它是某个Hidden Layer的Output,那Input一排a这个向量以后,Self-Attention要Output另外一排b这个向量</p>
<p>那这<strong>每一个b都是考虑了所有的a以后才生成出来的</strong>,所以这边刻意画了非常非常多的箭头,告诉你<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>考虑了<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>到<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span>產生的,<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>考虑<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>到<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span>產生的,<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span>也是一样,考虑整个input的sequence,才產生出来的</p>
<p>那接下来呢就是要跟大家说明,<strong>怎麼產生<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>这个向量</strong>,那你知道怎麼產生<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>这个向量以后,你就知道怎麼產生剩下<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span>剩下的向量</p>
<p>这里有一个<strong>特别的机制</strong>,<strong>这个机制是根据<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>这个向量,找出整个很长的sequence裡面,到底哪些部分是重要的,哪些部分跟判断<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>是哪一个label是有关係的,哪些部分是我们要决定<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>的class,决定<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>的regression数值的时候,所需要用到的资讯</strong></p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404202942477-66d02e9785433d3fe6df1042d10ccf79.png" width="780" height="503" class="img_ev3q"></p>
<p><strong>每一个向量跟<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>的关联的程度,用一个数值叫α来表示</strong></p>
<p>这个self-attention的module,<strong>怎麼自动决定两个向量之间的关联性</strong>呢,你给它两个向量<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>跟<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span>,它怎麼决定<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>跟<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span>有多相关,然后给它一个数值α呢,那这边呢你就需要一个<strong>计算attention的模组</strong></p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404204458431-82f2fe27c1d513a2bdcf510c1ff9d433.png" width="760" height="597" class="img_ev3q"></p>
<p>这个计算attention的模组,就是拿<strong>两个向量作為输入</strong>,然后它就直接输出α那个数值,</p>
<p>计算这个α的数值有各种不同的做法</p>
<ul>
<li>
<p>比较常见的做法呢,叫做用==dot product==,<strong>输入的这两个向量分别乘上两个不同的矩阵</strong>,左边这个向量乘上<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">q</span></span></span></span></span></span></span></span></span></span></span>这个矩阵得到矩阵<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span></span></span></span>,右边这个向量乘上<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span></span></span></span></span></span></span></span>这个矩阵得到矩阵<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></p>
<p>再把<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span></span></span></span>跟<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span>做dot product,就是把他们做element-wise 的相乘,再全部加起来以后就得到一个 scalar,这个scalar就是α,这是一种计算α的方式</p>
</li>
<li>
<p>有另外一个叫做==Additive==的计算方式,它的计算方法就是,把同样这两个向量通过<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">q</span></span></span></span></span></span></span></span></span></span></span> <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span></span></span></span></span></span></span></span>,得到<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span></span></span></span>跟<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span>,那我们不是把它做Dot-Product,是把它这个串起来,然后丢到这个过一个Activation Function</p>
<p>然后再通过一个Transform,然后得到α</p>
</li>
</ul>
<p>总之有非常多不同的方法,可以计算Attention,可以计算这个α的数值,可以计算这个关联的程度</p>
<p>但是在接下来的讨论裡面,我们都<strong>只用左边这个方法</strong>,这也是今日最常用的方法,也<strong>是用在Transformer裡面的方法</strong></p>
<p>那你就要把这边的<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>去跟这边的<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span>,分别都去计算他们之间的关联性,也就是计算他们之间的α</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404211656032-4f6884147d2fec57589980f05a5ac3ad.png" width="798" height="440" class="img_ev3q"></p>
<p>你把<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>乘上<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">q</span></span></span></span></span></span></span></span></span></span></span>得到<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>,那这个q有一个名字,我们叫做==Query==,它就像是你搜寻引擎的时候,去搜寻相关文章的问题,就像搜寻相关文章的关键字,所以这边叫做Query</p>
<p>然后接下来呢,<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span>你都要去把它乘上<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span></span></span></span></span></span></span></span>,得到<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span>这个Vector,<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span>这个Vector叫做==Key==,那你把这个<strong>Query q1,跟这个Key k2,算Inner-Product就得到α</strong></p>
<p>我们这边用<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span>来代表说,Query是1提供的,Key是2提供的时候,这个1跟2他们之间的关联性,这个α这个关联性叫做==Attention的Score==,叫做Attention的分数,</p>
<p>接下来也要跟<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span>来计算</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404211950882-fc31fe4c0fb6f007972c72d43e610803.png" width="812" height="423" class="img_ev3q"></p>
<p>把<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>乘上<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span></span></span></span></span></span></span></span>,得到另外一个Key也就是<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span>,<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span>乘上<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span></span></span></span></span></span></span></span>得到<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span>,然后你再把<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span>这个Key,跟<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>这个Query做Inner-Product,得到1跟3之间的关联性,得到1跟3的Attention,你把<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span>跟<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>做Dot-Product,得到<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mpunct mtight">,</span><span class="mord mtight">4</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span>,得到1跟4之间的关联性</p>
<p>其实一般在实作时候,<strong><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>也会跟自己算关联性</strong>,自己跟自己计算关联性这件事情有多重要,你可以自己在做作业的时候试试看,看这件事情的影响大不大了</p>
<p>计算出,a1跟每一个向量的关联性以后,接下来这边会<strong>接入一个Soft-Max</strong></p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404212945356-d213a53b590fe07f288a87cdb639ea6a.png" width="808" height="625" class="img_ev3q"></p>
<p><strong>这个Soft-Max跟分类的时候的那个Soft-Max是一模一样的</strong>,所以Soft-Max的输出就是一排α,所以本来有一排α,通过Soft-Max就得到<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>这边你<strong>不一定要用Soft-Max,用别的替代也没问题</strong>,比如说有人尝试过说做个ReLU,这边通通做个ReLU,那结果发现还比Soft-Max好一点,所以这边你不一定要用Soft-Max,这边你要用什麼Activation Function都行,你高兴就好,你可以试试看,那Soft-Max是最常见的,那你可以自己试试看,看能不能试出比Soft-Max更好的结果</p>
<p>接下来得到这个<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span>以后,我们就要根据这个<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span>去抽取出这个Sequence裡面重要的资讯,根据这个α我们已经知道说,哪些向量跟<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>是最有关係的,怎麼抽取重要的资讯呢,</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210404213559086-1921f6c4a4270d085815f81b6a2a32d7.png" width="827" height="575" class="img_ev3q"></p>
<ul>
<li>
<p>首先把<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>到<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span>这边每一个向量,乘上<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span></span></span></span></span></span></span></span></span></span></span>得到新的向量,这边分别就是用<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span>来表示</p>
</li>
<li>
<p>接下来把这边的<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>到<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span>,每一个向量都去乘上Attention的分数,都去乘上<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></p>
</li>
<li>
<p>然后再把它加起来,得到<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></p>
</li>
</ul>
<span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8641em"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.3277em;vertical-align:-1.2777em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em"><span style="top:-1.8723em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8019em"><span style="top:-2.453em;margin-left:-0.0037em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8747em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span></span></span></span>
<p>如果某一个向量它得到的分数越高,比如说如果<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>跟<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>的关联性很强,这个<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span>得到的值很大,那我们今天在做Weighted Sum以后,得到的<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>的值,就可能会比较接近<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></p>
<p>所以<strong>谁的那个Attention的分数最大,谁的那个<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>就会Dominant你抽出来的结果</strong></p>
<p>所以这边呢我们就讲了怎麼从一整个Sequence 得到<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#sophisticated-input" class="table-of-contents__link toc-highlight">Sophisticated Input</a></li><li><a href="#vector-set-as-input" class="table-of-contents__link toc-highlight">Vector Set as Input</a><ul><li><a href="#文字处理" class="table-of-contents__link toc-highlight">文字处理</a></li><li><a href="#声音信号" class="table-of-contents__link toc-highlight">声音信号</a></li><li><a href="#图" class="table-of-contents__link toc-highlight">图</a></li><li><a href="#分子信息" class="table-of-contents__link toc-highlight">分子信息</a></li></ul></li><li><a href="#what-is-the-output" class="table-of-contents__link toc-highlight">What is the output?</a><ul><li><a href="#1-每一个向量都有一个对应的label" class="table-of-contents__link toc-highlight">1. 每一个向量都有一个对应的Label</a></li><li><a href="#2-一整个sequence只需要输出一个label" class="table-of-contents__link toc-highlight">2. 一整个Sequence,只需要输出一个Label</a></li><li><a href="#3-机器要自己决定应该要输出多少个label" class="table-of-contents__link toc-highlight">3. 机器要自己决定,应该要输出多少个Label</a></li></ul></li><li><a href="#sequence-labeling" class="table-of-contents__link toc-highlight">Sequence Labeling</a></li><li><a href="#self-attention-1" class="table-of-contents__link toc-highlight">Self-Attention</a><ul><li><a href="#self-attention过程" class="table-of-contents__link toc-highlight">Self-Attention过程</a></li></ul></li></ul></div></div></div></div></main></div></div></div></div>
</body>
</html>