<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-码农/deep_learning/Local Minimum And Saddle Point" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">When gradient is small | Coisini</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://doc.minddiy.top/码农/deep_learning/Local Minimum And Saddle Point/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="When gradient is small | Coisini"><meta data-rh="true" name="description" content="Critical Point"><meta data-rh="true" property="og:description" content="Critical Point"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://doc.minddiy.top/码农/deep_learning/Local Minimum And Saddle Point/"><link data-rh="true" rel="alternate" href="https://doc.minddiy.top/码农/deep_learning/Local Minimum And Saddle Point/" hreflang="en"><link data-rh="true" rel="alternate" href="https://doc.minddiy.top/码农/deep_learning/Local Minimum And Saddle Point/" hreflang="x-default"><meta name="google-site-verification" content="1FUPX6Qo4y3ecU623ShEurhgnjhSTjK49rRMhEDlzFA">
<link rel="stylesheet" href="/katex/katex.min.css">
<script src="/js/matomo.js" async defer="defer"></script><link rel="stylesheet" href="/assets/css/styles.79037026.css">
<script src="/assets/js/runtime~main.468f2b27.js" defer="defer"></script>
<script src="/assets/js/main.4763ab3e.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Chialisp Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="Chialisp Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Coisini</b></a></div><div class="navbar__items navbar__items--right"><a href="https://minddiy.top" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Main site<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input" value=""><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><main class="docMainContainer_TBSr docMainContainerEnhanced_lQrH"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>When gradient is small</h1></header>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="critical-point">Critical Point<a href="#critical-point" class="hash-link" aria-label="Direct link to Critical Point" title="Direct link to Critical Point">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="training-fails-because">Training Fails because<a href="#training-fails-because" class="hash-link" aria-label="Direct link to Training Fails because" title="Direct link to Training Fails because">​</a></h3>
<p>​	现在我们要讲的是Optimization的部分,所以我们要讲的东西基本上跟Overfitting没有什麼太大的关联,我们只讨论Optimization的时候,怎麼把gradient descent做得更好,那為什麼Optimization会失败呢？</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210314152447908-ee016256ebcee49f42836e31f19ef1a5.png" width="752" height="355" class="img_ev3q"></p>
<p>​	你常常在做Optimization的时候,你会发现,<strong>随著你的参数不断的update,你的training的loss不会再下降</strong>,但是你对这个loss仍然不满意,就像我刚才说的,你可以把deep的network,跟linear的model,或比较shallow network 比较,发现说它没有做得更好,所以你觉得deepnetwork,没有发挥它完整的力量,所以Optimization显然是有问题的</p>
<p>​	但有时候你会甚至发现,一开始你的model就train不起来,一开始你不管怎麼update你的参数,你的loss通通都掉不下去,那这个时候到底发生了什麼事情呢？</p>
<p>​	过去常见的一个猜想,是因為我们现在走到了一个地方,<strong>这个地方参数对loss的微分為零</strong>,当你的参数对loss微分為零的时候,gradient descent就没有办法再update参数了,这个时候training就停下来了,loss当然就不会再下降了。</p>
<p>​	讲到gradient為零的时候,大家通常脑海中最先浮现的,可能就是==local minima==,所以常有人说做deep learning,用gradient descent会卡在local minima,然后所以gradient descent不work,所以deep learning不work。</p>
<p>​	但是如果有一天你要写,跟deep learning相关paper的时候,你千万不要讲卡在local minima这种事情,别人会觉得你非常没有水準,為什麼</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210314153200619-efa0d46b1a5aca715b196c83904700dc.png" width="627" height="270" class="img_ev3q"></p>
<p>​	因為<strong>不是只有local minima的gradient是零</strong>,还有其他可能会让gradient是零,比如说 ==saddle point==,所谓的saddle point,其实就是gradient是零,但是不是local minima,也不是local maxima的地方,像在右边这个例子裡面 红色的这个点,它在左右这个方向是比较高的,前后这个方向是比较低的,它就像是一个马鞍的形状,所以叫做saddle point,那中文就翻成鞍点</p>
<p>​	像saddle point这种地方,它也是gradient為零,但它不是local minima,那像这种gradient為零的点,统称為==critical point==,所以<strong>你可以说你的loss,没有办法再下降,也许是因為卡在了critical point,但你不能说是卡在local minima,因為saddle point也是微分為零的点</strong></p>
<p>​	但是今天如果你发现你的gradient,真的很靠近零,卡在了某个critical point,我们有没有办法知道,到底是local minima,还是saddle point？其实是有办法的</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210314153005913-e51b0babff860102d3a5a6568150a683.png" width="849" height="523" class="img_ev3q"></p>
<p>​	<strong>為什麼我们想要知道到底是卡在local minima,还是卡在saddle point呢</strong></p>
<ul>
<li>因為如果是<strong>卡在local minima,那可能就没有路可以走了</strong>,因為四周都比较高,你现在所在的位置已经是最低的点,loss最低的点了,往四周走 loss都会比较高,你会不知道怎麼走到其他的地方去</li>
<li>但saddle point就比较没有这个问题,如果你今天是**卡在saddle point的话,saddle point旁边还是有路可以走的,**还是有路可以让你的loss更低的,你只要逃离saddle point,你就有可能让你的loss更低</li>
</ul>
<p>​	所以鉴别今天我们走到,critical point的时候,到底是local minima,还是saddle point,是一个值得去探讨的问题,那怎麼知道今天一个critical point,到底是属於local minima,还是saddle point呢？</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="warning-of-math">Warning of Math<a href="#warning-of-math" class="hash-link" aria-label="Direct link to Warning of Math" title="Direct link to Warning of Math">​</a></h4>
<p>​	这边需要用到一点数学,以下这段其实没有很难的数学,就只是微积分跟线性代数,但如果你没有听懂的话,以下这段skip掉是没有关係的</p>
<p>​	那怎麼知道说一个点,到底是local minima,还是saddle point呢？</p>
<p>​	你要知道我们loss function的形状,可是我们怎麼知道,loss function的形状呢,network本身很复杂,用复杂network算出来的loss function,显然也很复杂,我们怎麼知道loss function,长什麼样子,虽然我们没有办法完整知道,整个loss function的样子</p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="tayler-series-approximation">Tayler Series Approximation<a href="#tayler-series-approximation" class="hash-link" aria-label="Direct link to Tayler Series Approximation" title="Direct link to Tayler Series Approximation">​</a></h5>
<p>​	但是如果给定某一组参数,比如说蓝色的这个<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span>,在<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span>附近的loss function,是有办法被写出来的,它写出来就像是这个样子</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210314154450970-c7f80103da58efd07e47b08cd27cd0d2.png" width="789" height="514" class="img_ev3q"></p>
<p>​	所以这个<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span></span></span></span>完整的样子写不出来,但是它在<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span>附近,你可以用这个式子来表示它,这个式子是,Tayler Series Appoximation泰勒级数展开,这个假设你在微积分的时候,已经学过了,所以我就不会细讲这一串是怎麼来的,但我们就只讲一下它的概念,这一串裡面包含什麼东西呢?</p>
<ul>
<li>
<p>第一项是<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>,就告诉我们说,当<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span></span></span></span>跟<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span>很近的时候,<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span></span></span></span>应该跟<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>还蛮靠近的</p>
</li>
<li>
<p>第二项是<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span></span></span></span></p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210314155508574-27e1082951f77420c7ed7b92f042baf9.png" width="769" height="429" class="img_ev3q"></p>
<p><strong><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span></span></span></span>是一个向量,这个g就是我们的gradient</strong>,我们用绿色的这个g来代表gradient,这个<strong>gradient会来弥补,<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span>跟<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span></span></span></span>之间的差距</strong>,我们虽然刚才说<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span>跟<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span></span></span></span>,它们应该很接近,但是中间还是有一些差距的,那这个差距,第一项我们用这个gradient,来表示他们之间的差距,有时候gradient会写成<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord">∇</span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>,这个地方的<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span></span></span></span>是一个向量,<strong>它的第i个component,就是θ的第i个component对L的微分</strong>,光是看g还是没有办法,完整的描述L(θ),你还要看第三项</p>
</li>
<li>
<p>第三项跟Hessian有关,这边有一个<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span></span></span></span></p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210314155802228-76c449cbb3f12764172505b50f0c4fe6.png" width="775" height="442" class="img_ev3q"></p>
<p>这个<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span></span></span></span>叫做Hessian,它是一个矩阵,这个第三项是,再<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>,所以第三项会再补足,再加上gradient以后,与真正的L(θ)之间的差距.<strong>H裡面放的是L的二次微分</strong>,<strong>它第i个row,第j个column的值,就是把θ的第i个component,对L作微分,再把θ的第j个component,对L作微分,再把θ的第i个component,对L作微分,做两次微分以后的结果</strong> 就是这个<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0813em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span></p>
</li>
</ul>
<p>​	如果这边你觉得有点听不太懂的话,也没有关係,反正你就记得这个<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span></span></span></span>,这个loss function,这个error surface在<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span>附近,可以写成这个样子,这个式子跟两个东西有关係,<strong>跟gradient有关係,跟hessian有关係,gradient就是一次微分,hessian就是裡面有二次微分的项目</strong></p>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="hession">Hession<a href="#hession" class="hash-link" aria-label="Direct link to Hession" title="Direct link to Hession">​</a></h5>
<p>​	那如果我们今天走到了一个critical point,意味著gradient為零,也就是绿色的这一项完全都不见了</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210314160538203-f1dd3227b4128b63af70e3fc8cc3c263.png" width="768" height="485" class="img_ev3q"></p>
<p>​	<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span></span></span></span><strong>是一个zero vector,绿色的这一项完全都不见了</strong>,只剩下红色的这一项,所以当在critical point的时候,这个loss function,它可以被近似為<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>,加上红色的这一项</p>
<p>​	我们可以<strong>根据红色的这一项来判断</strong>,在<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span>附近的error surface,到底长什麼样子</p>
<p>​	知道error surface长什麼样子,我就可以判断</p>
<p>​	<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span>它是一个==local minima==,是一个==local maxima==,还是一个==saddle point==</p>
<p>​	我们可以靠这一项来了解,这个error surface的地貌,大概长什麼样子,知道它地貌长什麼样子,我们就可以知道说,现在是在什麼样的状态,这个是Hessian</p>
<p>​	那我们就来看一下怎麼根据Hessian,怎麼根据红色的这一项,来判断θ&#x27;附近的地貌</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210314161411744-3df4d924d1ffafc38a062677749bd731.png" width="842" height="523" class="img_ev3q"></p>
<p>​	我们现在為了等一下符号方便起见,我们<strong>把<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>用<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>这个向量来表示</strong></p>
<ul>
<li>如果今天对任何可能的<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span><strong>,<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>都大於零</strong>,也就是说 现在θ不管代任何值,v可以是任何的v,也就是θ可以是任何值,不管θ代任何值,<strong>红色框框裡面通通都大於零</strong>,那意味著说 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span></span></span></span>不管代多少 只要在<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span>附近,<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span></span></span></span>都大於<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>,<strong>代表<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>是附近的一个最低点,所以它是local minima</strong></li>
<li>如果今天反过来说,对所有的<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>而言,<strong><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>都小於零,也就是红色框框裡面永远都小於零</strong>,也就是说<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span></span></span></span>不管代什麼值,红色框框裡面都小於零,意味著说<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>,<strong>代表<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>是附近最高的一个点,所以它是local maxima</strong></li>
<li>第三个可能是假设,<strong><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>,有时候大於零 有时候小於零</strong>,你代不同的v进去 代不同的θ进去,红色这个框框裡面有时候大於零,有时候小於零,意味著说在θ&#x27;附近,有时候L(θ)&gt;L(θ&#x27;) 有时候L(θ)&lt;L(θ&#x27;),在L(θ&#x27;)附近,有些地方高 有些地方低,这意味著什麼,<strong>这意味著这是一个saddle point</strong></li>
</ul>
<p>​	但是你这边是说我们要代所有的<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>,去看<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>是大於零,还是小於零.我们怎麼有可能把所有的v,都拿来试试看呢,所以有一个更简便的方法,去确认说这一个条件或这一个条件,会不会发生.</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210314182013101-e25048561cc94d2c0026bde29ec0f0a9.png" width="596" height="416" class="img_ev3q"></p>
<p>​	这个就直接告诉你结论,线性代数理论上是有教过这件事情的,如果今天对所有的v而言,<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>都大於零,那这种矩阵叫做<strong>positive definite 正定矩阵</strong>,positive definite的矩阵,<strong>它所有的eigen value特征值都是正的</strong></p>
<p>​	所以如果你今天算出一个hessian,你不需要把它跟所有的v都乘看看,你只要去直接看这个H的eigen value,如果你发现</p>
<ul>
<li><strong>所有eigen value都是正的</strong>,那就代表说这个条件成立,就<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>,会大於零,也就代表说是一个local minima。所以你从hessian metric可以看出,它是不是local minima,你只要算出hessian metric算完以后,看它的eigen value发现都是正的,它就是local minima。</li>
<li>那反过来说也是一样,如果今天在这个状况,对所有的v而言,<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>小於零,那H是negative definite,那就代表所有<strong>eigen value都是负的</strong>,就保证他是local maxima</li>
<li><strong>那如果eigen value有正有负</strong>,那就代表是saddle point,</li>
</ul>
<p>​	那假设在这裡你没有听得很懂的话,你就可以记得结论,<strong>你只要算出一个东西,这个东西的名字叫做hessian,它是一个矩阵,这个矩阵如果它所有的eigen value,都是正的,那就代表我们现在在local minima,如果它有正有负,就代表在saddle point。</strong></p>
<p>​	那如果刚才讲的,你觉得你没有听得很懂的话,我们这边举一个例子</p>
<p><img decoding="async" loading="lazy" alt="image-20210314183647928" src="/assets/images/image-20210314183647928-bdf2d54d2e390ba8578d66a1926ce495.png" width="356" height="121" class="img_ev3q"></p>
<p>​	我们现在有一个史上最废的network,输入一个x,它只有一个neuron，乘上<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>,而且这个neuron,还没有activation  function,所以x乘上<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>以后 之后就输出,然后再乘上<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 然后就再输出,就得到最终的数据就是y.总之这个function非常的简单</p>
<span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span>
<p>​	我们有一个史上最废的training set,这个data set说,我们只有一笔data,这笔data是x,是1的时候,它的level是1 所以输入1 进去,你希望最终的输出跟1越接近越好</p>
<p>​	而这个史上最废的training,它的error surface,也是有办法直接画出来的,因為反正只有两个参数 w₁ w₂,连bias都没有,假设没有bias,只有w₁跟w₂两个参数,这个network只有两个参数 w₁跟w₂,那我们可以穷举所有w₁跟w₂的数值,算出所有w₁ w₂数值所代来的loss,然后就画出error surface 长这个样</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210314185412324-d1adaee2c62f67f3201840cdac507316.png" width="530" height="426" class="img_ev3q"></p>
<p>​	四个角落loss是高的,好 那这个图上你可以看出来说,有一些critical point,这个黑点点的地方(0,0),<strong>原点的地方是critical point</strong>,然后事实上,<strong>右上三个黑点也是一排critical point,左下三个点也是一排critical point</strong></p>
<p>​	如果你更进一步要分析,他们是saddle point,还是local minima的话,那圆心这个地方,<strong>原点这个地方 它是saddle point</strong>,為什麼它是saddle point呢</p>
<p>​	你往左上这个方向走 loss会变大,往右下这个方向走 loss会变大,往左下这个方向走 loss会变小,往右下这个方向走 loss会变小,它是一个saddle point</p>
<p>​	而这两群critical point,它们都是local minima,所以这个山沟裡面,有一排local minima,这一排山沟里面有一排local minima,然后在原点的地方,有一个saddle point,这个是我们把error surface,暴力所有的参数,得到的loss function以后,得到的loss的值以后,画出error surface,可以得到这样的结论</p>
<p>​	现在假设如果不暴力所有可能的loss,如果要直接算说一个点,是local minima,还是saddle point的话 怎麼算呢</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210314190641535-1cc8f8c8c287352e6a965c1510e3b864.png" width="791" height="409" class="img_ev3q"></p>
<p>​	我们可以把loss的function写出来,这个loss的function 这个L  是</p>
<span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.1141em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord mathnormal">x</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>
<p>​	正确答案 ŷ减掉model的输出,也就是w₁ w₂x,这边取square error,这边<strong>只有一笔data,所以就不会summation over所有的training data</strong>,因為反正只有一笔data,x代1 ŷ代1,我刚才说过只有一笔训练资料最废的,所以只有一笔训练资料,所以loss function就是<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord mathnormal">x</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>,那你可以把这一个loss function,它的gradient 求出来,w₁对L的微分,w₂对L的微分写出来是这个样子</p>
<span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.2074em;vertical-align:-0.836em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:0.05556em">∂</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:0.05556em">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.836em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">2</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mopen">(</span><span class="mord">−</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>
<span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.2074em;vertical-align:-0.836em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:0.05556em">∂</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:0.05556em">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.836em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">2</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mopen">(</span><span class="mord">−</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>
<p>​	这个东西</p>
<span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.6504em;vertical-align:-1.0752em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5752em"><span style="top:-3.6951em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em">∂</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em"><span style="top:-2.357em;margin-left:-0.0269em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em">∂</span><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4451em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-2.3699em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mspace"> </span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em">∂</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em"><span style="top:-2.357em;margin-left:-0.0269em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em">∂</span><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4451em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0752em"><span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">]</span></span></span></span></span></span></span>
<p>​	就是所谓的g,所谓的gradient,什麼时候gradient会零呢,什麼时候会到一个critical point呢?</p>
<p>​	举例来说 如果w₁=0 w₂=0,就在圆心这个地方,如果w₁代0 w₂代0,w₁对L的微分 w₂对L的微分,算出来就都是零 就都是零,这个时候我们就知道说,原点就是一个critical point,但<strong>它是local maxima,它是local maxima,local minima,还是saddle point呢,那你就要看hessian才能够知道了</strong></p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210314192209655-45458a2d22a6386e777288b3ebd3075c.png" width="573" height="441" class="img_ev3q"></p>
<p>​	当然 我们刚才已经暴力所有可能的w₁ w₂了,所以你已经知道说,它显然是一个saddle point,但是现在假设还没有暴力所有可能的loss,所以我们要看看能不能够用H,用Hessian看出它是什麼样的critical point,那怎麼算出这个H呢</p>
<p>​	<strong>H它是一个矩阵,这个矩阵裡面元素就是L的二次微分</strong>,所以这个矩阵裡面第一个row,第一个coloumn的位置,就是w₁对L微分两次,第一个row 第二个coloumn的位置,就是先用w₂对L作微分,再用w₁对L作微分,然后这边就是w₁对L作微分,w₂对L作微分,然后w₂对L微分两次,这四个值组合起来,就是我们的hessian,那这个hessian的值是多少呢</p>
<p>​	这个hessian的式子,我都已经把它写出来了,你只要把w₁=0 w₂=0代进去,代进去 你就得到在原点的地方,hessian是这样的一个矩阵</p>
<span class="katex-display"><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.4em;vertical-align:-0.95em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em"><span style="top:-3.61em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord">0</span></span></span></span><span style="top:-2.41em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mspace"> </span><span class="mord"><span class="mord">−</span><span class="mord">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.95em"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em"></span><span class="arraycolsep" style="width:0.5em"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em"><span style="top:-3.61em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">−</span><span class="mord">2</span></span></span><span style="top:-2.41em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.95em"><span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">]</span></span></span></span></span></span></span>
<p>​	这个hessian告诉我们,它是local minima,还是saddle point呢,那你就要看这个矩阵的eigen value,算一下发现,这个矩阵有两个eigen value,2跟-2 <strong>eigen value有正有负,代表saddle point</strong></p>
<p>​	所以我们现在就是用一个例子,跟你操作一下 告诉你说,你怎麼从hessian看出一个点,它一个critical point 它是saddle point,还是local minima,</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="dont-afraid-of-saddle-point">Don&#x27;t afraid of saddle point<a href="#dont-afraid-of-saddle-point" class="hash-link" aria-label="Direct link to Don&#x27;t afraid of saddle point" title="Direct link to Don&#x27;t afraid of saddle point">​</a></h3>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210314195855327-7394a6b089954c85867789d5c4783f23.png" width="564" height="187" class="img_ev3q"></p>
<p>​	如果今天你卡的地方是saddle point,也许你就不用那麼害怕了,因為如果你今天你发现,你停下来的时候,是因為saddle point 停下来了,那其实就有机会可以放心了</p>
<p>​	因為H它不只可以帮助我们判断,现在是不是在一个saddle point,它还指出了我们参数,可以update的方向,就之前我们参数update的时候,都是看gradient 看g,但是我们走到某个地方以后,发现g变成0了 不能再看g了,g不见了 gradient没有了,但如果是一个saddle point的话,还可以再看H,怎麼再看H呢,H怎麼告诉我们,怎麼update参数呢</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210314200048825-2dd1c13cc40face6f83fe3f618291876.png" width="562" height="399" class="img_ev3q"></p>
<p>​	我们这边假设<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">μ</span></span></span></span>是H的eigenvector特征向量,然后<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">λ</span></span></span></span>是u的eigen value特征值。</p>
<p>​	如果我们把这边的<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>换成<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">μ</span></span></span></span>的话,我们把<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">μ</span></span></span></span>乘在H的左边,跟H的右边,也就是<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0358em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mord mathnormal">μ</span></span></span></span>,  <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mord mathnormal">μ</span></span></span></span>会得到<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal">λ</span><span class="mord mathnormal">μ</span></span></span></span>，因為<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">μ</span></span></span></span>是一个eigen vector。H乘上eigen vector特征向量会得到特征向量λ eigen value乘上eigen vector即<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal">λ</span><span class="mord mathnormal">μ</span></span></span></span></p>
<p><img decoding="async" loading="lazy" alt="image-20210314201708726" src="data:image/png;base64,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" width="236" height="35" class="img_ev3q"></p>
<p>​	所以我们在这边得到uᵀ乘上λu,然后再整理一下,把uᵀ跟u乘起来,得到‖u‖²,所以得到λ‖u‖²</p>
<p><img decoding="async" loading="lazy" alt="image-20210314202136510" src="data:image/png;base64,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" width="236" height="95" class="img_ev3q"></p>
<p>​	假设我们这边v,代的是一个eigen vector,我们这边θ减θ&#x27;,放的是一个eigen vector的话,会发现说我们这个红色的项裡面,其实就是λ‖u‖²</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210314200048825-2dd1c13cc40face6f83fe3f618291876.png" width="562" height="399" class="img_ev3q"></p>
<p>​	那今天如果λ<strong>小於零</strong>,eigen value小於零的话,那λ‖u‖²就会小於零,因為‖u‖²一定是正的,所以eigen value是负的,那这一整项就会是<strong>负的</strong>,也就是u的transpose乘上H乘上u,它是负的,也就是<strong>红色这个框裡是负的</strong></p>
<p>​	所以这意思是说假设<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">μ</span></span></span></span>,那这一项<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>就是负的,也就是<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p>
<p>​	也就是说假设<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">μ</span></span></span></span>,也就是,<strong>你在θ&#x27;的位置加上u,沿著u的方向做update得到θ,你就可以让loss变小</strong></p>
<p>​	因為根据这个式子,你只要θ减θ&#x27;等於u,loss就会变小,所以你今天只要让θ等於θ&#x27;加u,你就可以让loss变小,你只要沿著u,也就是eigen vector的方向,去更新你的参数 去改变你的参数,你就可以让loss变小了</p>
<p>​	所以虽然在critical point没有gradient,如果我们今天是在一个<strong>saddle point</strong>,你也不一定要惊慌,你只要<strong>找出负的eigen value,再找出它对应的eigen vector,用这个eigen vector去加θ&#x27;,就可以找到一个新的点,这个点的loss比原来还要低</strong></p>
<p>​	举具体的例子</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210314203757805-77c0e71023ec8896d0f4731c616dee12.png" width="581" height="445" class="img_ev3q"></p>
<p>​	刚才我们已经发现,原点是一个critical point,它的Hessian长这个样,那我现在发现说,这个Hessian有一个负的eigen value,这个eigen value等於-2,那它对应的eigen vector,它有很多个,其实是无穷多个对应的eigen vector,我们就取一个出来,我们取<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.4em;vertical-align:-0.95em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em"><span style="top:-3.61em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord">1</span></span></span></span><span style="top:-2.41em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mspace"> </span><span class="mord"><span class="mord">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.95em"><span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">]</span></span></span></span></span></span>是它对应的一个eigen vector,那我们其实只要顺著这个u的方向,顺著<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.4em;vertical-align:-0.95em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em"><span style="top:-3.61em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord">1</span></span></span></span><span style="top:-2.41em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mspace"> </span><span class="mord"><span class="mord">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.95em"><span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">]</span></span></span></span></span></span>这个vector的方向,去更新我们的参数,就可以找到一个,比saddle point的loss还要更低  的点</p>
<p>​	如果以今天这个例子来看的话,你的saddle point在(0,0)这个地方,你在这个地方会没有gradient,Hessian的eigen vector告诉我们,只要往<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.4em;vertical-align:-0.95em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em"><span style="top:-3.61em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord">1</span></span></span></span><span style="top:-2.41em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mspace"> </span><span class="mord"><span class="mord">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.95em"><span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">]</span></span></span></span></span></span>的方向更新,你就可以让loss变得更小,也就是说你可以逃离你的saddle point,然后让你的loss变小,所以从这个角度来看,似乎saddle point并没有那麼可怕</p>
<p>​	如果你今天在training的时候,你的gradient你的训练停下来,你的gradient变成零,你的训练停下来,是因為saddle point的话,那似乎还有解</p>
<p>​	<strong>但是当然实际上,在实际的implementation裡面,你几乎不会真的把Hessian算出来</strong>,这个要是二次微分,要计算这个矩阵的computation,需要的运算量非常非常的大,更遑论你还要把它的eigen value,跟 eigen vector找出来,所以在实作上,你几乎没有看到,有人用这一个方法来逃离saddle point</p>
<p>​	等一下我们会讲其他,也有机会逃离saddle point的方法,他们的运算量都比要算这个H,还要小很多,那今天之所以我们把,这个saddle point跟 eigen vector,跟Hessian的eigen vector拿出来讲,是想要告诉你说,如果是卡在saddle point,也许没有那麼可怕,最糟的状况下你还有这一招,可以告诉你要往哪一个方向走.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="saddle-point-vs-local-minima">Saddle Point v.s. Local Minima<a href="#saddle-point-vs-local-minima" class="hash-link" aria-label="Direct link to Saddle Point v.s. Local Minima" title="Direct link to Saddle Point v.s. Local Minima">​</a></h3>
<p>​	讲到这边你就会有一个问题了,这个问题是,那到底<strong>saddle point跟local minima,谁比较常见呢</strong>,我们说,saddle point其实并没有很可怕,那如果我们今天,常遇到的是saddle point,比较少遇到local minima,那就太好了,那到底saddle point跟local minima,哪一个比较常见呢?这边我们要讲一个不相干的故事,先讲一个故事</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210314204619093-a1f9942ab3994571259eb8a60f1a1038.png" width="541" height="335" class="img_ev3q"></p>
<p>​	这个故事发生在1543年,1543年发生了什麼事呢,那一年君士坦丁堡沦陷,这个是君士坦丁堡沦陷图,君士坦丁堡本来是东罗马帝国的领土,然后被鄂图曼土耳其帝国佔领了,然后东罗马帝国就灭亡了,在鄂图曼土耳其人进攻,君士坦丁堡的时候,那时候东罗马帝国的国王,是君士坦丁十一世,他不知道要怎麼对抗土耳其人,有人就献上了一策,找来了一个魔法师叫做狄奥伦娜</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210314204656457-3c6e30bc0d601b2994068c4a370f26b5.png" width="795" height="614" class="img_ev3q"></p>
<p>​	这是真实的故事,出自三体的故事,这个狄奥伦娜这样说,狄奥伦娜是谁呢,他有一个能力跟张飞一样,张飞不是可以万军从中取上将首级,如探囊取物吗,狄奥伦娜也是一样,他可以直接取得那个苏丹的头,他可以从万军中取得苏丹的头,大家想说狄奥伦娜怎麼这麼厉害,他真的有这麼强大的魔法吗,所以大家就要狄奥伦娜,先展示一下他的力量,这时候狄奥伦娜就拿出了一个圣杯,大家看到这个圣杯就大吃一惊,為什麼大家看到这个圣杯,要大吃一惊呢,因為这个圣杯,本来是放在圣索菲亚大教堂的地下室,而且它是被放在一个石棺裡面,这个石棺是密封的,没有人可以打开它.</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210314204743928-7d83009060ee82b9733dc187c8f9a255.png" width="798" height="484" class="img_ev3q"></p>
<p>​	但是狄奥伦娜他从裡面取得了圣杯,而且还放了一串葡萄进去,君士坦丁十一世為了要验证,狄奥伦娜是不是真的有这个能力,就带了一堆人真的去撬开了这个石棺,发现圣杯真的被拿走了,裡面真的有一串新鲜的葡萄,就知道狄奥伦娜真的有,这个万军从中取上将首级的能力,那為什麼迪奥伦娜可以做到这些事呢,那是因為这个石棺你觉得它是封闭的,那是因為你是从三维的空间来看,从三维的空间来看,这个石棺是封闭的,没有任何路可以进去,但是狄奥伦娜可以进入四维的空间,从高维的空间中,这个石棺是有路可以进去的,它并不是封闭的,至於狄奥伦娜有没有成功刺杀苏丹呢,你可以想像一定是没有嘛,所以君坦丁堡才沦陷,那至於為什麼没有,大家请见於三体这样 就不雷大家,</p>
<p>​	总之这个<strong>从三维的空间来看,是没有路可以走的东西,在高维的空间中是有路可以走的,error surface会不会也一样呢</strong></p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210314205016598-a33f18e6761a375e17c48d74742a296e.png" width="555" height="353" class="img_ev3q"></p>
<p>​	所以你在一维的空间中,一维的一个参数的error surface,你会觉得好像到 处都是local minima,但是会不会在二维空间来看,它就只是一个saddle point呢,常常会有人画类似这样的图,告诉你说Deep Learning的训练,是非常的复杂的,如果我们移动某两个参数,error surface的变化非常的复杂,是这个样子的,那显然它有非常多的local minima,我的这边现在有一个local minima,但是会不会这个local minima,只是在二维的空间中,看起来是一个local minima,在更高维的空间中,它看起来就是saddle point,在二维的空间中,我们没有路可以走,那会不会在更高的维度上,因為更高的维度,我们没办法visualize它,我们没办法真的拿出来看,会不会在更高维的空间中,其实有路可以走的,那如果维度越高,是不是可以走的路就越多了呢,所以 今天我们在训练,一个network的时候,我们的参数往往动輒百万千万以上,所以我们的error surface,其实是在一个非常高的维度中,对不对,我们参数有多少,就代表我们的error surface的,维度有多少,参数是一千万 就代表error surface,它的维度是一千万,竟然维度这麼高,会不会其实,根本就有非常多的路可以走呢,那既然有非常多的路可以走,会不会其实local minima,根本就很少呢,</p>
<p>​	而经验上,如果你自己做一些实验的话,也支持这个假说</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210314205517453-4041f8f134ae6bd85e57ff7eb4ae0ac5.png" width="556" height="435" class="img_ev3q"></p>
<p>​	这边是训练某一个network的结果,每一个点代表,训练那个network训练完之后,把它的Hessian拿出来进行计算,所以这边的每一个点,都代表一个network,就我们训练某一个network,然后把它训练训练,训练到gradient很小,卡在critical point,把那组参数出来分析,看看它比较像是saddle point,还是比较像是local minima</p>
<ul>
<li>纵轴代表training的时候的loss,就是我们今天卡住了,那个loss没办法再下降了,那个loss是多少,  那很多时候,你的loss在还很高的时候,训练就不动了 就卡在critical point,那很多时候loss可以降得很低,才卡在critical point,这是纵轴的部分</li>
<li>横轴的部分是minimum ratio,minimum ratio是<strong>eigen value的数目分之正的eigen value的数目</strong>,又<strong>如果所有的eigen value都是正的,代表我们今天的critical point,是local minima,如果有正有负代表saddle point</strong>,那在实作上你会发现说,你几乎找不到完全所有eigen value都是正的critical point,你看这边这个例子裡面,这个minimum ratio代表eigen value的数目分之正的eigen value的数目,最大也不过0.5到0.6间而已,代表说只有一半的eigen value是正的,还有一半的eigen value是负的,</li>
</ul>
<p>​	所以今天虽然在这个图上,越往右代表我们的critical point越像local minima,<strong>但是它们都没有真的,变成local minima</strong>,就算是在最极端的状况,我们仍然有一半的case,我们的eigen value是负的,这一半的case eigen value是正的,代表说在所有的维度裡面有一半的路,这一半的路 如果要让loss上升,还有一半的路可以让loss下降。</p>
<p>​	所以从经验上看起来,其实local minima并没有那麼常见,多数的时候,你觉得你train到一个地方,你gradient真的很小,然后所以你的参数不再update了,往往是因為你卡在了一个saddle point。</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#critical-point" class="table-of-contents__link toc-highlight">Critical Point</a><ul><li><a href="#training-fails-because" class="table-of-contents__link toc-highlight">Training Fails because</a></li><li><a href="#dont-afraid-of-saddle-point" class="table-of-contents__link toc-highlight">Don&#39;t afraid of saddle point</a></li><li><a href="#saddle-point-vs-local-minima" class="table-of-contents__link toc-highlight">Saddle Point v.s. Local Minima</a></li></ul></li></ul></div></div></div></div></main></div></div></div></div>
</body>
</html>