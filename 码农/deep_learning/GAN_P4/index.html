<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-码农/deep_learning/GAN_P4" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">GAN_P4 Learning from Unpaired Data | Coisini</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://doc.minddiy.top/码农/deep_learning/GAN_P4/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="GAN_P4 Learning from Unpaired Data | Coisini"><meta data-rh="true" name="description" content="有关GAN的最后一段,是一个GAN的神奇应用,它把GAN用在==unsupervised Learning==上,到目前為止,我们讲的几乎都是==Supervised Learning=="><meta data-rh="true" property="og:description" content="有关GAN的最后一段,是一个GAN的神奇应用,它把GAN用在==unsupervised Learning==上,到目前為止,我们讲的几乎都是==Supervised Learning=="><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://doc.minddiy.top/码农/deep_learning/GAN_P4/"><link data-rh="true" rel="alternate" href="https://doc.minddiy.top/码农/deep_learning/GAN_P4/" hreflang="en"><link data-rh="true" rel="alternate" href="https://doc.minddiy.top/码农/deep_learning/GAN_P4/" hreflang="x-default"><meta name="google-site-verification" content="1FUPX6Qo4y3ecU623ShEurhgnjhSTjK49rRMhEDlzFA">
<link rel="stylesheet" href="/katex/katex.min.css">
<script src="/js/matomo.js" async defer="defer"></script><link rel="stylesheet" href="/assets/css/styles.79037026.css">
<script src="/assets/js/runtime~main.468f2b27.js" defer="defer"></script>
<script src="/assets/js/main.4763ab3e.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Chialisp Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="Chialisp Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Coisini</b></a></div><div class="navbar__items navbar__items--right"><a href="https://minddiy.top" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Main site<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input" value=""><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><main class="docMainContainer_TBSr docMainContainerEnhanced_lQrH"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>GAN_P4 Learning from Unpaired Data</h1></header>
<p>有关GAN的最后一段,是一个GAN的神奇应用,它把GAN用在==unsupervised Learning==上,到目前為止,我们讲的几乎都是==Supervised Learning==</p>
<p><img decoding="async" loading="lazy" src="data:image/png;base64,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" width="443" height="133" class="img_ev3q"></p>
<p>我们要训练一个Network,Network的输入叫做X输出叫做Y,我们需要<strong>成对的资料</strong>,才有办法训练这样子的Network,</p>
<p>但是你可能会遇到一个状况是,我们有一堆X我们有一堆Y,但<strong>X跟Y是不成对</strong>的,在这种状况下,我们有没有办法拿这样的资料,来训练Network呢,像这一种没有成对的资料,我们就叫做==unlabeled==的资料</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210524133959416-a5edb8248eba274269c377a094de9691.png" width="658" height="437" class="img_ev3q"></p>
<p>其实在作业三跟作业五裡面,都提供给你两个例子,我们就把这个怎麼用,没有标註的资料,怎麼做S==emi-supervised Learning==,这件事情放在作业裡面,如果你有兴致的话就可以来,体验一下semi-supervised Learning,到底可以带多大的帮助</p>
<p>但是不管是作业三的pseudo labeling,还是作业五的back translation,这些方法或多或少,都<strong>还是需要一些成对的资料</strong></p>
<p>在作业三裡面,你得先训练出一个模型,这个模型可以帮你提供pseudo label,如果你一开始,根本就没有太多有标註的资料,你的模型很差,你根本就没有办法產生,比较好的pseudo label,或是back translation,你也得有一个,back translation 的model,你才办法做back translation,所以不管是作业三,还是作业五的方法,还是都需要一些成对的资料</p>
<p>但是假设我们遇到,一个更艰鉅的状况,是我们<strong>一点成对的资料都没有</strong>,那要什麼怎麼办呢？</p>
<p>我们这边举一个例子,<strong>影像风格转换</strong>,假设今天我要训练,一个Deep Network,它要做的事情是把X domain的图,X domain的图,我们假设是真人的照片,Y domain的图是二次元人物的头像</p>
<img decoding="async" loading="lazy" src="https://github.com/unclestrong/DeepLearning_LHY21_Notes/blob/master/Notes_pic/image-20210524134333519.png?raw=true" alt="image-20210524134333519" style="zoom:50%" class="img_ev3q">
<p>在这个例子裡面我们可能,就<strong>没有任何的成对的资料</strong>，在这种状况下,还有没有办法训练一个Network,输入一个X產生一个Y呢,这个就是GAN可以帮我们做的事情</p>
<p>那接下来我们就是看看怎麼用GAN,在这种完全没有成对资料的情况下,进行学习</p>
<p>这个是我们之前在讲,unconditional的generation的时候,你看到的generator的架构</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210524134830962-31db72d5354342f26fdea445eb6a1a7f.png" width="712" height="328" class="img_ev3q"></p>
<p>输入是一个Gaussian的分佈,输出可能是一个复杂的分佈</p>
<p>现在我们在稍微,转换一下我们的想法,<strong>输入</strong>我们不说它是Gaussian的分佈,我们说它是<strong>X domain的图片的分佈</strong>,那<strong>输出</strong>我们说,是<strong>Y domain图片的分佈</strong></p>
<img decoding="async" loading="lazy" src="https://github.com/unclestrong/DeepLearning_LHY21_Notes/blob/master/Notes_pic/image-20210524134901281.png?raw=true" alt="image-20210524134901281" style="zoom:50%" class="img_ev3q">
<p>乍听之下好像没有很难,你完全可以<strong>套用原来的GAN的想法</strong>,在原来的GAN裡面我们说,我们从Gaussian sample一个向量,丢到Generator裡面</p>
<p>那我们一开始也说,其实不一定只要Gaussian sample这一个distribution,只要是有办法被sample的就行了,我们选Gaussian只是因為Gaussian的formulation我们知道</p>
<p>那我们现在如果,输入是X domain的distribution,我们只要改成可以,从X domain sample就结束了,那你有没有办法,从X domain sample呢</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210524140340298-fac8ac2f87e418d86d74ce9eb4713202.png" width="541" height="168" class="img_ev3q"></p>
<p>可以 你就从人脸的照片裡面,真实的人脸裡面随便挑一张出来,这是一个死臭酸宅(老师本人)然后就结束了,你就可以从X domain,sample照片出来,你把这个照片丢到generator裡面,让它產生另外一张图片,產生另外一个distribution裡面的图片</p>
<p>那怎麼让它变成,是Y domain的distribution呢？</p>
<p>那就要两三个discriminator,那这个discriminator给它,看过很多Y domain的图,所以它能够分辨Y domain的图,跟不是Y domain的图的差异</p>
<img decoding="async" loading="lazy" src="https://github.com/unclestrong/DeepLearning_LHY21_Notes/blob/master/Notes_pic/image-20210524140458705.png?raw=true" alt="image-20210524140458705" style="zoom:50%" class="img_ev3q">
<p>看到<strong>Y domain的图</strong>就给它<strong>高分</strong>,看到<strong>不是Y domain的图</strong>,不是二次元人物就给它<strong>低分</strong>,那就这样结束了</p>
<p>但是光是套用原来的GAN训练,generator跟discriminator,好像是<strong>不够的</strong>,因為我们现在的discriminator,它要做的事情是要让这个generator,输出一张Y domain的图</p>
<p>那generator它可能真的,可以学到输出Y domain的图,但是它输出的Y domain的图,一定要<strong>跟输入有关係吗</strong>,你<strong>没有任何的限制</strong>要求你的generator做这件事</p>
<p>你的generator也许就把这张图片,当作一个Gaussian的noise,然后反正它就是看到,不管你输入什麼它都无视它,反正它就输出一个,像是二次元人物的图片,discriminator觉得它做得很好,其实就结束了</p>
<img decoding="async" loading="lazy" src="https://github.com/unclestrong/DeepLearning_LHY21_Notes/blob/master/Notes_pic/image-20210524141004712.png?raw=true" alt="image-20210524141004712" style="zoom:50%" class="img_ev3q">
<p>所以如果我们完全只套用,这个一般的GAN的做法,只训练一个generator,这个generator input的distribution,从Gaussian变成X domain的image,然后训练一个discriminator,显然是不够的,因為你训练出来的generator,它可以<strong>產生二次元人物的头像,但是跟输入的真实的照片,没有什麼特别的关係</strong>,那这个不是我们要的,</p>
<p>我们在conditional GAN的时候,是不是也看过一模一样的问题呢,在讲conditional GAN的时候,我有特别提到说,假设你的discriminator只看Y,那它可能会无视generator的输入,那產生出来的结果不是我们要的,但是这边啊,如果我们要<strong>从unpaired的data学习,我们也没有办法,直接套用conditional GAN的想法</strong>,因為在刚才讲的,<strong>conditional GAN裡面,我们是有成对的资料</strong>,来训练的discriminator</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="cycle-gan">Cycle GAN<a href="#cycle-gan" class="hash-link" aria-label="Direct link to Cycle GAN" title="Direct link to Cycle GAN">​</a></h2>
<p>这边这个想法叫做==Cycle GAN==,在Cycle GAN裡面,我们会train<strong>两个generator</strong></p>
<ul>
<li>第一个generator它的工作是,把X domain的图变成Y domain的图</li>
<li>第二个generator它的工作是,看到一张Y domain的图,把它<strong>还原</strong>回X domain的图</li>
</ul>
<img decoding="async" loading="lazy" src="https://github.com/unclestrong/DeepLearning_LHY21_Notes/blob/master/Notes_pic/image-20210524141830620.png?raw=true" alt="image-20210524141830620" style="zoom:50%" class="img_ev3q">
<p>在训练的时候,我们今天增加了一个额外的目标,就是我们希望输入一张图片,从X domain转成Y domain以后,要从Y domain转回原来,一模一样的X domain的图,经过两次转换以后,<strong>输入跟输出要越接近越好</strong></p>
<p>你说怎麼让两张图片越接近越好呢？</p>
<p><strong>两张图片</strong>就是<strong>两个向量</strong>,这两个向量之间的距离,你就是让这两个向量,它们之间的距离越接近越好,就是要两张图片越像越好</p>
<p>因為这边有一个循环,从X到Y 在从Y回到X,它是一个cycle,所以叫做Cycle GAN,这个要让输入经过两次转换以后,变成输出  输入跟输出越接近越好,这个叫做==Cycle的consistency==</p>
<p>所以现在这边我们有<strong>三个Network</strong></p>
<ol>
<li>第一个generator,它的工作是把X转成Y</li>
<li>第二个generator,它的工作是要把Y还原回原来的X</li>
<li>那这个discriminator,它的工作仍然是要看,蓝色的这个generator它的输出,像不像是Y domain的图</li>
</ol>
<p>那<strong>加入了这个橙色的</strong>从Y到X的generator以后,对於前面这个蓝色的generator来说,它就再也不<strong>能够随便乱做</strong>了,它就不能够随便產生乱七八糟,跟输入没有关係的人脸了</p>
<p>这边假设输入一个死臭酸宅,这边假设输出的是辉夜</p>
<img decoding="async" loading="lazy" src="https://github.com/unclestrong/DeepLearning_LHY21_Notes/blob/master/Notes_pic/image-20210524142659180.png?raw=true" alt="image-20210524142659180" style="zoom:50%" class="img_ev3q">
<p>另外一个这个不知道这是谁的,然后对第二个generator来说,它就是视这张辉夜作為输入,它根本无法想像说,要把辉夜还原回死臭酸宅,它根本不知道说,原来输入的图片长什麼样子</p>
<p>所以对第一个generator来说,為了要让第二个generator能够,成功的还原原来的图片,它產生出来的图片,就不能跟输入差太多,所以这边是一个死臭酸宅,这边输出至少也得是一个,戴眼镜的男生的角色才行</p>
<img decoding="async" loading="lazy" src="https://github.com/unclestrong/DeepLearning_LHY21_Notes/blob/master/Notes_pic/image-20210524142737091.png?raw=true" alt="image-20210524142737091" style="zoom:50%" class="img_ev3q">
<p>所以这边是一个戴眼镜男生的角色,然后第二个generator才能够,把这个角色还原回原来的输入,所以如果你加Cycle GAN,你至少可以强迫你的generator,它输出的Y domain的图片,至少跟输入的X domain的图片,有一些关係</p>
<p>这时你可能会有的一个问题就是,你这边只保证有一些关係啊,<strong>你怎麼知道这个关係是我们要的呢</strong>？</p>
<ul>
<li>机器有没有可能学到很奇怪的转换,输入一个戴眼镜的人,然后这个generator学到的是,看到眼镜就把眼镜抹掉,然后把它变成一颗痣,然后第二个generator橙色的学到的,就是看到痣就还原回眼镜,这样还是可以满足cycle consistency,还是可以把输入的图片,变成输出的图片</li>
<li>一个更极端的例子,假设第一个generator学到的就是,把图片反转 左右翻转,第二个generator它也只要学到,把图片左右翻转,你就可以还原了啊</li>
</ul>
<p>所以今天如果我们做Cycle GAN,用cycle consistency,似乎没有办法保证,我们输入跟输出的人脸,看起来真的很像,因為也许机器会学到很奇怪的转换,反正只要第二个generator,可以转得回来就好了</p>
<p><strong>确实有可能有这样的问题发生</strong>,而且<strong>目前没有什麼特别好的解法</strong></p>
<p>但我可以告诉你说,实际上你要使用Cycle GAN的时候,<strong>这样子的状况没有那麼容易出现</strong>,如果你实际上使用Cycle GAN,你会发现输入跟输出往往,真的就会看起来非常像,而且甚至在实作上,在实作的经验上,你就算没有第二个generator,你不用cycle GAN,拿一般的GAN来做,这种图片风格转换的任务,你往往也做得起来</p>
<p>因為在实作上你会发现,<strong>Network其实非常懒惰</strong>,它输入一个图片,它往往就想输出,by default就是想输出很像的东西,它不太想把输入的图片,做太复杂的转换,像是什麼眼镜变成一颗痣这种状况,它不爱这麼麻烦的东西,有眼镜就输出眼镜,可能对它来说是比较容易的抉择,所以在真的实作上,这个问题没有很大,输入跟输出会是像,但是理论上好像没有什麼保证说,输入跟输出的图片一定要很像,就算你加了cycle consistency</p>
<p>所以这个是实作与理论上,你可能会遇到的差异,总之虽然Cycle GAN没有保证说,输入跟输  出一定很像,但实际上你会发现输入跟输出,往往非常像,你只是改变了风格而已</p>
<p>那这个<strong>Cycle GAN可以是双向的</strong>,我们刚才有一个generator,输入Y domain的图片,输出X domain的图片,我们是先把X domain的图片转成Y,在把Y转回X</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210524143232414-bbb25e426629cdafab2e7ef68632fa75.png" width="873" height="520" class="img_ev3q"></p>
<p>在训练cycle GAN的时候,你可以同时做另外一个方向的训练,也就是</p>
<ul>
<li>把这个橙色的generator拿来,给它Y domain的图片,让它產生X domain的图片</li>
<li>然后在把蓝色的generator拿来,把X domain的图片,还原回原来Y domain的图片</li>
</ul>
<p>那你依然要让,<strong>输入跟输出越接近越好</strong>,那你一样要训练一个discriminator,这个discriminator是,X domain的discriminator,它是要看一张图片,像不像是真实人脸的discriminator,这个discriminator要去看说,这一个橙色的generator的输出,像不像是真实的人脸,这个橙色的generator它要去骗过,这个Dx这个绿色的左边,这一个discriminator,这个<strong>合起来就是Cycle GAN</strong></p>
<p>那除了Cycle GAN以外,你可能也听过很多其他的,可以做风格转换的GAN</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210524143654721-38c5eac1c47a6bae27b481f312acb965.png" width="864" height="617" class="img_ev3q"></p>
<p>比如说Disco GAN 比如说Dual GAN,他们跟Cycle GAN有什麼不同呢,就是没有半毛钱的不同这样子</p>
<p>你可以发现Disco GAN,Dual GAN跟Cycle GAN,其实是一样的东西,他们是一样的想法,神奇的事情是完全不同的团队,在几乎一样的时间,提出了几乎一模一样的想法,你发现这三篇文章,放到arxiv上的时间,都是17年的3月,17年的4月跟17年的3月</p>
<p>除了Cycle GAN以外,还有另外一个更进阶的,可以做影像风格转换的版本,叫做StarGAN</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210524143818480-8fb84043306698f9c4c152a49b68408a.png" width="871" height="637" class="img_ev3q"></p>
<p>Cycle GAN只能在两种风格间做转换,那StarGAN 它厉害的地方是,它可以在多种风格间做转换,不过这个就不是我们接下来,想要细讲的重点</p>
<p>这个真实的人脸转二次元的任务,实际上能不能做呢,实际上可以做了</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210524143955674-c839d30734860b12d8c8cffbe57b95c2.png" width="817" height="363" class="img_ev3q"></p>
<p>右上角这边放了一个连结,这个应该是一个韩国团队,他们做了一个网站,你可以上传一张图片,它可以帮你变成二次元的人物,他们实际上用的不是Cycle GAN啦,他们用的也是GAN的技术,但是是一个进阶版的东西,那我们这边就不细讲,我就把论文的连结,放在这边给大家参考</p>
<p>我就实际测试了一下,这个不知道大家认不认得,这是新垣结衣,这个是你老婆这样,你总该认得吧,把这个图片转成,把你老婆转成二次元的人物,长成是这个样子,你老婆二次元长这个样子知道吗</p>
<p>你会发现说机器确实,有学到一些二次元人物的特徵,比如说 把眼睛变大,本来眼睛其实没有很大,变成二次元人物之后,眼睛变这麼大,但有时候也是会失败</p>
<img decoding="async" loading="lazy" src="https://github.com/unclestrong/DeepLearning_LHY21_Notes/blob/master/Notes_pic/image-20210524144030930.png?raw=true" alt="image-20210524144030930" style="zoom:50%" class="img_ev3q">
<p>比如说 这个是美国前总统,转完以后变成这个样子,两隻眼睛一眼大一眼小就是了,它不是总是会成功的</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="text-style-transfer"><strong>Text Style Transfer</strong><a href="#text-style-transfer" class="hash-link" aria-label="Direct link to text-style-transfer" title="Direct link to text-style-transfer">​</a></h2>
<p>那同样的技术不是只能用在影像上,也可以用在文字上,你也可以做<strong>文字风格的转换</strong></p>
<p>比如说,把一句负面的句子转成正面的句子,当然如果你要做一个模型,输入一个句子输出的句子,这个模型就是要能够,吃一个sequence 输出一个sequence,所以它等於是一个,sequence to sequence的model</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210524145220024-58a0be2cdb110f29cbd8896f3f7f3f49.png" width="867" height="520" class="img_ev3q"></p>
<p>你可能就会用到,我们在作业五裡面的,Transformer的架构,来做这个文字风格转换的问题,我们在作业五做的是翻译嘛,输入一个语言输出另外一个语言嘛</p>
<p>现在如果要做文字风格转换,就是<strong>输入一个句子</strong>,<strong>输出另外一个风格的句子</strong></p>
<p>怎麼做文字的风格转换呢,跟Cycle GAN是一模一样的,首先你要有训练资料,<strong>收集一大堆负面的句子,收集一大堆正面的句子</strong></p>
<p>这个其实没有那麼难收集,你可以就是网路上爬一爬,像我们就是去PTT上爬,然后只要是推文就当作是正面的,嘘文就当作是负面的,就有一大堆正面的句子,跟负面的句子,只是成对的资料没有而已,你不知道这句推文,要怎麼转成这句嘘文,这些嘘文要怎麼转成这句推文,你没有这种资料,但是一堆推文一堆嘘文的资料,你总是可以找得到的</p>
<p>那接下来呢,完全套用Cycle GAN的方法,完全没有任何不同</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210524145344402-eb696cf85807b7f9282accde8bc982bb.png" width="873" height="597" class="img_ev3q"></p>
<p>这边就不需要再细讲 很快讲过</p>
<p>有一个discriminator,discriminator要看说,假设我们是要负面的句子,转正面的句子,discriminator要看说,现在generator的输出,像不像是真正的正面的句子</p>
<p>然后我们还要有另外一个generator,要 有<strong>两个generator</strong>,这个generator要学会,把<strong>正面的句子转回原来负面的句子</strong>,你要用Cycle consistency,负面的句子转成正面的以后,还可以<strong>转回原来负面的句子</strong></p>
<p>你可能会问说,这两个句子 它们两个是句子啊,怎麼算它们的相似度啊？</p>
<p>图片还比较好理解,图片就是个向量啊,两个向量的距离就是它们的相似度,那两个句子要怎麼做呢,这个如果你有兴趣,在留给你慢慢研究,那这边还有另外一个问题就是,这个sequence to sequence model,输出是文字,可是刚才不是有讲说,如果输出是文字,接到discriminator会有问题吗,对 会有问题 这边你就要用RL硬做</p>
<p>那做出来的结果怎麼样呢,这个是真正的demo，就是真的拿PTT的推文,当正面的句子,嘘文当负面的句子,那你就可以给它一个负面的句子,它就帮你转成正面的句子,做起来像是这个样子</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210524145626379-1ed88376269568a879f1b730a5d431dd.png" width="824" height="565" class="img_ev3q"></p>
<p>,你可能问说这个系统有什麼用,就是没有任何用处 没半点用处,但是如果你觉得,你的老闆说话特别坏的话,就可以把这个系统,装在你的耳机裡面,把所有的负面的句子,转成正面的句子,你的人生可能就会,过得特别快乐一点</p>
<p>那其实像这一种文字风格转换,还有很多其他的应用,<strong>不是只有正面句子转负面句子</strong></p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210524145920831-91941a82d235e362238528719fd01a1b.png" width="809" height="571" class="img_ev3q"></p>
<p>举例来说 假设我有很多长的文章,我有另外一堆摘要,这些摘要不是这些长的文章的摘要,是不同的来源,一堆长的文章 一堆摘要,让机器学习文字风格的转换,你可以让机器学会<strong>把长的文章,变成简短的摘要</strong>,让 它学会怎麼精简的写作,让它学会把长的文章变成短的句子</p>
<p>甚至还有更狂的,同样的想法可以做,unsupervised的翻译,什麼叫做unsupervised的翻译呢,收集一堆英文的句子,收集一堆中文的句子,没有任何成对的资料,这就跟你作业五不一样,作业五你有成对的资料嘛,你有知道说这句英文对到这句中文,但是unsupervised翻译就是,完全不用任何成对的资料,网路上爬一堆中文,网路上爬一堆英文,用刚才那个**Cycle GAN的做法硬做,机器就可以学会把中文翻成英文了,**你可以自己看一下文献,看看说机器做得怎麼样</p>
<p>到目前為止,我们说的两种风格都还是文字,可不可以两种风格,甚至是<strong>不同类型的资料</strong>呢,有可能做,这是我们实验室是最早做的,我们试图去做<strong>非督导式的语音辨识</strong>,,语音辨识就是,你需要收集成对的资料啊,你需要收集一大堆的声音讯号,然后找工读生,帮你把这些声音讯号标註,机器才能够学会,某个语言的语音辨识,但是要标註资料所费不貲,所以我们想要挑战,非督导式的语音辨识,也就是机器只听了一堆声音,这些声音没有对应的文字,机器上网爬一堆文字,这些文字没有对应的声音,然后用Cycle GAN硬做,看看机器有没有办法,把声音转成文字,看看它的正确率,可以做到什麼样的地步,至於正确率可以做到,什麼样的地步呢,那我把文献留在这边给大家参考,那以上就是有关GAN的部分</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="concluding-remarks">Concluding Remarks<a href="#concluding-remarks" class="hash-link" aria-label="Direct link to Concluding Remarks" title="Direct link to Concluding Remarks">​</a></h2>
<p><img decoding="async" loading="lazy" alt="image-20210524150138457" src="/assets/images/image-20210524150138457-6368ccf1acdbe43440883d64bfdaa7a5.png" width="585" height="326" class="img_ev3q"></p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#cycle-gan" class="table-of-contents__link toc-highlight">Cycle GAN</a></li><li><a href="#text-style-transfer" class="table-of-contents__link toc-highlight"><strong>Text Style Transfer</strong></a></li><li><a href="#concluding-remarks" class="table-of-contents__link toc-highlight">Concluding Remarks</a></li></ul></div></div></div></div></main></div></div></div></div>
</body>
</html>