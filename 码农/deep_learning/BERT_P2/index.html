<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-码农/deep_learning/BERT_P2" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">BERT P2_Fun Facts about BERT | Coisini</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://doc.minddiy.top/码农/deep_learning/BERT_P2/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="BERT P2_Fun Facts about BERT | Coisini"><meta data-rh="true" name="description" content="Why does BERT work?"><meta data-rh="true" property="og:description" content="Why does BERT work?"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://doc.minddiy.top/码农/deep_learning/BERT_P2/"><link data-rh="true" rel="alternate" href="https://doc.minddiy.top/码农/deep_learning/BERT_P2/" hreflang="en"><link data-rh="true" rel="alternate" href="https://doc.minddiy.top/码农/deep_learning/BERT_P2/" hreflang="x-default"><meta name="google-site-verification" content="1FUPX6Qo4y3ecU623ShEurhgnjhSTjK49rRMhEDlzFA">
<link rel="stylesheet" href="/katex/katex.min.css">
<script src="/js/matomo.js" async defer="defer"></script><link rel="stylesheet" href="/assets/css/styles.79037026.css">
<script src="/assets/js/runtime~main.468f2b27.js" defer="defer"></script>
<script src="/assets/js/main.4763ab3e.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Chialisp Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="Chialisp Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Coisini</b></a></div><div class="navbar__items navbar__items--right"><a href="https://minddiy.top" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Main site<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input" value=""><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><main class="docMainContainer_TBSr docMainContainerEnhanced_lQrH"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>BERT P2_Fun Facts about BERT</h1></header>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-does-bert-work">Why does BERT work?<a href="#why-does-bert-work" class="hash-link" aria-label="Direct link to Why does BERT work?" title="Direct link to Why does BERT work?">​</a></h2>
<p>&quot;为什么BERT有用？&quot;</p>
<p>最常见的解释是，当输入一串文本时，每个文本都有一个对应的向量。对于这个向量，我们称之为embedding。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210601220428320-1e0d29507516361fea4298d566b45897.png" width="440" height="460" class="img_ev3q"></p>
<p>它的特别之处在于，这些向量代表了<strong>输入词</strong>的<strong>含义</strong>。例如，模型输入 &quot;台湾大学&quot;（国立台湾大学），输出4个向量。这4个向量分别代表 &quot;台&quot;、&quot;湾&quot;、&quot;大 &quot;和 &quot;学&quot;</p>
<p>更具体地说，如果你把这些词所对应的向量画出来，或者计算它们之间的<strong>距离</strong></p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210601221038391-4810ae9b3fa79c9ef02ceefe62d44161.png" width="404" height="457" class="img_ev3q"></p>
<p>你会发现，<strong>意思比较相似的词</strong>，它们的<strong>向量比较接近</strong>。例如，水果和草都是植物，它们的向量比较接近。但这是一个假的例子，我以后会给你看一个真正的例子。&quot;鸟 &quot;和 &quot;鱼 &quot;是动物，所以它们可能更接近。</p>
<p>你可能会问，中文有歧义，其实不仅是中文，很多语言都有歧义，<strong>BERT可以考虑上下文</strong>，所以，同一个词，比如说 &quot;苹果&quot;，它的上下文和另一个 &quot;苹果 &quot;不同，它们的向量也不会相同。</p>
<p>水果 &quot;苹果 &quot;和手机 &quot;苹果 &quot;都是 &quot;苹果&quot;，但根据上下文，它们的<strong>含义是不同</strong>的。所以，它的<strong>向量和相应的embedding会有很大不同</strong>。水果 &quot;苹果 &quot;可能更接近于 &quot;草&quot;，手机 &quot;苹果 &quot;可能更接近于 &quot;电&quot;。</p>
<p>现在我们看一个真实的例子。假设我们现在考虑 &quot;苹果 &quot;这个词，我们会收集很多有 &quot;苹果 &quot;这个词的句子，比如 &quot;喝苹果汁&quot;、&quot;苹果Macbook &quot;等等。然后，我们把这些句子放入BERT中。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210601222431820-4f5e89a37f8ad243beb0be34a88a8dc1.png" width="836" height="431" class="img_ev3q"></p>
<p>接下来，我们将计算 &quot;苹果 &quot;一词的相应embedding。输入 &quot;喝苹果汁&quot;，得到一个 &quot;苹果 &quot;的向量。为什么不一样呢？在Encoder中存在Self-Attention，所以根据 &quot;苹果 &quot;一词的不同语境，得到的向量会有所不同。接下来，我们计算这些结果之间的==cosine similarity==，即计算它们的相似度。</p>
<p>结果是这样的，这里有10个句子</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210602212341684-0d22eda6711eeefca893f93d7e37848b.png" width="864" height="663" class="img_ev3q"></p>
<ul>
<li>
<p>前5个句子中的 &quot;苹果 &quot;代表<strong>可食用</strong>的苹果。例如，第一句是 &quot;我今天买了苹果吃&quot;，第二句是 &quot;进口富士苹果平均每公斤多少钱&quot;，第三句是 &quot;苹果茶很难喝&quot;，第四句是 &quot;智利苹果的季节来了&quot;，第五句是 &quot;关于进口苹果的事情&quot;，这五个句子都有 &quot;苹果 &quot;一词，</p>
</li>
<li>
<p>后面五个句子也有 &quot;苹果 &quot;一词，但提到的是<strong>苹果公司</strong>的苹果。例如，&quot;苹果即将在下个月发布新款iPhone&quot;，&quot;苹果获得新专利&quot;，&quot;我今天买了一部苹果手机&quot;，&quot;苹果股价下跌&quot;，&quot;苹果押注指纹识别技术&quot;，共有十个 &quot;苹果&quot;</p>
</li>
</ul>
<p>计算每一对之间的相似度，得到一个10×10的矩阵。<strong>相似度越高，这个颜色就越浅</strong>。所以，自己和自己之间的相似度一定是最大的，自己和别人之间的相似度一定是更小的。</p>
<p>但前五个 &quot;苹果 &quot;和后五个 &quot;苹果 &quot;之间的相似度相对较低。</p>
<p>BERT知道，前五个 &quot;苹果 &quot;是指可食用的苹果，所以它们比较接近。最后五个 &quot;苹果 &quot;指的是苹果公司，所以它们比较接近。所以<strong>BERT知道，上下两堆 &quot;苹果 &quot;的含义不同</strong>。</p>
<p>BERT的这些向量是输出向量，每个向量代表该词的含义。BERT在填空的过程中已经学会了每个汉字的意思。&quot;,也许它真的理解了中文，对它来说，汉字不再是毫无关联的，既然它理解了中文，它就可以在接下来的任务中做得更好。</p>
<p>那么接下来你可能会问，&quot;为什么BERT有如此神奇的能力？&quot;,为什么......,为什么它能输出代表输入词含义的向量？ 这里，约翰-鲁伯特-弗斯，一位60年代的语言学家，提出了一个假说。他说，要知道一个词的意思，我们需要看它的 &quot;<strong>Company</strong>&quot;，也就是经常和它<strong>一起出现的词汇</strong>，也就是它的<strong>上下文</strong>。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210602213418100-15302ed9f78516c19668b476074759fd.png" width="827" height="632" class="img_ev3q"></p>
<p>一个词的意思，取决于它的上下文</p>
<ul>
<li>
<p>所以以苹果（apple）中的果字为例。如果它经常与 &quot;吃&quot;、&quot;树 &quot;等一起出现，那么它可能指的是可食用的苹果。</p>
</li>
<li>
<p>如果它经常与电子、专利、股票价格等一起出现，那么它可能指的是苹果公司。</p>
</li>
</ul>
<p>当我们训练BERT时，我们给它w1、w2、w3和w4，我们覆盖w2，并告诉它预测w2，而它就是从上下文中提取信息来预测w2。所以这个向量是其上下文信息的精华，可以用来预测w2是什么。</p>
<p>这样的想法在BERT之前已经存在了。在word embedding中，有一种技术叫做CBOW。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210602213758001-a0baf6c9e7727fdf92ca047b5fd6f01f.png" width="882" height="642" class="img_ev3q"></p>
<p>CBOW所做的，与BERT完全一样。做一个空白，并要求它预测空白处的内容。这个CBOW，这个word embedding技术，可以给每个词汇一个向量，代表这个词汇的意义。</p>
<p>CBOW是一个非常简单的模型，它使用两个变换，是一个<strong>非常简单的模型</strong>，有人会问，&quot;为什么它只使用两个变换？&quot;，&quot;它可以更复杂吗？&quot;，CBOW的作者，Thomas Mikolov，曾经来到台湾。当时我在上课的时候，经常有人问我，为什么CBOW只用线性，为什么不用深度学习，我问过Thomas Mikolov这个问题，他说可以用深度学习，但是之所以选择线性模型，一个简单的模型，最大的担心，其实是<strong>算力问题</strong>。当时的计算能力和现在不在一个数量级上，可能是2016年的时候，几年前的技术也不在一个数量级上，当时要训练一个非常大的模型还是比较困难的，所以他选择了一个比较简单的模型。</p>
<p>今天，当你使用<strong>BERT</strong>的时候，就相当于一个<strong>深度版本的CBOW</strong>，你可以做更复杂的事情，而且BERT还可以根据不同的语境，从同一个词汇产生不同的embedding。因为它是一个考虑到语境的高级版本的词embedding，BERT也被称为Contextualized embedding，这些由BERT提取的向量或embedding被称为Contextualized embedding，希望大家能接受这个答案。</p>
<p>但是，这个答案，它真的是真的吗？这是你在文献中听到最多的答案。当你和别人讨论BERT时，这是大多数人都会告诉你的理由。它真的是真的吗？这里有一个难以理解的，由我们实验室的一个学生做的实验。实验是这样的：我们应用为文本训练的BERT对蛋白质、DNA链和音乐进行分类。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210602214418404-f6597cad3ff05c7065cb7f1237a0b31c.png" width="656" height="514" class="img_ev3q"></p>
<p>让我们以DNA链的分类为例。DNA是一系列的脱氧核团核酸，有四种，分别用A、T、C和G表示，所以一条DNA链是这样的。</p>
<p>你可能会问，&quot;EI IE和N代表什么？&quot;不要在意细节，我也不知道，总之，这是一个分类问题。只要用训练数据和标记数据来训练它，就可以了。</p>
<p>神奇的部分来了，DNA可以用ATCG来表示，现在，我们要用BERT来对DNA进行分类</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210602214824169-c0868411397c6284e065f6571d1ab056.png" width="647" height="495" class="img_ev3q"></p>
<p>例如，&quot;A &quot;是 &quot;we&quot;，&quot;T &quot;是 &quot;you&quot;，&quot;C &quot;是 &quot;he&quot;，&quot;G &quot;是 &quot;she&quot;。对应的词并不重要，你可以随机生成。&quot;A &quot;可以对应任何词汇，&quot;T&quot;、&quot;C &quot;和 &quot;G &quot;也可以，这并不重要，对结果影响很小。只是这串文字无法理解。</p>
<p>例如，&quot;AGAC &quot;变成了 &quot;we she we he&quot;，不知道它在说什么。</p>
<p>然后，把它扔进一个一般的BERT，用CLS标记，一个输出向量，一个Linear transform，对它进行分类。只是分类到了DNA类,我不知道他们是什么意思。</p>
<p>和以前一样，Linear transform使用随机初始化，而BERT是通过预训练模型初始化的。但用于初始化的模型，是学习填空的模型。它已经学会了英语填空。</p>
<p>你可能会认为,这个实验完全是无稽之谈。如果我们把一个DNA序列预处理成一个无意义的序列,那么BERT的目的是什么? 大家都知道,BERT可以分析一个有效句子的语义,你怎么能给它一个无法理解的句子呢? 做这个实验的意义是什么?</p>
<p>蛋白质有三种分类，那么蛋白质是由氨基酸组成的，有十种氨基酸，只要给每个氨基酸一个随机的词汇，那么DNA是一组ATCG，音乐也是一组音符，给它每个音符一个词汇，然后，把它作为一个文章分类问题来做。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210602215524076-9c4613199484e3e15f30ccbc87d68b58.png" width="668" height="381" class="img_ev3q"></p>
<p>你会发现，如果你不使用BERT，你得到的结果是蓝色部分，如果你使用BERT，你得到的结果是红色部分，这实际上更好，你们大多数人现在一定很困惑。</p>
<p>这个实验只能用神奇来形容，没有人知道它为什么有效，而且目前还没有很好的解释，我之所以要谈这个实验，是想告诉你们，要了解BERT的力量，还有很多工作要做。</p>
<p>我并不是要否认BERT能够分析句子的含义这一事实。从embedding中，我们清楚地观察到，BERT知道每个词的含义，它能找出含义相似的词和不相似的词。但正如我想指出的那样，即使你给它一个无意义的句子，它仍然可以很好地对句子进行分类。</p>
<p>所以，<strong>也许它的力量并不完全来自于对实际文章的理解</strong>。也许还有其他原因。例如，也许，BERT只是一套更好的初始参数。也许这与语义不一定有关。也许这套初始参数，只是在训练大型模型时更好。</p>
<p>是这样吗？这个问题<strong>需要进一步研究</strong>来回答。我之所以要讲这个实验，是想让大家知道，我们目前使用的模型往往是非常新的，需要进一步的研究，以便我们了解它的能力。</p>
<p>你今天学到的关于BERT的知识，只是沧海一粟。我会把一些视频的链接放在这里。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210602215747103-d1810cf4fd8f70849dd72d824b781529.png" width="749" height="535" class="img_ev3q"></p>
<p>如果你想了解更多关于BERT的知识，你可以参考这些链接。你的作业不需要它，,这学期剩下的时间也不需要。我只想告诉你，BERT还有很多其他的变种。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="multi-lingual-bert">Multi-lingual BERT<a href="#multi-lingual-bert" class="hash-link" aria-label="Direct link to Multi-lingual BERT" title="Direct link to Multi-lingual BERT">​</a></h2>
<p>接下来，我要讲的是，一种叫做Multi-lingual BERT的BERT。Multi-lingual BERT有什么神奇之处？</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210602220031527-bcecfc55e6ad99837de1d283edb76d8f.png" width="669" height="346" class="img_ev3q"></p>
<p>它是由很多语言来训练的，比如中文、英文、德文、法文等等，用填空题来训练BERT，这就是Multi-lingual BERT的训练方式。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="zero-shot-reading-comprehension">Zero-shot Reading Comprehension<a href="#zero-shot-reading-comprehension" class="hash-link" aria-label="Direct link to Zero-shot Reading Comprehension" title="Direct link to Zero-shot Reading Comprehension">​</a></h3>
<p>google训练了一个Multi-lingual BERT，它能够做这104种语言的填空题。神奇的地方来了，如果你用<strong>英文</strong>问答<strong>数据</strong>训练它，它就会自动学习如何做<strong>中文问答</strong></p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210602220224565-9994a2ed03095901fbe2bfe4b31d38c7.png" width="620" height="384" class="img_ev3q"></p>
<p>我不知道你是否完全理解我的意思，所以这里有一个真实的实验例子。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210602220621646-00c2675c3cec995d36d77d18d4afa64f.png" width="668" height="372" class="img_ev3q"></p>
<p>这是一些训练数据。他们用SQuAD进行fine-tune。这是一个英文Q&amp;A数据集。中文数据集是由台达电发布的，叫DRCD。这个数据集也是我们在作业中要用到的数据集。</p>
<p>在BERT提出之前，效果并不好。在BERT之前，最强的模型是QANet。它的正确率只有......，嗯，我是说F1得分，而不是准确率，但你可以暂时  把它看成是准确率或正确率。</p>
<p>如果我们允许用中文填空题进行预训练，然后用中文Q&amp;A数据进行微调，那么它在中文Q&amp;A测试集上的正确率达到89%。因此，其表现是相当令人印象深刻的。</p>
<p>神奇的是，如果我们把一个Multi-lingual的BERT，用英文Q&amp;A数据进行微调，它仍然可以回答中文Q&amp;A问题，并且有78%的正确率，这几乎与QANet的准确性相同。它从未接受过中文和英文之间的翻译训练，也从未阅读过中文Q&amp;A的数据收集。,它在没有任何准备的情况下参加了这个中文Q&amp;A测试，尽管它从未见过中文测试，但不知为何，它能回答这些问题。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cross-lingual-alignment">Cross-lingual Alignment?<a href="#cross-lingual-alignment" class="hash-link" aria-label="Direct link to Cross-lingual Alignment?" title="Direct link to Cross-lingual Alignment?">​</a></h3>
<p>你们中的一些人可能会说：&quot;它在预训练中读过104种语言，104种语言中的一种是中文，是吗？ 如果是，这并不奇怪。&quot;但是在预训练中，学习的目标是填空。它只能用中文填空。有了这些知识，再加上做英文问答的能力，不知不觉中，它就自动学会了做中文问答。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210602220948753-ed0fd419367b7da5edf31352d13b6da9.png" width="823" height="439" class="img_ev3q"></p>
<p>听起来很神奇，那么BERT是怎么做到的呢？一个简单的解释是：也许对于多语言的BERT来说，<strong>不同的语言并没有那么大的差异</strong>。无论你用中文还是英文显示，对于具有相同含义的单词，它们的embedding都很接近。汉语中的 &quot;跳 &quot;与英语中的 &quot;jump &quot;接近，汉语中的 &quot;鱼 &quot;与英语中的 &quot;fish &quot;接近，汉语中的 &quot;游 &quot;与英语中的 &quot;swim &quot;接近，也许在学习过程中它已经自动学会了。</p>
<p>它是可以被验证的。我们实际上做了一些验证。验证的标准被称为Mean Reciprocal Rank，缩写为MRR。我们在这里不做详细说明。你只需要知道，<strong>MRR的值越高，不同embedding之间的Alignment就越好</strong>。</p>
<p>更好的Alignment意味着，具有相同含义但来自不同语言的词将被转化为更接近的向量。如果MRR高，那么具有相同含义但来自不同语言的词的向量就更接近。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210602221257119-30ecfa20d9346b344beb9a11f701f42a.png" width="676" height="497" class="img_ev3q"></p>
<p>这条深蓝色的线是谷歌发布的104种语言的Multi-lingual BERT的MRR，它的值非常高，这说明不同语言之间没有太大的差别。Multi-lingual BERT只看意思，不同语言对它没有太大的差别。</p>
<p>橙色这条是我们试图自己训练Multi-lingual BERT。我们使用的<strong>数据较少</strong>，每种语言只使用了20万个句子。数据较少。我们自我训练的模型结果并不好。我们不知道为什么我们的Multi-lingual BERT不能将不同的语言统一起来。似乎它不能学习那些在不同语言中具有相同含义的符号，它们应该具有相同的含义。这个问题困扰了我们很长时间。</p>
<p>为什么我们要做这个实验？为什么我们要自己训练Multi-lingual BERT？因为我们想了解，是什么让Multi-lingual BERT。我们想设置不同的参数，不同的向量，看看哪个向量会影响Multi-lingual BERT。</p>
<p>但是我们发现，对于我们的Multi-lingual BERT来说，无论你如何调整参数，它就是不能达到Multi-lingual的效果，它就是不能达到Alignment的效果。我们把数据量<strong>增加了五倍</strong>，看看能不能达到Alignment的效果。在做这个实验之前，大家都有点抵触，大家都觉得有点害怕，因为训练时间要比原来的长五倍。</p>
<p>训练了两天后，什么也没发生，损失甚至不能减少，就在我们要放弃的时候，损失突然下降了</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210602221752573-b9031c21a0ab74ef078ea302348e63f5.png" width="888" height="599" class="img_ev3q"></p>
<p>用了8个V100来训练，我们的实验室也没有8个V100，是在NCHC（国家高性能计算中心）的机器上运行的，训练了两天后，损失没有下降，似乎失败了。当我们要放弃的时候，损失下降了。</p>
<p>这是某个学生在Facebook上发的帖子，我在这里引用它来告诉你，我当时心里的感叹。整个实验，必须运行一个多星期，才能把它学好，每一种语言1000K的数据。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210602221914782-1db691a8914b6bfa0269cce90806f2f3.png" width="863" height="612" class="img_ev3q"></p>
<p>所以看起来，<strong>数据量是一个非常关键的因素</strong>，关系到能否成功地将不同的语言排列在一起。所以有时候，神奇的是，很多问题或很多现象，只有在有足够的数据量时才会显现出来。它可以在A语言的QA上进行训练，然后直接转移到B语言上，从来没有人说过这一点</p>
<p>这是过去几年才出现的，一个可能的原因是，过去没有足够的数据，现在有足够的数据，现在有大量的计算资源，所以这个现象现在有可能被观察到。</p>
<p>最后一个神奇的实验，我觉得这件事很奇怪</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210602222118762-c0a6654794f77594b0c6340c4a5f6565.png" width="826" height="627" class="img_ev3q"></p>
<p>你说BERT可以把不同语言中含义相同的符号放在一起，使它们的向量接近。但是，当训练多语言的BERT时，如果给它英语，它可以用英语填空，如果给它中文，它可以用中文填空，它不会混在一起</p>
<p>那么，如果不同语言之间没有区别，怎么可能只用英语标记来填英语句子呢？为什么它不会用中文符号填空呢？它就是不填，这说明它知道语言的信息也是不同的，那些不同语言的符号毕竟还是不同的，它并没有完全抹去语言信息，所以我想出了一个研究课题，我们来找找，语言信息在哪里。</p>
<p>后来我们发现，语言信息并没有隐藏得很深。一个学生发现，我们把所有<strong>英语单词</strong>的embedding，放到多语言的BERT中，<strong>取embedding的平均值</strong>，我们对<strong>中文单词</strong>也做<strong>同样的事情</strong>。在这里，我们给Multi-lingual BERT一个英语句子，并得到它的embedding。我们在embedding中<strong>加上</strong>这个<strong>蓝色的向量</strong>，这就是<strong>英语和汉语之间的差距</strong>。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210602222406050-a59dc3285a1b7f57158d908e71b788d1.png" width="839" height="641" class="img_ev3q"></p>
<p>这些向量，从Multi-lingual BERT的角度来看，变成了汉语。有了这个神奇的东西，你可以做一个奇妙的无监督翻译。</p>
<p>例如，你给BERT看这个中文句子。</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210602222544789-e59f70e137f7b96ae3a929baa10757e8.png" width="871" height="607" class="img_ev3q"></p>
<p>这个中文句子是，&quot;能帮助我的小女孩在小镇的另一边，，没人能够帮助我&quot;，现在我们把这个句子扔到Multi-lingual BERT中。</p>
<p>然后我们取出Multi-lingual BERT中的一个层，它不需要是最后一层，可以是任何一层。我们拿出某一层，给它一个embedding，加上这个蓝色的向量。对它来说，这个句子马上就从中文变成了英文。</p>
<p>在向BERT输入英文后，通过在中间加一个<strong>蓝色的向量来转换隐藏</strong>层，转眼间，中文就出来了。&quot;没有人可以帮助我&quot;，变成了 &quot;是（是）没有人（没有人）可以帮助我（我）&quot;，&quot;我 &quot;变成了 &quot;我&quot;，&quot;没有人 &quot;变成了 &quot;没有人&quot;，所以它在某种程度上可以做无监督的标记级翻译，尽管它并不完美，神奇的是，Multi-lingual的BERT仍然保留了语义信息。</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#why-does-bert-work" class="table-of-contents__link toc-highlight">Why does BERT work?</a></li><li><a href="#multi-lingual-bert" class="table-of-contents__link toc-highlight">Multi-lingual BERT</a><ul><li><a href="#zero-shot-reading-comprehension" class="table-of-contents__link toc-highlight">Zero-shot Reading Comprehension</a></li><li><a href="#cross-lingual-alignment" class="table-of-contents__link toc-highlight">Cross-lingual Alignment?</a></li></ul></li></ul></div></div></div></div></main></div></div></div></div>
</body>
</html>