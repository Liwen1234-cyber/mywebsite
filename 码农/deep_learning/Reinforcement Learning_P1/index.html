<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-码农/deep_learning/Reinforcement Learning_P1" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Reinforcement Learning P1 ：Basics | Coisini</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://doc.minddiy.top/码农/deep_learning/Reinforcement Learning_P1/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Reinforcement Learning P1 ：Basics | Coisini"><meta data-rh="true" name="description" content="那这一堂课啊,我们要讲的是,Deep Reinforcement Learning,也就是 RL,那我想这个 RL 啊,Reinforcement Learning 啊,大家一定一点都不陌生,因为你知道很多很潮的应用,AlphaGo 等等,它背后呢,用的就是 RL 的技术,那 RL 可以讲的技术啊,非常非常地多,它不是在一堂课裡面可以讲得完的,我甚至觉得说,如果有人要把它开成一整个学期的课,可能也是有这麽多东西可以讲"><meta data-rh="true" property="og:description" content="那这一堂课啊,我们要讲的是,Deep Reinforcement Learning,也就是 RL,那我想这个 RL 啊,Reinforcement Learning 啊,大家一定一点都不陌生,因为你知道很多很潮的应用,AlphaGo 等等,它背后呢,用的就是 RL 的技术,那 RL 可以讲的技术啊,非常非常地多,它不是在一堂课裡面可以讲得完的,我甚至觉得说,如果有人要把它开成一整个学期的课,可能也是有这麽多东西可以讲"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://doc.minddiy.top/码农/deep_learning/Reinforcement Learning_P1/"><link data-rh="true" rel="alternate" href="https://doc.minddiy.top/码农/deep_learning/Reinforcement Learning_P1/" hreflang="en"><link data-rh="true" rel="alternate" href="https://doc.minddiy.top/码农/deep_learning/Reinforcement Learning_P1/" hreflang="x-default"><meta name="google-site-verification" content="1FUPX6Qo4y3ecU623ShEurhgnjhSTjK49rRMhEDlzFA">
<link rel="stylesheet" href="/katex/katex.min.css">
<script src="/js/matomo.js" async defer="defer"></script><link rel="stylesheet" href="/assets/css/styles.79037026.css">
<script src="/assets/js/runtime~main.468f2b27.js" defer="defer"></script>
<script src="/assets/js/main.4763ab3e.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Chialisp Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="Chialisp Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Coisini</b></a></div><div class="navbar__items navbar__items--right"><a href="https://minddiy.top" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Main site<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input" value=""><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><main class="docMainContainer_TBSr docMainContainerEnhanced_lQrH"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Reinforcement Learning P1 ：Basics</h1></header>
<p>那这一堂课啊,我们要讲的是,Deep Reinforcement Learning,也就是 RL,那我想这个 RL 啊,Reinforcement Learning 啊,大家一定一点都不陌生,因为你知道很多很潮的应用,AlphaGo 等等,它背后呢,用的就是 RL 的技术,那 RL 可以讲的技术啊,非常非常地多,它不是在一堂课裡面可以讲得完的,我甚至觉得说,如果有人要把它开成一整个学期  的课,可能也是有这麽多东西可以讲</p>
<p>所以今天啊,这堂课的目的,并不是要告诉你有关 RL 的一切,而是让大家有一个基本的认识,大概知道 RL 是什么样的东西,那 RL 相关的课程,你其实在网路上可以找到,非常非常多的参考的资料,那 RL 如果要讲得非常地艰涩,其实也是可以讲得非常地艰涩的</p>
<p>可是今天这一堂课啊,我们儘量避开太过理论的部分,我期待这堂课可以让你做到的,并不是让你听了觉得,哇 RL 很困难啊,搞不清楚在做什么,而是期待让你觉得说,啊 <strong>RL 原来就只是这样而已</strong>,我自己应该也做得起来,希望这一堂课可以达到这一个目的</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="supervised-learningrl">Supervised Learning→RL<a href="#supervised-learningrl" class="hash-link" aria-label="Direct link to Supervised Learning→RL" title="Direct link to Supervised Learning→RL">​</a></h2>
<p>那什么是 Reinforcement Learning 呢,到目前为止啊,我们讲的几乎都是 Supervised Learning,假设你要做一个 Image 的 Classifier,你不只要告诉机器,它的 Input 是什么,你还要告诉机器,它应该输出什么样的 Output,然后接下来呢,你就可以 Train 一个 Image 的 Classifier</p>
<p><img decoding="async" loading="lazy" alt="image-20210911230950586" src="/assets/images/image-20210911230950586-02f058c1c27206745f398dabfd72ae0a.png" width="403" height="108" class="img_ev3q"></p>
<p>那在多数这门课讲到目前为止的技术,基本上都是基于 Supervised Learning 的方法,就算是我们在讲 Self Supervised Learning 的时候,我们其实也是,很类似 Supervised Learning 的方法,只是我们的 Label,不需要特别僱用人力去标记,它可以自动产生</p>
<p>或者是我们在讲 Auto-encoder 的时候,我们虽然说它是一个 Unsupervised 的方法,我们没有用到人类的标记,但事实上,我们还是有一个 Label,只是这个 Label,不需要耗费人类的力量来产生而已</p>
<p>但是 RL 就是另外一个面向的问题了,在 RL 裡面,我们遇到的问题是这样子的,我们,<strong>机器当我们给它一个输入的时候,我们不知道最佳的输出应该是什么</strong></p>
<p>举例来说,假设你要叫机器学习下围棋</p>
<p><img decoding="async" loading="lazy" alt="image-20210911231032976" src="/assets/images/image-20210911231032976-72a00de175682c7b1fa8a88e0dab7754.png" width="455" height="163" class="img_ev3q"></p>
<p>用 Supervised Learning 的方法,好像也可以做,你就是告诉机器说,看到现在的盘势长这个样子的时候,下一步应该落子的位置在哪裡,但是问题是,下一步应该落子的位置到底应该在哪裡呢,哪一个是最好的下一步呢,哪一步是神之一手呢,可能人类根本就不知道</p>
<p>当然你可以说,让机器阅读很多职业棋士的棋谱,让机器阅读很多高段棋士的棋谱,也许这些棋谱裡面的答案,也许这些棋谱裡面给某一个盘势,人类下的下一步,就是一个很好的答案,但它是不是最好的答案呢,我们不知道,在这个<strong>你不知道正确答案是什么的情况下,往往就是 RL 可以派上用场的时候</strong>,所以当你今天,你发现你要收集有标注的资料很困难的时候,正确答案人类也不知道是什么的时候,也许就是你可以考虑使用 RL 的时候</p>
<p>但是 <strong>RL 在学习的时候,机器其实也不是一无所知的</strong>,我们虽然不知道正确的答案是什么,但是机器会知道什么是好,什么是不好,机器会跟环境去做互动,得到一个叫做 ==Reward== 的东西,这我们等一下都还会再细讲</p>
<p>所以机器会知道,它现在的输出是好的还是不好的,来藉由跟环境的互动,藉由知道什么样的输出是好的,什么样的输出是不好的,机器还是可以学出一个模型</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="outline">Outline<a href="#outline" class="hash-link" aria-label="Direct link to Outline" title="Direct link to Outline">​</a></h2>
<p>好 那接下来呢,这是今天这份投影片的 Outline</p>
<p><img decoding="async" loading="lazy" alt="image-20210911231230408" src="/assets/images/image-20210911231230408-53a088b96c9b08e373df0e662fc8fb69.png" width="455" height="253" class="img_ev3q"></p>
<p>首先呢,我们会从最基本的 RL 的概念开始,那在介绍这个 RL 概念的时候,有很多不同的切入点啦,也许你比较常听过的切入点是这样,比如说从 Markov Decision Process 开始讲起</p>
<p>那我们这边选择了一个比较不一样的切入点,我要告诉你说,虽然如果你自己读 RL 的文献的话,你会觉得,哇 RL 很複杂哦,跟一般的 Machine Learning 好像不太一样哦,但是我这边要告诉你说,<strong>RL 它跟我们这一门课学的 Machine Learning,是一样的框架</strong></p>
<p>我们在今天这个这学期,一开始的第一堂课就告诉你说,Machine Learning 就是三个步骤,那 RL 呢,RL 也是一模一样的三个步骤,等一下会再跟大家说明</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="machine-learning--looking-for-a-function">Machine Learning ≈ Looking for a Function<a href="#machine-learning--looking-for-a-function" class="hash-link" aria-label="Direct link to Machine Learning ≈ Looking for a Function" title="Direct link to Machine Learning ≈ Looking for a Function">​</a></h2>
<p>在今天,在这个本学期这一门课的第一开始,就告诉你说,什么是机器学习,<strong>机器学习就是找一个 Function</strong>,Reinforcement Learning,RL 也是机器学习的一种,那它也在找一个 Function,它在找什么样的 Function 呢</p>
<p>那 Reinforcement Learning 裡面呢,我们会有一个 ==Actor==,还有一个 ==Environment==,那这个 <strong>Actor 跟 Environment,会进行互动</strong></p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210911232111496-1449e583ec054c9f54b6171c39111a4c.png" width="819" height="495" class="img_ev3q"></p>
<ul>
<li>你的这 个 Environment,你的这个环境啊,会给 Actor 一个 Observation,会给,那这个 Observation 呢,就是 Actor 的输入</li>
<li>那 Actor 呢,看到这个 Observation 以后呢,它会有一个输出,这个输出呢,叫做 Action,那这个 Action 呢,会去影响 Environment</li>
<li>这个 Actor 採取 Action 以后呢,Environment 就会给予新的 Observation,然后 Actor 呢,会给予新的 Action,那这个 Observation 是 Actor 的输入,那这个 Action 呢,是 Actor 的输出</li>
</ul>
<p>所以 <strong>Actor 本身啊,它就是一个 Function</strong>,其实 Actor,它就是我们要找的 Function,这个 Function 它的轮入,就是环境给它的 Observation,输出就是这个 Actor 要採取的 Action,而今天在这个互动的过程中呢,<strong>这个 Environment,会不断地给这个 Actor 一些 Reward</strong>,告诉它说,你现在採取的这个 Action,它是好的还是不好的</p>
<p>而我们今天要找的这个 Actor,我们今天要找的这个 Function,可以拿 Observation 当作 Input,Actor 当作 Output 的 Function,这个 Function 的目标,是要去 Maximizing,我们可以从 Environment,获得到的 Reward 的总和,我们希望呢,找一个 Function,那用这个 Function 去跟环境做互动,<strong>用 Observation 当作 Input,输出 Action,最终得到的 Reward 的总和,可以是最大的,这个就是 RL 要找的 Function</strong></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="example-playing-video-game">Example: Playing Video Game<a href="#example-playing-video-game" class="hash-link" aria-label="Direct link to Example: Playing Video Game" title="Direct link to Example: Playing Video Game">​</a></h3>
<p>那我知道这样讲,你可能还是觉得有些抽象,所以我们举更具体的例子,那等一下举的例子呢,都是用 Space Invader 当作例子啦,那 Space Invader 就是一个非常简单的小游戏,那 RL 呢,最早的几篇论文,也都是玩,让那个机器呢,去玩这个 Space Invader 这个游戏</p>
<p>在 Space Invader 裡面呢</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210912211107238-63ce6cfb4dc3dfadea43bab88bdbb7e2.png" width="626" height="402" class="img_ev3q"></p>
<ul>
<li>
<p>你要<strong>操控的是下面这个绿色的东西</strong>,这个下面这个绿色的东西呢,是你的太空梭,你可以採取的行为,也就是 Action 呢 有三个,<strong>左移 右移跟开火</strong>,就这三个行为,然后你现在要做的事情啊,就是杀掉画面上的这些外星人</p>
</li>
<li>
<p>画面上这些黄色的东西,也就是<strong>外星人</strong>啦,然后你开火,击中那些外星人的话,那外星人就死掉了</p>
</li>
<li>
<p>那前面这些东西是什么呢,那个是你的<strong>防护罩</strong>,如果你不小心打到自己的防护罩的话,你的防护罩呢,也是会被打掉的,那你可以躲在防护罩后面,你就可以挡住外星人的攻击</p>
</li>
<li>
<p>然后接下来呢 会有分数,那在萤幕画面上会有分数,当你杀死外星人的时候,你会得到<strong>分数</strong>,或者是在有些版本的 Space Invader 裡面,会有一个补给包,从上面横过去 飞过去,那你打到补给包的话,会被加一个很高的分数,那这个 Score 呢,就是 Reward,就是环境给我们的 Reward</p>
</li>
</ul>
<p>那这个游戏呢,它是会终止的,那什么时候终止呢,当所有的外星人都被杀光的时候就终止,或者是呢,外星人其实也会对你的母舰开火啦,外星人击中你的母舰 你也是会,这个你就被摧毁了,那这个游戏呢,也就终止了,好 那这个是介绍一下,Space Invader 这一个游戏,</p>
<p>那如果你今天呢,要用 Actor 去玩 Space Invader,大概会像是什么样子呢</p>
<p>现在你的 Actor 啊,<strong>Actor 虽然是一个机器,但是它是坐在人的这一个位置,它是站在人这一个角度,去操控摇杆</strong>,去控制那个母舰,去跟外星人对抗,而你的环境是什么,<strong>你的环境呢,是游戏的主机</strong>,游戏的主机这边去操控那些外  星人,外星人去攻击你的母舰,所以 Observation 是游戏的画面</p>
<p><img decoding="async" loading="lazy" src="/assets/images/image-20210912211546018-d461c993b11356cd6ba0e790de214755.png" width="515" height="297" class="img_ev3q"></p>
<p>所以对 Actor 来说,它看到的,其实就跟人类在玩游戏的时候,看到的东西是一样的,就看到一个游戏的画面</p>
<p>那输出呢,就是 Actor 可以採取的行为,那可以採取哪些行为,通常是事先定义好的,在这个游戏裡面,就只有向左 向右跟开火,三种可能的行为而已,好 那当你的 Actor 採取向右这个行为的时候,那它会得到 Reward</p>
<p>那因为在这个游戏裡面,只有<strong>杀掉外星人会得到分数,而我们就是把分数定义成我们的 Reward</strong>,那向左 向右其实并不会,不可能杀掉任何的外星人,所以你得到的 Reward 呢,就是 0 分,好 那你採取一个 Action 以后呢,游戏的画面就变了</p>
<ul>
<li>
<p>游戏的画面变的时候,就代表了有了新的 Observation 进来</p>
</li>
<li>
<p>有了新的 Observation 进来,你的 Actor 就会决定採取新的 Action</p>
</li>
</ul>
<p><img decoding="async" loading="lazy" alt="image-20210912211850357" src="/assets/images/image-20210912211850357-c0eb5a7385ae2a10b929b70beb7e6701.png" width="536" height="321" class="img_ev3q"></p>
<p>你的 Actor 是一个 Function,这个 Function 会根据输入的 Observation,输出对应的 Action,那新的画面进来,假设你的 Actor,现在它採取的行为是开火,而开火这个行为正好杀掉一隻外星人的时候,你就会得到分数,那这边假设得到的分数是 5 分,杀那个外星人,得到的分数是 5 分,那你就得到 Reward 等于 5</p>
<p>那这个呢,就是拿 Actor 去玩,玩这个 Space Invader 这个游戏的状况,好 那这个 Actor 呢,它想要学习的是什么呢,我们在玩游戏的过程中,会不断地得到 Reward,那在刚才例子裡面,做第一个行为的时候,向右的时候得到的是 0 分,做第 二个行为,开火的时候得到的是 5 分,那接下来你採取了一连串行为,都有可能给你分数</p>
<p>而 Actor 要做的事情,我<strong>们要学习的目标,我们要找的这个 Actor</strong> 就是,我们想要 Learn 出一个 Actor,这个 Actor,这个 Function,我们使用它在这个游戏裡面的时候,<strong>可以让我们得到的 Reward 的总和会是最大的</strong>,那这个就是拿 Actor 去,这个就是 RL 用在玩这个小游戏裡面的时候,做的事情</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="example-learning-to-play-go">Example: Learning to play Go<a href="#example-learning-to-play-go" class="hash-link" aria-label="Direct link to Example: Learning to play Go" title="Direct link to Example: Learning to play Go">​</a></h3>
<p>那其实如果把 RL 拿来玩围棋,拿来下围棋,其实做的事情跟小游戏,其实也没有那麽大的差别,只是规模跟问题的複杂度不太一样而已</p>
<p>那如果今天你要让机器来下围棋,那你的 Actor 就是就是 AlphaGo,那你的环境是什么,你的环境就是 AlphaGo 的人类对手</p>
<p><img decoding="async" loading="lazy" alt="image-20210912212328358" src="/assets/images/image-20210912212328358-23fd1cf66386991e7f6231a13fa33260.png" width="355" height="292" class="img_ev3q"></p>
<p>你的 Actor 的输入就是棋盘,棋盘上黑子跟白子的位置,那如果是在游戏的一开始,棋盘上就空空的,空空如也,上面什么都没有,没有任何黑子跟白子</p>
<p>那这个 Actor 呢,看到这个棋盘呢,它就要产生输出,它就要决定它下一步,应该落子在哪裡,那如果是围棋的话,你的输出的可能性就是有 19×19 个可能性,那这 19×19 个可能性,每一个可能性,就对应到棋盘上的一个位置</p>
<p>那假设现在你的 Actor,决定要落子在这个地方</p>
<p><img decoding="async" loading="lazy" alt="image-20210912212426858" src="/assets/images/image-20210912212426858-aa4989bcc3ca270f256d3b6d81fe4fb2.png" width="541" height="294" class="img_ev3q"></p>
<p>那这一个结果,就会输入给你的环境,那其实就是一个棋士,然后呢 这个环境呢,就会再产生新的 Observation,因为这个李世石这个棋士呢,也会再落一子,那现在看到的环境又不一样了,那你的 Actor 看到这个新的 Observation,它就会产生新的 Action,然后就这样反覆继续下去</p>
<p><img decoding="async" loading="lazy" alt="image-20210912212456020" src="/assets/images/image-20210912212456020-69796fd72caf5076709c4cab746ad1bf.png" width="545" height="291" class="img_ev3q"></p>
<p>你就可以让机器做下围棋这件事情,好 那在这个,在这个下围棋这件事情裡面的 Reward,是怎麽计算的呢</p>
<p><strong>在下围棋裡面,你所採取的行为,几乎都没有办法得到任何 Reward</strong>,在下围棋这个游戏裡,在下围棋这件事情裡面呢,你会定义说,如果赢了,就得到 1 分,如果输了就得到 -1 分</p>
<p><img decoding="async" loading="lazy" alt="image-20210912212536103" src="/assets/images/image-20210912212536103-9e818bbf304337fb41fffe0bc3b45eb3.png" width="547" height="321" class="img_ev3q"></p>
<p>也就是说在下围棋这整个,这个你的 Actor 跟环境互动的过程中,其实<strong>只有游戏结束,只有整场围棋结束的最后一子,你才能够拿到 Reward</strong>,就你最后,最后 Actor 下一子下去,赢了,就得到 1 分,那最后它落了那一子以后,游戏结束了,它输了,那就得到 -1 分,那在中间整个互动的过程中的 Reward,就都算是 0 分,没有任何的 Reward,那这个 Actor 学习的目标啊,就是要去最大化,它可能可以得到的 Reward</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="machine-learning-is-so-simple-">Machine Learning is so simple ……<a href="#machine-learning-is-so-simple-" class="hash-link" aria-label="Direct link to Machine Learning is so simple ……" title="Direct link to Machine Learning is so simple ……">​</a></h2>
<p>刚才讲的也许你都已经听过了,那这个是 RL 最常见的一种解说方式,那接下来要告诉你说,RL 跟机器学习的 Framework,它们之间的关係是什么</p>
<p><img decoding="async" loading="lazy" alt="image-20210912212810370" src="/assets/images/image-20210912212810370-3dee59337ae3c0456f6abc50c9fbbb99.png" width="493" height="262" class="img_ev3q"></p>
<p>开学第一堂课就告诉你说,Machine Learning 就是三个步骤</p>
<ol>
<li>第一个步骤,你有一个 Function,那个 Function 裡面有一些未知数,Unknown 的 Variable,这些未知数是要被找出来的</li>
<li>第二步,订一个 Loss Function,第三步,想办法找出未知数去最小化你的 Loss</li>
<li>第三步就是 Optimization</li>
</ol>
<p>而 RL 其实也是一模一样的三个步骤</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-1-function-with-unknown">Step 1: Function with Unknown<a href="#step-1-function-with-unknown" class="hash-link" aria-label="Direct link to Step 1: Function with Unknown" title="Direct link to Step 1: Function with Unknown">​</a></h3>
<p>第一个步骤,我们现在有未知数的这个 Function,到底是什么呢,这个有未知数的 Function,就是我们的 Actor,那在 RL 裡面,<strong>你的 Actor 呢,就是一个 Network</strong>,那我们现在通常叫它 Policy 的 Network</p>
<p><img decoding="async" loading="lazy" alt="image-20210912213733742" src="/assets/images/image-20210912213733742-b4b51a0457fc98a127953c6a51a0f90e.png" width="329" height="182" class="img_ev3q"></p>
<p>那在过去啊,在还没有把 Deep Learning 用到 RL 的时候,通常你的 Actor 是比较简单的,它不是 Network,它可能只是一个 Look-Up-Table,告诉你说看到什么样的输入,就产生什么样的输出,那今天我们都知道要用 Network,来当做这个 Actor,那这个 Network,其实就是一个很複杂的 Function</p>
<p><img decoding="async" loading="lazy" alt="image-20210912213816733" src="/assets/images/image-20210912213816733-1e32bcded2490a4e0afffdc8203bdcc7.png" width="484" height="312" class="img_ev3q"></p>
<p>这个複杂的 Function 它的输入是什么呢,它的<strong>输入就是游戏的画面</strong>,就是游戏的画面,这个游戏画面上的 Pixel,像素,就是这一个 Actor 的输入</p>
<p>那它的输出是什么呢,它的<strong>输出就是,每一个可以採取的行为</strong>,它的分数,每一个可以採取的 Action 它的分数,举例来说 输入这样的画面,给你的 Actor,你的 Actor 其实就是一个 Network,它的输出可能就是给,向左 0.7 分,向右 0.2 分,开火 0.1 分</p>
<p>那事实上啊,这件事情<strong>跟分类是没有什么两样</strong>的,你知道分类就是输入一张图片,输出就是决定这张图片是哪一个类别,那你的 Network 会给每一个类别,一个分数,你可能会通过一个 Softmax Layer,然后每一个类别都有个分数,而且这些分数的总和是 1</p>
<p>那其实在 RL 裡面,你的 Actor 你的 Policy Network,跟分类的那个 Network,其实是一模一样的,你就是输入一张图片,输出其实最后你也会有个 Softmax Layer,然后呢,你就会 Left、Right 跟 Fire,三个 Action 各给一个分数,那这些分数的总和,你也会让它是 1</p>
<p><img decoding="async" loading="lazy" alt="image-20210912213951581" src="/assets/images/image-20210912213951581-a9ff7ddd985c0f1785e655e68f90ea70.png" width="532" height="207" class="img_ev3q"></p>
<p>那至于这个 <strong>Network 的架构呢,那你就可以自己设计了</strong>,要设计怎麽样都行,比如说如果输入是一张图片,欸 也许你就会想要用 CNN 来处理</p>
<p>不过在助教的程式裡面,其实不是用 CNN 来处理啦,因为在我们的作业裡面,其实在玩游戏的时候,不是直接让我们的 Machine 去看游戏的画面,让它直接去看游戏的,让它直接去看游戏的画面比较难做啦,所以我们是让,看这个跟现在游戏的状况有关的一些参数而已,所以在这个助教的,在这个作业的这个 Sample Code 裡面呢,还没有用到 CNN 那麽複杂,就是一个简单的 Fully Connected Network,但是假设你要让你的 Actor,它的输入真的是游戏画面,欸 那你可能就会採取这个 CNN,你可能就用 CNN 当作你的 Network 的架构</p>
<p><strong>甚至你可能说</strong>,我不要只看现在这一个时间点的游戏画面,我要看整场游戏到目前为止发生的所有事情,可不可以呢</p>
<p>可以,那过去你可能会用 RNN 考虑,现在的画面跟过去所有的画面,那现在你可能会想要用 Transformer,考虑所有发生过的事情,所以 Network 的架构是你可以自己设计的,只要能够输入游戏的画面,输出类似像类别这样的 Action 就可以了,那最后机器会决定採取哪一个 Action,取决于每一个 Action 取得的分数</p>
<p>常见的做法啊,是直接把这个分数,就当做一个机率,然后按照这个机率,去 Sample,去随机决定要採取哪一个 Action</p>
<p><img decoding="async" loading="lazy" alt="image-20210912213951581" src="/assets/images/image-20210912213951581-a9ff7ddd985c0f1785e655e68f90ea70.png" width="532" height="207" class="img_ev3q"></p>
<p>举例来说 在这个例子裡面,向左得到 0.7 分,那就是有 70% 的机率会採取向左,20% 的机率会採取向右,10% 的机率会採取开火</p>
<p>那你可能会问说,为什么不是用argmax呢,为什么不是看 Left 的分数最高,就直接向左呢,你也可以这麽做,但是在助教的程式裡面,还有多数 RL 应用的时候 你会发现,我们都是採取 Sample,</p>
<p>採取 <strong>Sample 有一个好处是说,今天就算是看到同样的游戏画面,你的机器每一次採取的行为,也会略有不同</strong>,那在很多的游戏裡面这种随机性,也许是重要的,比如说你在做剪刀石头布的时候,如果你总是会出石头,就跟小叮噹一样,那你就很容易被打爆,如果你有一些随机性,就比较不容易被打爆</p>
<p>那其实之所以今天的输出,是用随机 Sample 的,还有另外一个很重要的理由,那这个我们等一下会再讲到,好 所以这是第  一步,我们有一个 Function,这个 Function 有 Unknown 的 Variable,我们有一个 Network,那裡面有参数,这个参数就是 Unknown 的 Variable,就是要被学出来的东西,这是第一步</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-2-define-loss">Step 2: Define “Loss”<a href="#step-2-define-loss" class="hash-link" aria-label="Direct link to Step 2: Define “Loss”" title="Direct link to Step 2: Define “Loss”">​</a></h3>
<p>然后接下来第二步,我们要定义 Loss,在 RL 裡面,我们的 Loss 长得是什么样子呢,我们再重新来看一下,我们的机器跟环境互动的过程,那只是现在用不一样的方法,来表示刚才说过的事情</p>
<p><img decoding="async" loading="lazy" alt="image-20210912214755516" src="/assets/images/image-20210912214755516-8c238edfd72250ad6b529f5bce24b0d9.png" width="276" height="333" class="img_ev3q"></p>
<p>首先有一个初始的游戏画面,这个初始的游戏画面,被作为你的 Actor 的输入</p>
<p>你的 Actor 那就输出了一个 Action,比如说向右,输入的游戏画面呢,我们叫它 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>,然后输出的 Action 呢,就叫它 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></p>
<p>那现在会得到一个 Reward,这边因为向右没有做任何事情,没有杀死任何的外星人,所以得到的 Reward 可能就是 0 分</p>
<p><img decoding="async" loading="lazy" alt="image-20210912214950357" src="/assets/images/image-20210912214950357-7e243b616b97861fde9961e8cc6d673e.png" width="534" height="326" class="img_ev3q"></p>
<p>採取向右以后,会看到新的游戏画面,这个叫做 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>,根据新的游戏画面 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>,你的 Actor 会採取新的行为,比如说开火,这边用 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>,来表示看到游戏画面 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 的时候,所採取的行为</p>
<p>那假设开火恰好杀死一隻外星人,和你的 Actor 就得到 Reward,这个 Reward 的分数呢,是 5 分,然 后採取开火这个行为以后</p>
<p><img decoding="async" loading="lazy" alt="image-20210912215036248" src="/assets/images/image-20210912215036248-037f2d435b08a138ff837954bc8b1d87.png" width="557" height="269" class="img_ev3q"></p>
<p>接下来你会看到新的游戏画面,那机器又会採取新的行为,那这个互动的过程呢,就会反覆持续下去,直到机器在採取某一个行为以后,游戏结束了,那什么时候游戏结束呢,就看你游戏结束的条件是什么嘛</p>
<p>举例来说,採取最后一个行为以后,比如说向右移,正好被外星人的子弹打中,那你的飞船就毁了,那游戏就结束了,或者是最后一个行为是开火,把最后一隻外星人杀掉,那游戏也就结束了,就你执行某一个行为,满足游戏结束的条件以后,游戏就结束了</p>
<p><img decoding="async" loading="lazy" alt="image-20210912215129396" src="/assets/images/image-20210912215129396-16b542ee6ba9c526261c9bbab25769fc.png" width="555" height="366" class="img_ev3q"></p>
<p>那从游戏开始到结束的这整个过程啊,被称之为一个 ==Episode==,那在整个游戏的过程中,机器会採取非常多的行为,每一个行为都可能得到 Reward,把所有的 Reward 通通集合起来,我们就得到一个东西,叫做整场游戏的 ==Total Reward==,</p>
<p><img decoding="async" loading="lazy" alt="image-20210912215203605" src="data:image/png;base64,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" width="171" height="157" class="img_ev3q"></p>
<p>那这个 Total Reward 呢,就是从游戏一开始得到的 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>,一直得,一直加,累加到游戏最后结束的时候,得到的 rt,假设这个游戏裡面会互动,T 次,那麽就得到一个 Total Reward,我们这边用 R,来表示 Total Reward,其实这个 Total Reward 又有另外一个名字啊,叫做 ==Return== 啦,你在这个 RL 的文献上,常常会同时看到 Reward 跟 Return,这两个词会出现,那 Reward 跟 Return 其实有点不一样,Reward 指的是你採取某一个行为的时候,立即得到的好处,这个是 Reward,把整场游戏裡面所有的 Reward 通通加起来,这个叫做 Return</p>
<p>但是我知道说,很快你就会忘记 Reward 跟 Return 的差别了,所以我们等一下就不要再用 Return 这个词彙,我们直接告诉你说,整场游戏的 Reward 的总和,就是 Total 的 Reward,而这个 Total 的 Reward 啊,就是我们想要去最大化的东西,就是我们训练的目标</p>
<p>那你可能会说,欸 这个跟 Loss 不一样啊,Loss 是要越小越好啊,这个 Total Reward 是要越大越好啊,所以有点不一样吧,但是我们可以说在 RL 的这个情境下,我们把那个 Total Reward 的负号,<strong>负的 Total Reward,就当做我们的 Loss,Total Reward 是要越大越好,那负的 Total Reward,当然就是要它越小越好</strong>吧,就我们完全可以说负的 Total Reward,就是我们的 Loss,就是 RL 裡面的 Loss</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-3-optimization">Step 3: Optimization<a href="#step-3-optimization" class="hash-link" aria-label="Direct link to Step 3: Optimization" title="Direct link to Step 3: Optimization">​</a></h3>
<p>那我们再把这个环境跟,Agent 互动的这一件事情啊,再用不一样的图示,再显示一次</p>
<p><img decoding="async" loading="lazy" alt="image-20210912220011837" src="/assets/images/image-20210912220011837-bd9537ee26c11eacb7493aa26a6065af.png" width="419" height="169" class="img_ev3q"></p>
<p>这个是你的环境,你的环境呢,输出一个 Observation,叫做 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></p>
<ul>
<li>
<p>这个 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 呢,会变成你的 Actor 的输入</p>
</li>
<li>
<p>你的 Actor 呢,接下来就是输出 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></p>
</li>
<li>
<p>然后这个 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 呢,又变成环境的输入</p>
</li>
<li>
<p>你的环境呢,看到 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 以后,又输出 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></p>
</li>
</ul>
<p>然后这个互动的过程啊,就会继续下去,<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>  又输入给 Actor,它就输出 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>,<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 又输入给 Environment,它就输出给,它就产生 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></p>
<p>它这个互动呢,一直下去,直到满足游戏中止的条件,好 那这个 s 跟 a 所形成的这个 Sequence,就是 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 这个 Sequence,又叫做 Trajectory,那我们用 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span></span></span></span>来表示 Trajectory</p>
<p><img decoding="async" loading="lazy" alt="image-20210912220319849" src="/assets/images/image-20210912220319849-0457d5014f7dab408ca34f10df1e8df8.png" width="411" height="320" class="img_ev3q"></p>
<p>那根据这个互动的过程,Machine 会得到 Reward,你其实可以把 Reward 也想成是一个 Function,我们这边用一个绿色的方块来代表,这个 Reward 所构成的 Function</p>
<p>那这个 Reward 这个 Function,有不同的表示方法啦,在有的游戏裡面,也许你的 Reward,只需要看你採取哪一个 Action 就可以决定,不过通常我们在决定 Reward 的时候,光看 Action 是不够的,你还要看现在的 Observation 才可以,因为并不是每一次开火你都一定会得到分数,开火要正好有击到外星人,外星人正好在你前面,你开火才有分数</p>
<p>所以通常 <strong>Reward Function 在定义的时候,不是只看 Action,它还需要看 Observation</strong>,同时看 Action 跟 Observation,才能够知道现在有没有得到分数,所以 Reward 是一个 Function,</p>
<p>这个 Reward 的 Function,它拿 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 跟 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 当作输入,然后它产生 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 作为输出,它拿 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 跟 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 当作输入,产生 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 作为输出,把所有的 r 通通结合起来,把 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 加 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 加 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 一直加到 T,全部结合起来就得到 R,这个就是 Total Reward,也就是 Return,这个是我们要最大化要去 Maximize 的对象</p>
<p>这个 Optimization 的问题是这个样子,你要去找一个 Network,其实是 Network 裡面的参数,你要去 Learn 出一组参数,这一组参数放在 Actor 的裡面,它可以让这个 R 的数值越大越好,就这样,结束了,整个 Optimization 的过程就是这样,你要去找一个 Network 的参数,让这边产生出来的 R 越大越好</p>
<p><img decoding="async" loading="lazy" alt="image-20210912220319849" src="/assets/images/image-20210912220319849-0457d5014f7dab408ca34f10df1e8df8.png" width="411" height="320" class="img_ev3q"></p>
<p>那乍看之下,如果这边的,这个 Environment Actor 跟 Reward,它们<strong>都是 Network 的话,这个问题其实也没有什么难的</strong>,这个搞不好你现在都可以解,它看起来就有点像是一个 Recurrent Network,这是一个 Recurrent Network,然后你的 Loss 就是这个样子,那只是这边是 Reward 不是 Loss,所以你是要让它越大越好,你就去 Learn 这个参数,用 Gradient Descent 你就可以让它越大越好</p>
<p>但是 RL 困难的地方是,<strong>这不是一个一般的 Optimization 的问题</strong>,因为你的 Environment,这边有很多问题导致说,它跟一般的 Network Training 不太一样</p>
<p>第一个问题是,<strong>你的 Actor 的输出是有随机性的</strong></p>
<p><img decoding="async" loading="lazy" alt="image-20210912221135459" src="/assets/images/image-20210912221135459-c1e1673aba02918e78e53aebdb7f02fc.png" width="427" height="189" class="img_ev3q"></p>
<p>这个 <strong><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 它是用 Sample 产生的</strong>,你定同样的 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 每次产生的 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 不一定会一样,所以假设你把 Environment Actor 跟 Reward,合起来当做是一个巨大的 Network 来看待,这个 Network 可不是一般的 Network,<strong>这个 Network 裡面是有随机性的</strong>,这个 Network 裡面的某一个 Layer 是,每次产生出来结果是不一样的,<strong>这个 Network 裡面某一个 Layer 是,它的输出每次都是不一样的</strong></p>
<p>另外还有一个更大的问题就是,<strong>你的 Environment 跟 Reward,它根本就不是 Network 啊,它只是一个黑盒子而已</strong>,你根本不知道裡面发生了什么事情,Environment 就是游戏机,那这个游戏机它裡面发生什么事情你不知道,你只知道说你输入一个东西会输出一个东西,你採取一个行为它会有对应的回应,但是到底是怎麽产生这个对应的回应,我们不知道,它只是一个黑盒子,</p>
<p><strong>Reward 可能比较明确,但它也不是一个 Network,它就是一条规则</strong>嘛,它就是一个规则说,看到这样子的 Optimization 跟这样的 Action,会得到多少的分数,它就只是一个规则而已,所以它也不是 Network</p>
<p>而且更麻烦的地方是,<strong>往往 Reward 跟 Environment,它也是有随机性的</strong>,如果是在电玩裡面,通常 Reward 可能比较不会有随机性,因为规则是定好的,对有一些 RL 的问题裡面,Reward 是有可能有随机性的</p>
<p>但是在 Environment 裡面,就算是在电玩的这个应用中,它也是有随机性的,你给定同样的行为,到底游戏机会怎麽样回应,它裡面可能也是有乱数的,它可能每次的回应也都是不一样,如果是下围棋,你落同一个子,你落在,你落子在同一个位置, 你的对手会怎麽样回应,每次可能也是不一样</p>
<p><img decoding="async" loading="lazy" alt="image-20210912230423421" src="/assets/images/image-20210912230423421-b6710381cd95015f1122bfcdd228b49f.png" width="449" height="278" class="img_ev3q"></p>
<p>所以环境很有可能也是有随机性的,所以这不是一个一般的 Optimization 的问题,你可能不能够用我们这门课已经学过的,训练 Network 的方法来找出这个 Actor,来最大化 Reward</p>
<p><strong>所以 RL 真正的难点就是,我们怎麽解这一个 Optimization 的问题</strong>,怎麽找到一组 Network 参数,可以让 R 越大越好</p>
<p>其实你再仔细想一想啊,这整个问题<strong>跟 GAN 其实有异曲同工之妙</strong>,它们有一样的地方,也有不一样的地方</p>
<ul>
<li>
<p>先说它们<strong>一样</strong>的地方在哪裡,你记不记得在训练 GAN 的时候,在训练 Generator 的时候,你会把 Generator 跟 Discriminator 接在一起,然后你希望去调整 Generator 的参数,让 Discriminator 的输出越大越好,今天在 RL 裡面,我们也可以说这个 Actor 就像是 Generator,Environment 跟 Reward 就像是 Discriminator,我们要去<strong>调整 Generator 的参数,让 Discriminator 的输出越大越好</strong>,所以它跟 GAN 有异曲同工之妙</p>
</li>
<li>
<p>但什么地方<strong>不一样</strong>呢,在 GAN 裡面你的 Discriminator,也是一个 Neural Network,你了解 Discriminator 裡面的每一件事情,它也是一个 Network,你可以用 Gradient Descent,来 train 你的 Generator,让 Discriminator 得到最大的输出,但是在 RL 的问题裡面,你的 <strong>Reward 跟 Environment,你可以把它们当 Discriminator 来看,但它们不是 Network,它们是一个黑盒子,所以你没有办法用,一般 Gradient Descent 的方法来调整你的参数,来得到最大的输出</strong>,所以这是 RL,跟一般 Machine Learning不一样的地方</p>
</li>
</ul>
<p>但是我们还是可以把 RL 就看成三个阶段,只是在 Optimization 的时候,在你怎麽 Minimize Loss,也就怎麽 Maximize Reward 的时候,跟之前我们学到的方法是不太一样的</p>
<h1>Policy Gradient</h1>
<p>接下来啊,我们就要讲一个拿来解 RL,拿来做 Optimization 那一段常用的一个演算法,叫做 Policy Gradient</p>
<p><img decoding="async" loading="lazy" alt="image-20210912231113114" src="/assets/images/image-20210912231113114-cd73954e68ea6f74da61540f072d8cf5.png" width="490" height="341" class="img_ev3q"></p>
<p>那如果你真的想知道,Policy Gradient 是哪裡来的,你可以参见过去上课的<a href="https://youtu.be/W8XF3ME8G2I" target="_blank" rel="noopener noreferrer">录影</a>,对 Policy Gradient 有比较详细的推导,那今天我们是从另外一个角度,来讲 Policy Gradient 这件事情</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-control-your-actor">How to control your actor<a href="#how-to-control-your-actor" class="hash-link" aria-label="Direct link to How to control your actor" title="Direct link to How to control your actor">​</a></h3>
<p>那在讲 Policy Gradient 之前,我们先来想想看,我们要怎麽操控一个 Actor 的输出,我们要怎麽让一个 Actor,在看到某一个特定的 Observation 的时候,採取某一个特定的行为呢,我们怎麽让一个 Actor,它的输入是 s 的时候,它就要输出 Action <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span> 呢</p>
<p><img decoding="async" loading="lazy" alt="image-20210912231731634" src="/assets/images/image-20210912231731634-768403665c02ea8fed3fcc5027b87d26.png" width="476" height="163" class="img_ev3q"></p>
<p>那你其实完全可以把它<strong>想成一个分类的问题</strong>,也就是说假设你要让 Actor 输入 s,输出就是 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span>,假设 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span> 就是向左好了,假设你要让,假设你已经知道,假设你就是要教你的 Actor 说,看到这个游戏画面向左就是对的,你就是给我向左,那你要怎麽让你的 Actor 学到这件事呢</p>
<p><img decoding="async" loading="lazy" alt="image-20210912231832863" src="/assets/images/image-20210912231832863-3a63797d6291847f33177f2baf3408f3.png" width="489" height="190" class="img_ev3q"></p>
<p>那也就说 s 是 Actor 的输入,<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span> 就是我们的 Label,就是我们的 Ground Truth,就是我们的正确答案,而接下来呢,你就可以计算你的 Actor,它的输出跟 Ground Truth 之间的 Cross-entropy,那接下来你就可以定义一个 Loss</p>
<p>假设你希望你的 Actor,它<strong>採取 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span> 这个行为的话</strong>,你就定一个 Loss,这个 Loss 等于 Cross-entropy</p>
<p><img decoding="async" loading="lazy" alt="image-20210912232003035" src="/assets/images/image-20210912232003035-b245f815a464f3906435852c357e27c3.png" width="457" height="258" class="img_ev3q"></p>
<p>然后呢,你再去 Learn 一个 θ,你再去 Learn 一个 θ,然后这个 θ 可以让 Loss 最小,那你就可以让这个 Actor 的输出,跟你的 Ground Truth 越接近越好</p>
<p>你就可以让你的 Actor 学到说,看到这个游戏画面的时候,它就是要向左,这个是要让你的 Actor,採取某一个行为的时候的做法</p>
<p>但是假设你想要让你的 Actor,<strong>不要採取某一个行为</strong>的话,那要怎麽做呢,假设你希望做到的事情是,你的 Actor 看到某一个 Observation s 的时候, 我就千万不要向左的话怎麽做呢,其实很容易,你<strong>只需要把 Loss 的定义反过来就好</strong></p>
<p><img decoding="async" loading="lazy" alt="image-20210912232223834" src="/assets/images/image-20210912232223834-45fe6b836988453e6c89fdf8598c87cc.png" width="435" height="131" class="img_ev3q"></p>
<p>你希望你的 Actor 採取 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span> 这个行为,你就定义你的大 L 等于 Cross-entropy,然后你要 Minimize Cross-entropy,假设你要让你的 Actor,不要採取 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span> 这个行为的话,那你就把你就定一个 Loss,叫做负的 Cross-entropy,Cross-entropy 乘一个负号,那你去 Minimize 这个 L,你去 Minimize 这个 L,就是让 Cross-entropy 越大越好,那也就是让 a 跟 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span> 的距离越远越好,那你就可以避免你的 Actor 在看到 s 的时候,去採取 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span> 这个行为,所以我们有办法控制我们的 Actor,做我们想要做的事,只要我们给它适当的 Label 跟适当的 Loss,</p>
<p>所以假设我们要让我们的 Actor,<strong>看到 s 的时候採取 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span>,看到 s&#x27; 的时候不要採取 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span>&#x27;</strong> 的话,要怎麽做呢</p>
<p>这个时候你就会说,Given s 这个 Observation,我们的 Ground Truth 叫做 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span>,Given s&#x27; 这个 Observation 的时候,我们有个 Ground Truth 叫做 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span>&#x27;,那对这两个 Ground Truth, 我们都可以去计算 Cross-entropy,<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 跟 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></p>
<p><img decoding="async" loading="lazy" alt="image-20210912232542292" src="/assets/images/image-20210912232542292-aa151d4a44840bd969075a05dce1b2a5.png" width="475" height="308" class="img_ev3q"></p>
<p>然后接下来呢,我们就定义说我们的 Loss,就是 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 减 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>,也就是说我们要让这个 Case,它的 Cross-entropy 越小越好,这个 Case 它的 Cross-entropy 越大越好</p>
<p>然后呢,我们去找一个 θ 去 Minimize Loss,得到 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⋆</span></span></span></span></span></span></span></span></span></span></span>,那就是一个可以在 s,可以在看到 s 的时候採取 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span>,看到 s&#x27; 的时候採取 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span>&#x27; 的 Actor,所以藉由很像是在,Train 一个 Classifier 的这种行为,藉由很像是现在 Train 一个 Classifier,的这种 Data,我们可以去控制一个 Actor 的行为,</p>
<p>有一个同学问了一个非常好的问题</p>
<ul>
<li>
<p>**Q:**就是如果以 Alien 的游戏来说的话,因为只有射中 Alien 才会有 Reward,这样 Model 不是就会一直倾向于射击吗</p>
<p>**A:**对 这个问题我们等一下会来解决它,之后的投影片就会来解决它</p>
</li>
</ul>
<p>然后又有另外一个同学,问了一个非常好的问题就是</p>
<ul>
<li>**Q:**哇 这样不就回到 Supervised Learning 了嘛,这个投影片上看起来,就是在训练一个 Classifier 而已啊,我们就是在训练 Classifier,你只是告诉它说,看到 s 的时候就要输出 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span>,看到 s&#x27; 的时候就不要输出 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span>,<span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span>&#x27;,这不就是 Supervised Learning 吗</li>
<li>**A:**这就是 Supervised Learning,这个就是跟 Supervised Learning Train 的,Image Classifier 是一模一样的,但等下我们会看到它跟,一般的 Supervised Learning 不一样在哪裡,</li>
</ul>
<p>那所以呢,如果我们要训练一个 Actor,我们其实就需要收集一些训练资料,就收集训练资料说,我希望在 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 的时候採取 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span>1,我希望在 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 的时候不要採取 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span>2</p>
<p><img decoding="async" loading="lazy" alt="image-20210912233019856" src="/assets/images/image-20210912233019856-8fc87a388b87394e53cf5d60b69e1ac3.png" width="481" height="233" class="img_ev3q"></p>
<p>但可能会问说,欸 这个训练资料哪来的,这个我们等一下再讲训练资料哪来的</p>
<p>所以你就<strong>收集一大堆的资料,这个跟 Train 一个 Image 的 Classifier 很像的</strong>,这个 s 你就想成是 Image,这个 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span> 你就想成是 Label,只是现在有的行为是想要被採取的,有的行为是不想要被採取的,你就收集一堆这种资料,你就可以去定义一个 Loss Function,有了这个 Loss Function 以后,你就可以去训练你的 Actor,去 <strong>Minimize 这个 Loss Function,就结束了</strong>,你就可以训练一个 Actor,期待它执行我们的行为,期待它执行的行为是我们想要的</p>
<p>而你甚至还可以更进一步,你可以说<strong>每一个行为并不是只有好或不好</strong>,并不是有想要执行跟不想要执行而已,<strong>它是有程度的差别的</strong>,有执行的非常好的,有 Nice to have 的,有有点不好的,有非常差的</p>
<p>所以刚才啊,我们是说每一个行为就是要执行   不要执行,这是一个 Binary 的问题,这是我们就用 ±1 来表示</p>
<p><img decoding="async" loading="lazy" alt="image-20210912233324323" src="/assets/images/image-20210912233324323-1c659a2da4a16ed11370557fb6be0c27.png" width="439" height="225" class="img_ev3q"></p>
<p>但是现在啊,我们改成<strong>每一个 s 跟 a 的 Pair,它有对应的一个分数</strong>,这个分数代表说,我们多希望机器在看到 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 的时候,执行 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 这个行为</p>
<p>那比如说这边第一笔资料跟第三笔资料,我们分别是定 +1.5 跟 +0.5,就代表说我们期待机器看到 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 的时候,它可以做 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>,看到 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 的时候它可以做 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>,但是我们期待它看到 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 的时候,做 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 的这个期待更强烈一点,比看到 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 做 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 的期待更强烈一点</p>
<p>那我们希望它在看到 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 的时候,不要做 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>,我们期待它看到 sN 的时候,不要做 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>,而且我们非常不希望,它在看到 sN 的时候做 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></p>
<p>有了这些资讯,你一样可以定义一个 Loss Function,你只是在你的原来的 Cross-entropy 前面,本来是 Cross-entropy 前面,要嘛是 +1 要嘛是 -1</p>
<p>现在改成乘上 An 这一项,改成乘上 An 这一项,告诉它说有一些行为, 我们非常期待 Actor 去执行,有一些行为我们非常不期待 Actor 去执行,有一些行为如果执行是比较好的,有一些行为希望儘量不要执行比较好,但就算执行了也许伤害也没有那麽大</p>
<p>所以我们<strong>透过这个 An 来控制说,每一个行为我们多希望 Actor 去执行</strong>,然后接下来有这个 Loss 以后,一样 Train 一个 θ,Train 下去你就找一个 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⋆</span></span></span></span></span></span></span></span></span></span></span>,你就有个 Actor 它的行为是符合我们期待的</p>
<p><img decoding="async" loading="lazy" alt="image-20210912233513464" src="/assets/images/image-20210912233513464-c3b6d8485fdd094a1a4436ec16364880.png" width="455" height="254" class="img_ev3q"></p>
<p>那接下来的难点就是,要怎麽定出这一个 a 呢,这个就是我们接下来的难点,就是我们接下来要面对的问题,我们还有另外一个要面对的问题是,怎麽产生这个 s 跟 a 的 Pair 呢,怎麽知道在 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 的时候要执行 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span>,或在 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 的时候不要执行 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span> 呢,那这个也是等一下我们要处理的问题,</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#supervised-learningrl" class="table-of-contents__link toc-highlight">Supervised Learning→RL</a></li><li><a href="#outline" class="table-of-contents__link toc-highlight">Outline</a></li><li><a href="#machine-learning--looking-for-a-function" class="table-of-contents__link toc-highlight">Machine Learning ≈ Looking for a Function</a><ul><li><a href="#example-playing-video-game" class="table-of-contents__link toc-highlight">Example: Playing Video Game</a></li><li><a href="#example-learning-to-play-go" class="table-of-contents__link toc-highlight">Example: Learning to play Go</a></li></ul></li><li><a href="#machine-learning-is-so-simple-" class="table-of-contents__link toc-highlight">Machine Learning is so simple ……</a><ul><li><a href="#step-1-function-with-unknown" class="table-of-contents__link toc-highlight">Step 1: Function with Unknown</a></li><li><a href="#step-2-define-loss" class="table-of-contents__link toc-highlight">Step 2: Define “Loss”</a></li><li><a href="#step-3-optimization" class="table-of-contents__link toc-highlight">Step 3: Optimization</a></li><li><a href="#how-to-control-your-actor" class="table-of-contents__link toc-highlight">How to control your actor</a></li></ul></li></ul></div></div></div></div></main></div></div></div></div>
</body>
</html>