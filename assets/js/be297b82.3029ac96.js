"use strict";(self.webpackChunkmybase=self.webpackChunkmybase||[]).push([[80406],{49351:(r,n,e)=>{e.r(n),e.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>i,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"\u5b66\u4e60/Csi-feedback","title":"Csi-feedback","description":"\u8fd9\u4e2a\u4ee3\u7801\u4eff\u771f\u5b9e\u73b0\u4e86CsiNet,CsiCNNTransformerNet,CsiTransformerNet,CS_Net","source":"@site/docs/\u5b66\u4e60/Csi-feedback.md","sourceDirName":"\u5b66\u4e60","slug":"/\u5b66\u4e60/Csi-feedback","permalink":"/\u5b66\u4e60/Csi-feedback","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"someSidebar","previous":{"title":"Cramer-Rao Lower Bound","permalink":"/\u5b66\u4e60/\u514b\u62c9\u7f8e-\u7f57\u754c"},"next":{"title":"\u901a\u4fe1\u7b14\u8bb0","permalink":"/\u5b66\u4e60/\u901a\u4fe1\u7b14\u8bb0"}}');var s=e(74848),o=e(28453);const i={},a="Csi-feedback",l={},c=[{value:"load data and pre-processing data",id:"load-data-and-pre-processing-data",level:2},{value:"loss",id:"loss",level:2},{value:"Modules",id:"modules",level:2},{value:"image segmentation",id:"image-segmentation",level:2},{value:"train",id:"train",level:2},{value:"Csi_Net",id:"csi_net",level:2},{value:"Csi_Transformer_Net",id:"csi_transformer_net",level:2},{value:"Csi_CNN_Transformer_Net",id:"csi_cnn_transformer_net",level:2},{value:"CS_Net",id:"cs_net",level:2}];function d(r){const n={blockquote:"blockquote",br:"br",code:"code",h1:"h1",h2:"h2",header:"header",img:"img",p:"p",pre:"pre",...(0,o.R)(),...r.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"csi-feedback",children:"Csi-feedback"})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"\u8fd9\u4e2a\u4ee3\u7801\u4eff\u771f\u5b9e\u73b0\u4e86Csi_Net,Csi_CNN_Transformer_Net,Csi_Transformer_Net,CS_Net"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"load-data-and-pre-processing-data",children:"load data and pre-processing data"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import numpy as np\r\nimport h5py\r\nimport torch\r\nfrom torch.utils.data import Dataset\r\n\r\nimg_height = 16\r\nimg_width = 32\r\nimg_channels = 2\r\n\r\nclass DatasetFolder(Dataset):\r\n\r\n    def __init__(self, matData):\r\n        self.matdata = matData\r\n\r\n    def __len__(self):\r\n        return self.matdata.shape[0]\r\n\r\n    def __getitem__(self, index):\r\n        return self.matdata[index] \r\n\r\ndef load_data(\r\n        file_path,\r\n        shuffle = False,\r\n        train_test_ratio=0.8,\r\n        batch_size=32,\r\n        num_workers=0,\r\n        pin_memory=True,\r\n        drop_last=True):\r\n\r\n    print(\"loading data...\")\r\n    mat = h5py.File('Hdata.mat', 'r')\r\n    data = np.transpose(mat['H_train'])\r\n    data = data.astype('float32')\r\n    data = np.reshape(data, [len(data), img_channels, img_height, img_width])\r\n    \r\n    # Use only 1/40 of the data\r\n    num_samples = len(data)\r\n    reduced_size = num_samples // 40\r\n    data = data[:reduced_size]\r\n\r\n    if shuffle:\r\n        data_copy = np.copy(data)\r\n        data_transpose = data_copy.transpose()\r\n        np.random.shuffle(data_transpose)\r\n        data_shuffle = data_transpose.transpose()\r\n\r\n    partition = int(data.shape[0] * train_test_ratio)\r\n    x_train, x_test = data[:partition], data[partition:]\r\n    x_train_shuffle, x_test_shuffle = data_shuffle[:partition], data_shuffle[partition:]\r\n\r\n    # dataLoader for training\r\n    train_dataset = DatasetFolder(x_train)\r\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\r\n                                               shuffle=True, num_workers=num_workers,\r\n                                               pin_memory=pin_memory, drop_last=drop_last)\r\n    # dataLoader for validating\r\n    test_dataset = DatasetFolder(x_test)\r\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\r\n                                              shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\r\n\r\n    if shuffle:\r\n        train_shuffle_dataset = DatasetFolder(x_train_shuffle)\r\n        train_shuffle_loader = torch.utils.data.DataLoader(train_shuffle_dataset, batch_size=batch_size,\r\n                                                  shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\r\n\r\n        test_shuffle_dataset = DatasetFolder(x_test_shuffle)\r\n        test_shuffle_loader = torch.utils.data.DataLoader(test_shuffle_dataset, batch_size=batch_size,\r\n                                                          shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\r\n\r\n\r\n        return train_loader, test_loader, train_dataset, test_dataset,                train_shuffle_loader, test_shuffle_loader, train_shuffle_dataset, test_shuffle_dataset\r\n\r\n    return train_loader, test_loader, train_dataset, test_dataset\r\n\n"})}),"\n",(0,s.jsx)(n.h2,{id:"loss",children:"loss"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch.nn as nn\r\ndef NMSE(x, x_hat):\r\n    x_real = np.reshape(x[:, :, :, 0], (len(x), -1))\r\n    x_imag = np.reshape(x[:, :, :, 1], (len(x), -1))\r\n    x_hat_real = np.reshape(x_hat[:, :, :, 0], (len(x_hat), -1))\r\n    x_hat_imag = np.reshape(x_hat[:, :, :, 1], (len(x_hat), -1))\r\n    x_C = x_real - 0.5 + 1j * (x_imag - 0.5)\r\n    x_hat_C = x_hat_real - 0.5 + 1j * (x_hat_imag - 0.5)\r\n    power = np.sum(abs(x_C) ** 2, axis=1)\r\n    mse = np.sum(abs(x_C - x_hat_C) ** 2, axis=1)\r\n    nmse = np.mean(mse / power)\r\n    return nmse\r\n\r\ndef NMSE_cuda(x, x_hat):\r\n    x_real = x[:, 0, :, :].view(len(x), -1) - 0.5\r\n    x_imag = x[:, 1, :, :].view(len(x), -1) - 0.5\r\n    x_hat_real = x_hat[:, 0, :, :].contiguous().view(len(x_hat), -1) - 0.5\r\n    x_hat_imag = x_hat[:, 1, :, :].contiguous().view(len(x_hat), -1) - 0.5\r\n    power = torch.sum(x_real ** 2 + x_imag ** 2, axis=1)\r\n    mse = torch.sum((x_real - x_hat_real) ** 2 + (x_imag - x_hat_imag) ** 2, axis=1)\r\n    nmse = mse / power\r\n    return nmse\r\n\r\nclass NMSELoss(nn.Module):\r\n    def __init__(self, reduction='sum'):\r\n        super(NMSELoss, self).__init__()\r\n        self.reduction = reduction\r\n\r\n    def forward(self, x_hat, x):\r\n        nmse = NMSE_cuda(x, x_hat)\r\n        if self.reduction == 'mean':\r\n            nmse = torch.mean(nmse)\r\n        else:\r\n            nmse = torch.sum(nmse)\r\n        return nmse\r\n\r\ndef rho(x, x_hat):\r\n    x_real = x[:, 0, :, :].view(len(x), -1) - 0.5\r\n    x_imag = x[:, 1, :, :].view(len(x), -1) - 0.5\r\n    x_hat_real = x_hat[:, 0, :, :].contiguous().view(len(x_hat), -1) - 0.5\r\n    x_hat_imag = x_hat[:, 1, :, :].contiguous().view(len(x_hat), -1) - 0.5\r\n\r\n    cos = nn.CosineSimilarity(dim=1, eps=0)\r\n    out_real = cos(x_real,x_hat_real)\r\n    out_imag = cos(x_imag, x_hat_imag)\r\n    result_real = out_real.sum() / len(out_real)\r\n    reault_imag = out_imag.sum() / len(out_imag)\r\n    return 0.5*(result_real + reault_imag)\r\n\r\nclass CosSimilarity(nn.Module):\r\n    def __init__(self, reduction='sum'):\r\n        super(CosSimilarity, self).__init__()\r\n        self.reduction = reduction\r\n\r\n    def forward(self, x_hat, x):\r\n        cos = rho(x, x_hat)\r\n        # if self.reduction == 'mean':\r\n        #     cos = torch.mean(cos)\r\n        # else:\r\n        #     cos = torch.sum(cos)\r\n        return cos\n"})}),"\n",(0,s.jsx)(n.h2,{id:"modules",children:"Modules"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import torch.nn.functional as F\r\nfrom collections import OrderedDict\r\n\r\ndef conv3x3(in_planes, out_planes, stride=1):\r\n    """3x3 convolution with padding"""\r\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\r\n                     padding=1, bias=True)\r\n\r\nclass ConvBN(nn.Sequential):\r\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, groups=1):\r\n        if not isinstance(kernel_size, int):\r\n            padding = [(i - 1) // 2 for i in kernel_size]\r\n        else:\r\n            padding = (kernel_size - 1) // 2\r\n        super(ConvBN, self).__init__(OrderedDict([\r\n            (\'conv\', nn.Conv2d(in_planes, out_planes, kernel_size, stride,\r\n                               padding=padding, groups=groups, bias=False)),\r\n            (\'bn\', nn.BatchNorm2d(out_planes)),\r\n            (\'LeakyReLU\', nn.LeakyReLU(negative_slope=0.3, inplace=False))\r\n        ]))\r\n\r\nclass ConvBN_linear(nn.Sequential):\r\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, groups=1):\r\n        if not isinstance(kernel_size, int):\r\n            padding = [(i - 1) // 2 for i in kernel_size]\r\n        else:\r\n            padding = (kernel_size - 1) // 2\r\n        super(ConvBN_linear, self).__init__(OrderedDict([\r\n            (\'conv\', nn.Conv2d(in_planes, out_planes, kernel_size, stride,\r\n                               padding=padding, groups=groups, bias=False)),\r\n            (\'bn\', nn.BatchNorm2d(out_planes))\r\n        ]))\r\n\r\nclass ResBlock(nn.Module):\r\n\r\n    def __init__(self, ch, nblocks=1, shortcut=True):\r\n        super().__init__()\r\n        self.shortcut = shortcut\r\n        self.module_list = nn.ModuleList()\r\n        for i in range(nblocks):\r\n            resblock_one = nn.ModuleList()\r\n            resblock_one.append(ConvBN(ch, 8, 3))\r\n            resblock_one.append(ConvBN(8, 16, 3))\r\n            resblock_one.append(ConvBN_linear(16, ch, 3))\r\n            self.module_list.append(resblock_one)\r\n\r\n    def forward(self, x):\r\n        for module in self.module_list:\r\n            h = x\r\n            for res in module:\r\n                h = res(h)\r\n            x = x + h if self.shortcut else h\r\n        return x\r\n\r\nclass PatchEmbed(nn.Module):\r\n\r\n    def __init__(self, H=16, W=32, patch_size=4, in_chans=2, embed_dim=32):\r\n        super().__init__()\r\n        num_patches = H * W / patch_size ** 2\r\n        self.img_size = [H, W]\r\n        self.patch_size = [patch_size, patch_size]\r\n        self.num_patches = num_patches\r\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\r\n        \r\n    def forward(self, x):\r\n        x = self.proj(x).flatten(2).transpose(1, 2)\r\n        return x\r\n\r\nclass FixedPositionalEncoding(nn.Module):\r\n    \r\n    def __init__(self, embedding_dim, max_length=5000):\r\n        super(FixedPositionalEncoding, self).__init__()\r\n        pe = torch.zeros(max_length, embedding_dim)\r\n        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\r\n        div_term = torch.exp(\r\n            torch.arange(0, embedding_dim, 2).float()\r\n            * (-torch.log(torch.tensor(10000.0)) / embedding_dim)\r\n        )\r\n        pe[:, 0::2] = torch.sin(position * div_term)\r\n        pe[:, 1::2] = torch.cos(position * div_term)\r\n        pe = pe.unsqueeze(0).transpose(0, 1)\r\n        self.register_buffer(\'pe\', pe)\r\n\r\n    def forward(self, x):\r\n        x = x + self.pe[: x.size(0), :]\r\n        return x\r\n\r\nclass TransformerEncoder(torch.nn.Module):\r\n    def __init__(self, embed_dim, num_heads, dropout, feedforward_dim):\r\n        super().__init__()\r\n        self.attn = torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\r\n        self.linear_1 = torch.nn.Linear(embed_dim, feedforward_dim)\r\n        self.linear_2 = torch.nn.Linear(feedforward_dim, embed_dim)\r\n        self.layernorm_1 = torch.nn.LayerNorm(embed_dim)\r\n        self.layernorm_2 = torch.nn.LayerNorm(embed_dim)\r\n\r\n    def forward(self, x_in):\r\n        attn_out, _ = self.attn(x_in, x_in, x_in)\r\n        x = self.layernorm_1(x_in + attn_out)\r\n        ff_out = self.linear_2(torch.nn.functional.relu(self.linear_1(x)))\r\n        x = self.layernorm_2(x + ff_out)\r\n        return x\r\n\r\nclass Csi_Encoder(nn.Module):\r\n\r\n    def __init__(self, feedback_bits):\r\n        super(Csi_Encoder, self).__init__()\r\n\r\n        self.convban = nn.Sequential(OrderedDict([\r\n            ("conv3x3_bn", ConvBN_linear(1, 2, 1)),\r\n        ]))\r\n        self.fc = nn.Linear(2048, int(feedback_bits))\r\n\r\n    def forward(self, x_in):\r\n        x_in = x_in.view(32,1,32,32)\r\n        out = self.convban(x_in)\r\n        out = out.view(32,-1)\r\n        out = self.fc(out)\r\n        return out\r\n\r\nclass Csi_Decoder(nn.Module):\r\n\r\n    def __init__(self, feedback_bits):\r\n        super(Csi_Decoder, self).__init__()\r\n\r\n        self.feedback_bits = feedback_bits\r\n        self.fc = nn.Linear(int(feedback_bits), 1024)\r\n        decoder = OrderedDict([\r\n            ("decoder1",ResBlock(1)),\r\n            (\'LeakyReLU\', nn.LeakyReLU(negative_slope=0.3, inplace=False)),\r\n            ("decoder2",ResBlock(1))\r\n        ])\r\n        self.decoder_feature = nn.Sequential(decoder)\r\n        self.out_cov = ConvBN_linear(1,1,3)\r\n        self.sig = nn.Sigmoid()\r\n\r\n    def forward(self, x):\r\n        out = x\r\n        out = self.fc(out)\r\n        out = out.view(32, 1, -1, 32)\r\n        out = self.decoder_feature(out)\r\n        out = self.out_cov(out)\r\n        out = self.sig(out)\r\n        out = out.view(32,2,16,32)\r\n        return out\r\n\r\nclass Csi_Attention_Encoder(nn.Module):\r\n\r\n    def __init__(self, feedback_bits):\r\n        super(Csi_Attention_Encoder, self).__init__()\r\n        \r\n        # with positional encoding\r\n        # self.patch_embedding = nn.Sequential(OrderedDict([\r\n        #     ("patch_embedding", PatchEmbed(H=16, W=32, patch_size=4, in_chans=2, embed_dim=32))\r\n        # ]))\r\n        # self.positional_encoding = nn.Sequential(OrderedDict([\r\n        #     ("positional_encoding", FixedPositionalEncoding(32,32))\r\n        # ]))\r\n        # self.transformer_layer =  nn.Sequential(OrderedDict([\r\n        #     ("transformer_encoder1", TransformerEncoder(32,8,0,512)) # after [32, 512, 32]\r\n        # ])) # for added positional encoding\r\n        \r\n        # without positional encoding\r\n        self.conv_layer = ConvBN_linear(1,2,1)\r\n        self.transformer_layer = nn.Sequential(OrderedDict([\r\n\r\n                ("transformer_encoder1", TransformerEncoder(64,8,0,512))\r\n            ])) # without positional encoding\r\n        self.fc = nn.Linear(2048, int(feedback_bits))\r\n\r\n    def forward(self, x_in):\r\n\r\n        # with pos encoding\r\n        ##x_in = self.patch_embedding(x_in)\r\n        ##x_in = self.positional_encoding(x_in)\r\n        # without pos encoding\r\n        x_in = x_in.view(32,1,32,32)\r\n        x_in = self.conv_layer(x_in)\r\n\r\n        x_in = x_in.view(32,32,64)\r\n        out = self.transformer_layer(x_in)\r\n        #out = out.contiguous().view(32,-1) with pos encoding\r\n        out = out.contiguous().view(-1, 2048) # without pos encoding\r\n        out = self.fc(out)\r\n        return out\r\n\r\nclass Csi_Attention_Decoder(nn.Module):\r\n\r\n    def __init__(self, feedback_bits):\r\n        super(Csi_Attention_Decoder, self).__init__()\r\n\r\n        self.feedback_bits = feedback_bits\r\n        self.fc = nn.Linear(int(feedback_bits), 2048)\r\n        decoder = OrderedDict([\r\n            ("transformer_decoder1",TransformerEncoder(64,8,0,feedforward_dim=128))\r\n        ])\r\n        self.decoder_feature = nn.Sequential(decoder)\r\n        self.conv_linear = ConvBN_linear(2,1,1)\r\n        self.sig = nn.Sigmoid()\r\n\r\n    def forward(self, x):\r\n        out = x\r\n        out = self.fc(out) \r\n        out = out.view(32, -1, 64)\r\n        out = self.decoder_feature(out)\r\n        out = out.view([32,2,32,32])\r\n        out = self.conv_linear(out)\r\n        out = self.sig(out)\r\n        out = out.view(32,2,16,32)\r\n        return out\r\n\r\nclass Csi_Net(nn.Module):\r\n\r\n    def __init__(self, feedback_bits):\r\n        super(Csi_Net, self).__init__()\r\n        self.encoder = Csi_Encoder(feedback_bits)\r\n        self.decoder = Csi_Decoder(feedback_bits)\r\n\r\n    def forward(self, x):\r\n        feature = self.encoder(x)\r\n        out = self.decoder(feature)\r\n        return out\r\n\r\nclass Csi_Transformer_Net(nn.Module):\r\n\r\n    def __init__(self, feedback_bits):\r\n        super(Csi_Transformer_Net, self).__init__()\r\n        self.encoder = Csi_Attention_Encoder(feedback_bits)\r\n        self.decoder = Csi_Attention_Decoder(feedback_bits)\r\n\r\n    def forward(self, x):\r\n        feature = self.encoder(x)\r\n        out = self.decoder(feature)\r\n        return out\r\n\r\nclass CS_Net(nn.Module):\r\n\r\n    def __init__(self, feedback_bits):\r\n        super(CS_Net, self).__init__()\r\n        self.A = np.random.uniform(low=-0.5, high=0.5, size=(1024, feedback_bits))\r\n        self.A = torch.from_numpy(self.A)\r\n        self.A = self.A.float()\r\n        self.decoder = Csi_Decoder(feedback_bits)\r\n\r\n    def forward(self, x):\r\n        \r\n        x = x.view(32, -1)\r\n        out = x @ self.A\r\n        out = self.decoder(out)\r\n        return out\r\n\r\nclass Csi_CNN_Transformer_Net(nn.Module):\r\n\r\n    def __init__(self, feedback_bits):\r\n        super(Csi_CNN_Transformer_Net, self).__init__()\r\n        self.encoder = Csi_Encoder(feedback_bits)\r\n        self.decoder = Csi_Attention_Decoder(feedback_bits)\r\n\r\n    def forward(self, x):\r\n        feature = self.encoder(x)\r\n        out = self.decoder(feature)\r\n        return out\n'})}),"\n",(0,s.jsx)(n.h2,{id:"image-segmentation",children:"image segmentation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import matplotlib as plt\r\nimport os\r\nimport random\r\nimport time\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\ngpu_list = '0'\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_list\r\n\r\n\r\ndef seed_everything(seed=42):\r\n    random.seed(seed)\r\n    os.environ['PYTHONHASHSEED'] = str(seed)\r\n    np.random.seed(seed)\r\n    torch.manual_seed(seed)\r\n    torch.cuda.manual_seed(seed)\r\n    torch.backends.cudnn.deterministic = True\r\n\r\ndef channel_visualization(image):\r\n    fig, ax = plt.subplots()\r\n    ax.imshow(image, cmap=plt.cm.gray, interpolation='nearest', origin='upper')\r\n    ax.spines['left'].set_position(('outward', 10))\r\n    ax.spines['bottom'].set_position(('outward', 10))\r\n    ax.spines['right'].set_visible(False)\r\n    ax.spines['top'].set_visible(False)\r\n    ax.yaxis.set_ticks_position('left')\r\n    ax.xaxis.set_ticks_position('bottom')\r\n    plt.show()\r\n\r\nSEED = 42\r\nprint(\"seeding everything...\")\r\nseed_everything(SEED)\r\nprint(\"initializing parameters...\")\r\n\r\n\r\nclass model_trainer():\r\n\r\n    def __init__(self,\r\n                 epochs,\r\n                 net,\r\n                 feedbackbits=128,\r\n                 batch_size=32,\r\n                 learning_rate=1e-3,\r\n                 lr_decay_freq=30,\r\n                 lr_decay=0.1,\r\n                 best_loss=100,\r\n                 num_workers=0,\r\n                 print_freq=100,\r\n                 train_test_ratio=0.8):\r\n\r\n        self.epochs = epochs\r\n        self.batch_size = batch_size\r\n        self.learning_rate = learning_rate\r\n        self.lr_decay_freq = lr_decay_freq\r\n        self.lr_decay = lr_decay\r\n        self.best_loss = best_loss\r\n        self.num_workers = num_workers\r\n        self.print_freq = print_freq\r\n        self.train_test_ratio = train_test_ratio\r\n        # parameters for data\r\n        self.feedback_bits = feedbackbits\r\n        self.img_height = 16\r\n        self.img_width = 32\r\n        self.img_channels = 2\r\n\r\n        self.model = eval(net)(self.feedback_bits)\r\n        self.x_label = []\r\n        self.y_label = []\r\n        self.ys_label = []\r\n        self.t_label = []\r\n\r\n        # if len(gpu_list.split(',')) > 1:\r\n        #     self.model = torch.nn.DataParallel(self.model).cuda()  # model.module\r\n        # else:\r\n        #     self.model = self.model.cuda()\r\n        self.model = self.model\r\n        self.criterion = NMSELoss(reduction='mean')  # nn.MSELoss()\r\n        self.criterion_test = NMSELoss(reduction='sum')\r\n        # self.criterion_rho = CosSimilarity(reduction='mean')\r\n        # self.criterion_test_rho = CosSimilarity(reduction='sum')\r\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)\r\n\r\n        # train_loader, test_loader, train_dataset, test_dataset, \\\r\n        # train_shuffle_loader, test_shuffle_loader, train_shuffle_dataset, test_shuffle_dataset\r\n\r\n        self.train_loader, self.test_loader, self.train_dataset,        self.test_dataset, self.train_shuffle_loader, self.test_shuffle_loader,        self.train_shuffle_dataset, self.test_shuffle_dataset =             load_data('data/',shuffle = True)\r\n\r\n    def model_save(self, model_path):\r\n        print('Saving the entire model...')\r\n        torch.save(self.model, model_path)\r\n        print(f'Model saved to {model_path}')\r\n\r\n        print('Model saved!')\r\n        self.best_loss = self.average_loss\r\n\r\n    def model_train(self):\r\n\r\n        for epoch in range(self.epochs):\r\n            print('========================')\r\n            print('lr:%.4e' % self.optimizer.param_groups[0]['lr'])\r\n            # train model\r\n            self.model.train()\r\n   \r\n            # decay lr\r\n            if epoch % self.lr_decay_freq == 0 and epoch > 0:\r\n                self.optimizer.param_groups[0]['lr'] = self.optimizer.param_groups[0]['lr'] * self.lr_decay\r\n\r\n            # training...\r\n            for i, input in enumerate(self.train_loader):\r\n                # input = input.cuda()  # input [batch=32,2,16,32]\r\n                output = self.model(input)\r\n                loss = self.criterion(output, input)\r\n                loss.backward()\r\n                self.optimizer.step()\r\n                self.optimizer.zero_grad()\r\n                if i % self.print_freq == 0:\r\n                    print('Epoch: [{0}][{1}/{2}]\\t'\r\n                          'Loss {loss:.4f}\\t'.format(\r\n                        epoch, i, len(self.train_loader), loss=loss.item()))\r\n            self.model.eval()\r\n\r\n            # evaluating...\r\n            self.total_loss = 0\r\n            self.total_rho = 0\r\n            start = time.time()\r\n            with torch.no_grad():\r\n\r\n                for i, input in enumerate(self.test_loader):\r\n                    \r\n                    # input = input.cuda()\r\n                    output = self.model(input)\r\n                    self.total_loss += self.criterion_test(output, input).item()\r\n                    # self.total_rho += self.criterion_rho(output,input).item()\r\n                    #print(rho(output,input), type(rho(output,input)))\r\n                    self.total_rho += (rho(output,input))\r\n                    \r\n                end = time.time()\r\n                t = end - start\r\n                self.average_loss = self.total_loss / len(self.test_dataset)\r\n                self.average_rho = self.total_rho / len(list(enumerate(self.test_loader)))\r\n                self.x_label.append(epoch)\r\n                self.y_label.append(self.average_loss)\r\n                self.t_label.append(t)\r\n                print('NMSE %.4f \u03c1 %.3f time %.3f' % (self.average_loss,self.average_rho, t))\r\n\r\n        for i, input in enumerate(self.test_loader): # visualize one sample\r\n            if i == 3: # set shuffle = False to ensure the same sample each time\r\n                ones = torch.ones(32,32)\r\n                image1 = input[0].view(32,32)\r\n                image1 = ones - image1\r\n                image1 = image1.numpy()\r\n                channel_visualization(image1)\r\n                # input = input.cuda()\r\n                output = self.model(input)\r\n                output = output.cpu()\r\n                image2 = output[0].view(32,32)\r\n                image2 = ones - image2\r\n                image2 = image2.detach().numpy()\r\n                channel_visualization(image2)\r\n\r\n        return self.x_label, self.y_label, sum(self.t_label)/len(self.t_label) # , self.ys_label\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"seeding everything...\r\ninitializing parameters...\n"})}),"\n",(0,s.jsx)(n.h2,{id:"train",children:"train"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport h5py\r\nimport torch\r\nimport os\r\nimport torch.nn as nn\r\nimport random\r\n\r\n# gpu_list = '0'\r\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_list\r\n\r\ndef seed_everything(seed=42):\r\n    random.seed(seed)\r\n    os.environ['PYTHONHASHSEED'] = str(seed)\r\n    np.random.seed(seed)\r\n    torch.manual_seed(seed)\r\n    torch.cuda.manual_seed(seed)\r\n    torch.backends.cudnn.deterministic = True\r\n\r\nSEED = 42\r\nseed_everything(SEED)\r\n\r\n\r\n# bits = 256\r\nbits = 128\r\n# bits = 64\r\n# bits = 32\r\n\r\n\n"})}),"\n",(0,s.jsx)(n.h2,{id:"csi_net",children:"Csi_Net"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'print("="*30)\r\nprint("Encoder: CNN; Decoder: CNN")\r\nprint("compressed codeword bits: {}".format(bits))\r\nagent3 = model_trainer(epochs=40, net="Csi_Net",feedbackbits=bits)\r\nx3, agent3_NMSE, t3 = agent3.model_train()\r\nprint("Csi_Net")\r\nprint(agent3_NMSE)\r\nprint("average time used is:", t3)\r\nplt.plot(x3, agent3_NMSE, label="cnn")\r\n # \u4fdd\u5b58\u6a21\u578b\r\nmodel_path = f\'models/Csi_Net_model_{bits}.pth\'\r\nagent3.model_save(model_path)\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"==============================\r\nEncoder: CNN; Decoder: CNN\r\ncompressed codeword bits: 128\r\nloading data...\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [0][0/200]\tLoss 14.1170\t\r\nEpoch: [0][100/200]\tLoss 1.6231\t\r\nNMSE 1.3334 \u03c1 0.058 time 1.204\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [1][0/200]\tLoss 1.2072\t\r\nEpoch: [1][100/200]\tLoss 1.3410\t\r\nNMSE 1.1423 \u03c1 0.174 time 0.573\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [2][0/200]\tLoss 1.1115\t\r\nEpoch: [2][100/200]\tLoss 1.1585\t\r\nNMSE 1.0452 \u03c1 0.293 time 0.535\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [3][0/200]\tLoss 1.0717\t\r\nEpoch: [3][100/200]\tLoss 0.9346\t\r\nNMSE 0.8754 \u03c1 0.463 time 1.153\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [4][0/200]\tLoss 0.8362\t\r\nEpoch: [4][100/200]\tLoss 0.6380\t\r\nNMSE 0.6576 \u03c1 0.614 time 0.521\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [5][0/200]\tLoss 0.6203\t\r\nEpoch: [5][100/200]\tLoss 0.5353\t\r\nNMSE 0.5380 \u03c1 0.693 time 0.568\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [6][0/200]\tLoss 0.5071\t\r\nEpoch: [6][100/200]\tLoss 0.5355\t\r\nNMSE 0.4787 \u03c1 0.733 time 0.545\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [7][0/200]\tLoss 0.4867\t\r\nEpoch: [7][100/200]\tLoss 0.4088\t\r\nNMSE 0.4406 \u03c1 0.759 time 0.640\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [8][0/200]\tLoss 0.4613\t\r\nEpoch: [8][100/200]\tLoss 0.4002\t\r\nNMSE 0.4013 \u03c1 0.782 time 0.528\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [9][0/200]\tLoss 0.3757\t\r\nEpoch: [9][100/200]\tLoss 0.3755\t\r\nNMSE 0.3983 \u03c1 0.787 time 0.537\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [10][0/200]\tLoss 0.3914\t\r\nEpoch: [10][100/200]\tLoss 0.4193\t\r\nNMSE 0.4489 \u03c1 0.762 time 0.529\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [11][0/200]\tLoss 0.4755\t\r\nEpoch: [11][100/200]\tLoss 0.3684\t\r\nNMSE 0.3594 \u03c1 0.809 time 0.515\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [12][0/200]\tLoss 0.3788\t\r\nEpoch: [12][100/200]\tLoss 0.3704\t\r\nNMSE 0.3356 \u03c1 0.820 time 0.580\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [13][0/200]\tLoss 0.3617\t\r\nEpoch: [13][100/200]\tLoss 0.3398\t\r\nNMSE 0.3221 \u03c1 0.827 time 0.750\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [14][0/200]\tLoss 0.3272\t\r\nEpoch: [14][100/200]\tLoss 0.4293\t\r\nNMSE 0.3350 \u03c1 0.820 time 0.525\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [15][0/200]\tLoss 0.3543\t\r\nEpoch: [15][100/200]\tLoss 0.3455\t\r\nNMSE 0.3119 \u03c1 0.835 time 0.516\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [16][0/200]\tLoss 0.2992\t\r\nEpoch: [16][100/200]\tLoss 0.9529\t\r\nNMSE 0.2986 \u03c1 0.840 time 0.599\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [17][0/200]\tLoss 0.2856\t\r\nEpoch: [17][100/200]\tLoss 0.3119\t\r\nNMSE 0.2954 \u03c1 0.843 time 0.573\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [18][0/200]\tLoss 0.2677\t\r\nEpoch: [18][100/200]\tLoss 0.2961\t\r\nNMSE 0.2801 \u03c1 0.850 time 0.847\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [19][0/200]\tLoss 0.2459\t\r\nEpoch: [19][100/200]\tLoss 0.2546\t\r\nNMSE 0.2758 \u03c1 0.852 time 0.615\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [20][0/200]\tLoss 0.2563\t\r\nEpoch: [20][100/200]\tLoss 0.2762\t\r\nNMSE 0.2699 \u03c1 0.855 time 0.553\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [21][0/200]\tLoss 0.2847\t\r\nEpoch: [21][100/200]\tLoss 0.2760\t\r\nNMSE 0.2617 \u03c1 0.859 time 0.501\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [22][0/200]\tLoss 0.2808\t\r\nEpoch: [22][100/200]\tLoss 0.2431\t\r\nNMSE 0.2645 \u03c1 0.856 time 1.056\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [23][0/200]\tLoss 0.2779\t\r\nEpoch: [23][100/200]\tLoss 0.2001\t\r\nNMSE 0.2553 \u03c1 0.863 time 1.128\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [24][0/200]\tLoss 0.2286\t\r\nEpoch: [24][100/200]\tLoss 0.2272\t\r\nNMSE 0.2539 \u03c1 0.864 time 0.647\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [25][0/200]\tLoss 0.2420\t\r\nEpoch: [25][100/200]\tLoss 0.2283\t\r\nNMSE 0.2385 \u03c1 0.871 time 0.533\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [26][0/200]\tLoss 0.1934\t\r\nEpoch: [26][100/200]\tLoss 0.2241\t\r\nNMSE 0.2377 \u03c1 0.871 time 0.531\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [27][0/200]\tLoss 0.2343\t\r\nEpoch: [27][100/200]\tLoss 0.2251\t\r\nNMSE 0.2111 \u03c1 0.884 time 0.531\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [28][0/200]\tLoss 0.1790\t\r\nEpoch: [28][100/200]\tLoss 0.1600\t\r\nNMSE 0.2141 \u03c1 0.883 time 0.520\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [29][0/200]\tLoss 0.1748\t\r\nEpoch: [29][100/200]\tLoss 0.1895\t\r\nNMSE 0.2047 \u03c1 0.888 time 0.562\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [30][0/200]\tLoss 0.1748\t\r\nEpoch: [30][100/200]\tLoss 0.1557\t\r\nNMSE 0.1984 \u03c1 0.892 time 0.519\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [31][0/200]\tLoss 0.1814\t\r\nEpoch: [31][100/200]\tLoss 0.2022\t\r\nNMSE 0.1978 \u03c1 0.892 time 0.544\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [32][0/200]\tLoss 0.1897\t\r\nEpoch: [32][100/200]\tLoss 0.1875\t\r\nNMSE 0.1973 \u03c1 0.893 time 0.514\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [33][0/200]\tLoss 0.1683\t\r\nEpoch: [33][100/200]\tLoss 0.1652\t\r\nNMSE 0.1970 \u03c1 0.893 time 0.532\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [34][0/200]\tLoss 0.2179\t\r\nEpoch: [34][100/200]\tLoss 0.1884\t\r\nNMSE 0.1967 \u03c1 0.893 time 0.513\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [35][0/200]\tLoss 0.1835\t\r\nEpoch: [35][100/200]\tLoss 0.1834\t\r\nNMSE 0.1962 \u03c1 0.893 time 0.618\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [36][0/200]\tLoss 0.1966\t\r\nEpoch: [36][100/200]\tLoss 0.1879\t\r\nNMSE 0.1959 \u03c1 0.893 time 0.558\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [37][0/200]\tLoss 0.1554\t\r\nEpoch: [37][100/200]\tLoss 0.2014\t\r\nNMSE 0.1955 \u03c1 0.894 time 0.532\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [38][0/200]\tLoss 0.1609\t\r\nEpoch: [38][100/200]\tLoss 0.1458\t\r\nNMSE 0.1952 \u03c1 0.894 time 0.611\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [39][0/200]\tLoss 0.1746\t\r\nEpoch: [39][100/200]\tLoss 0.2031\t\r\nNMSE 0.1949 \u03c1 0.894 time 0.560\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:e(38680).A+"",width:"430",height:"427"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:e(34291).A+"",width:"430",height:"427"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Csi_Net\r\n[1.3333566904067993, 1.1422800970077516, 1.0451844179630279, 0.8753661751747132, 0.6575556528568268, 0.5379940551519394, 0.4787355536222458, 0.4405597126483917, 0.40132262706756594, 0.39827447295188906, 0.4489418357610703, 0.3594482445716858, 0.33555667102336884, 0.322117959856987, 0.3350206232070923, 0.31192928612232207, 0.29864764750003814, 0.29539691090583803, 0.2801084813475609, 0.27577628135681154, 0.2699457517266273, 0.2617357462644577, 0.264499531686306, 0.2552609643340111, 0.2539336371421814, 0.2384874066710472, 0.237672161757946, 0.211108820438385, 0.2141134124994278, 0.20471315175294877, 0.1983871877193451, 0.1978167027235031, 0.19734854340553284, 0.19701571255922318, 0.1967495545744896, 0.1962449449300766, 0.19589142352342606, 0.1955234381556511, 0.19522991210222243, 0.1948663568496704]\r\naverage time used is: 0.6228328704833984\r\nSaving the entire model...\r\nModel saved to models/Csi_Net_model_128.pth\r\nModel saved!\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:e(89157).A+"",width:"547",height:"413"})}),"\n",(0,s.jsx)(n.h2,{id:"csi_transformer_net",children:"Csi_Transformer_Net"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'print("="*30)\r\nprint("Encoder: transformer; Decoder: transformer")\r\nprint("compressed codeword bits: {}".format(bits))\r\nagent1 = model_trainer(epochs=40, net="Csi_Transformer_Net",feedbackbits=bits)\r\nx1, agent1_NMSE, t1 = agent1.model_train()\r\nprint("Csi_Transformer_Net")\r\nprint(agent1_NMSE)\r\nprint("average time used is:", t1)\r\nplt.plot(x1, agent1_NMSE, label="Csi_Transformer_Net")\r\n# \u4fdd\u5b58\u6a21\u578b\r\nmodel_path = f\'models/Csi_Transformer_Net_model_{bits}.pth\'\r\nagent1.model_save(model_path)\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"==============================\r\nEncoder: transformer; Decoder: transformer\r\ncompressed codeword bits: 128\r\nloading data...\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [0][0/200]\tLoss 16.8835\t\r\nEpoch: [0][100/200]\tLoss 2.8815\t\r\nNMSE 1.9566 \u03c1 0.061 time 1.629\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [1][0/200]\tLoss 1.9483\t\r\nEpoch: [1][100/200]\tLoss 1.5439\t\r\nNMSE 1.3547 \u03c1 0.370 time 1.601\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [2][0/200]\tLoss 1.3673\t\r\nEpoch: [2][100/200]\tLoss 1.2373\t\r\nNMSE 1.1730 \u03c1 0.497 time 1.647\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [3][0/200]\tLoss 1.0943\t\r\nEpoch: [3][100/200]\tLoss 1.0867\t\r\nNMSE 1.0903 \u03c1 0.565 time 1.658\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [4][0/200]\tLoss 1.0267\t\r\nEpoch: [4][100/200]\tLoss 1.0023\t\r\nNMSE 1.0311 \u03c1 0.614 time 0.919\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [5][0/200]\tLoss 1.0738\t\r\nEpoch: [5][100/200]\tLoss 0.9662\t\r\nNMSE 0.9886 \u03c1 0.622 time 0.737\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [6][0/200]\tLoss 0.9682\t\r\nEpoch: [6][100/200]\tLoss 0.9577\t\r\nNMSE 0.9677 \u03c1 0.648 time 0.792\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [7][0/200]\tLoss 0.8769\t\r\nEpoch: [7][100/200]\tLoss 0.9669\t\r\nNMSE 0.9511 \u03c1 0.663 time 0.759\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [8][0/200]\tLoss 0.9183\t\r\nEpoch: [8][100/200]\tLoss 0.8877\t\r\nNMSE 0.9386 \u03c1 0.662 time 0.894\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [9][0/200]\tLoss 0.9066\t\r\nEpoch: [9][100/200]\tLoss 0.8874\t\r\nNMSE 0.7703 \u03c1 0.700 time 0.754\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [10][0/200]\tLoss 0.7551\t\r\nEpoch: [10][100/200]\tLoss 0.6318\t\r\nNMSE 0.6484 \u03c1 0.733 time 0.902\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [11][0/200]\tLoss 0.6150\t\r\nEpoch: [11][100/200]\tLoss 0.6004\t\r\nNMSE 0.5911 \u03c1 0.756 time 0.774\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [12][0/200]\tLoss 0.5684\t\r\nEpoch: [12][100/200]\tLoss 0.5429\t\r\nNMSE 0.5425 \u03c1 0.755 time 0.776\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [13][0/200]\tLoss 0.5027\t\r\nEpoch: [13][100/200]\tLoss 0.5482\t\r\nNMSE 0.5385 \u03c1 0.757 time 0.773\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [14][0/200]\tLoss 0.5368\t\r\nEpoch: [14][100/200]\tLoss 0.4899\t\r\nNMSE 0.5361 \u03c1 0.760 time 0.761\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [15][0/200]\tLoss 0.5421\t\r\nEpoch: [15][100/200]\tLoss 0.5330\t\r\nNMSE 0.5345 \u03c1 0.762 time 0.777\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [16][0/200]\tLoss 0.4967\t\r\nEpoch: [16][100/200]\tLoss 0.5264\t\r\nNMSE 0.5326 \u03c1 0.759 time 0.809\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [17][0/200]\tLoss 0.5447\t\r\nEpoch: [17][100/200]\tLoss 0.5212\t\r\nNMSE 0.5316 \u03c1 0.767 time 0.911\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [18][0/200]\tLoss 0.4953\t\r\nEpoch: [18][100/200]\tLoss 0.5240\t\r\nNMSE 0.5301 \u03c1 0.761 time 0.918\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [19][0/200]\tLoss 0.4699\t\r\nEpoch: [19][100/200]\tLoss 0.5144\t\r\nNMSE 0.5300 \u03c1 0.760 time 0.775\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [20][0/200]\tLoss 0.5295\t\r\nEpoch: [20][100/200]\tLoss 0.5224\t\r\nNMSE 0.5300 \u03c1 0.761 time 0.792\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [21][0/200]\tLoss 0.4950\t\r\nEpoch: [21][100/200]\tLoss 0.5359\t\r\nNMSE 0.5283 \u03c1 0.764 time 0.821\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [22][0/200]\tLoss 0.5202\t\r\nEpoch: [22][100/200]\tLoss 0.5535\t\r\nNMSE 0.5279 \u03c1 0.765 time 0.844\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [23][0/200]\tLoss 0.5108\t\r\nEpoch: [23][100/200]\tLoss 0.5283\t\r\nNMSE 0.5328 \u03c1 0.750 time 0.785\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [24][0/200]\tLoss 0.5188\t\r\nEpoch: [24][100/200]\tLoss 0.4741\t\r\nNMSE 0.5282 \u03c1 0.761 time 0.776\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [25][0/200]\tLoss 0.4862\t\r\nEpoch: [25][100/200]\tLoss 0.4990\t\r\nNMSE 0.5272 \u03c1 0.761 time 1.599\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [26][0/200]\tLoss 0.4873\t\r\nEpoch: [26][100/200]\tLoss 0.4856\t\r\nNMSE 0.5251 \u03c1 0.764 time 1.647\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [27][0/200]\tLoss 0.4745\t\r\nEpoch: [27][100/200]\tLoss 0.5390\t\r\nNMSE 0.5240 \u03c1 0.765 time 1.595\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [28][0/200]\tLoss 0.4830\t\r\nEpoch: [28][100/200]\tLoss 0.3965\t\r\nNMSE 0.4337 \u03c1 0.792 time 1.619\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [29][0/200]\tLoss 0.4264\t\r\nEpoch: [29][100/200]\tLoss 0.4338\t\r\nNMSE 0.4340 \u03c1 0.784 time 1.655\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [30][0/200]\tLoss 0.4309\t\r\nEpoch: [30][100/200]\tLoss 0.4537\t\r\nNMSE 0.4268 \u03c1 0.795 time 1.595\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [31][0/200]\tLoss 0.4027\t\r\nEpoch: [31][100/200]\tLoss 0.4340\t\r\nNMSE 0.4264 \u03c1 0.796 time 1.602\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [32][0/200]\tLoss 0.4187\t\r\nEpoch: [32][100/200]\tLoss 0.3953\t\r\nNMSE 0.4264 \u03c1 0.794 time 1.624\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [33][0/200]\tLoss 0.4534\t\r\nEpoch: [33][100/200]\tLoss 0.3923\t\r\nNMSE 0.4260 \u03c1 0.797 time 1.643\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [34][0/200]\tLoss 0.3964\t\r\nEpoch: [34][100/200]\tLoss 0.4492\t\r\nNMSE 0.4259 \u03c1 0.797 time 1.596\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [35][0/200]\tLoss 0.4250\t\r\nEpoch: [35][100/200]\tLoss 0.4602\t\r\nNMSE 0.4257 \u03c1 0.797 time 1.619\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [36][0/200]\tLoss 0.3898\t\r\nEpoch: [36][100/200]\tLoss 0.3978\t\r\nNMSE 0.4255 \u03c1 0.796 time 1.591\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [37][0/200]\tLoss 0.3960\t\r\nEpoch: [37][100/200]\tLoss 0.3914\t\r\nNMSE 0.4252 \u03c1 0.798 time 1.620\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [38][0/200]\tLoss 0.4531\t\r\nEpoch: [38][100/200]\tLoss 0.4079\t\r\nNMSE 0.3436 \u03c1 0.836 time 1.630\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [39][0/200]\tLoss 0.3105\t\r\nEpoch: [39][100/200]\tLoss 0.3446\t\r\nNMSE 0.3365 \u03c1 0.839 time 1.663\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:e(23910).A+"",width:"430",height:"427"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:e(65149).A+"",width:"430",height:"427"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Csi_Transformer_Net\r\n[1.9566105437278747, 1.3546967101097107, 1.1729794073104858, 1.0902969622612, 1.0310835707187653, 0.9886464655399323, 0.967725396156311, 0.9511337721347809, 0.9386063098907471, 0.7703430867195129, 0.6483797013759613, 0.5910915243625641, 0.5425180727243424, 0.5384866672754288, 0.536125123500824, 0.5344688516855239, 0.5326178723573685, 0.5316342955827713, 0.5301343083381653, 0.5299815499782562, 0.5299837756156921, 0.5282985603809357, 0.5278837376832962, 0.5328169310092926, 0.5282338237762452, 0.527239038348198, 0.5250765424966812, 0.523982360959053, 0.4337254935503006, 0.4340094310045242, 0.4267952162027359, 0.42642789661884306, 0.4263604950904846, 0.426031568646431, 0.4258938354253769, 0.4257040703296661, 0.4255121678113937, 0.42521106243133544, 0.34364676058292387, 0.3364664989709854]\r\naverage time used is: 1.1970998823642731\r\nSaving the entire model...\r\nModel saved to models/Csi_Transformer_Net_model_128.pth\r\nModel saved!\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:e(89931).A+"",width:"547",height:"413"})}),"\n",(0,s.jsx)(n.h2,{id:"csi_cnn_transformer_net",children:"Csi_CNN_Transformer_Net"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'print("="*30)\r\nprint("Encoder: CNN; Decoder: transformer")\r\nprint("compressed codeword bits: {}".format(bits))\r\nagent2 = model_trainer(epochs=40, net="Csi_CNN_Transformer_Net",feedbackbits=bits)\r\nx2, agent2_NMSE, t2 = agent2.model_train()\r\nprint("Csi_CNN_Transformer_Net")\r\nprint(agent2_NMSE)\r\nprint("average time used is:", t2)\r\nplt.plot(x2, agent2_NMSE, label="Csi_CNN_Transformer_Net")\r\n\r\nprint(x2)\r\nplt.show()\r\n\r\n# \u4fdd\u5b58\u6a21\u578b\r\nmodel_path = f\'models/Csi_CNN_Transformer_Net_model_{bits}.pth\'\r\nagent2.model_save(model_path)\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"==============================\r\nEncoder: CNN; Decoder: transformer\r\ncompressed codeword bits: 128\r\nloading data...\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [0][0/200]\tLoss 16.7723\t\r\nEpoch: [0][100/200]\tLoss 2.7850\t\r\nNMSE 1.7937 \u03c1 0.337 time 0.447\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [1][0/200]\tLoss 1.8238\t\r\nEpoch: [1][100/200]\tLoss 1.4794\t\r\nNMSE 1.3830 \u03c1 0.503 time 0.437\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [2][0/200]\tLoss 1.2672\t\r\nEpoch: [2][100/200]\tLoss 1.2981\t\r\nNMSE 1.2839 \u03c1 0.556 time 0.449\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [3][0/200]\tLoss 1.1113\t\r\nEpoch: [3][100/200]\tLoss 1.1200\t\r\nNMSE 1.1895 \u03c1 0.638 time 0.518\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [4][0/200]\tLoss 1.1125\t\r\nEpoch: [4][100/200]\tLoss 1.1114\t\r\nNMSE 0.8790 \u03c1 0.593 time 0.876\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [5][0/200]\tLoss 0.8571\t\r\nEpoch: [5][100/200]\tLoss 0.6774\t\r\nNMSE 1.9164 \u03c1 0.755 time 0.913\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [6][0/200]\tLoss 0.6592\t\r\nEpoch: [6][100/200]\tLoss 0.5653\t\r\nNMSE 0.5816 \u03c1 0.758 time 0.912\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [7][0/200]\tLoss 0.5619\t\r\nEpoch: [7][100/200]\tLoss 0.5827\t\r\nNMSE 0.6244 \u03c1 0.668 time 0.936\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [8][0/200]\tLoss 0.5068\t\r\nEpoch: [8][100/200]\tLoss 0.5359\t\r\nNMSE 0.4789 \u03c1 0.800 time 0.916\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [9][0/200]\tLoss 0.4513\t\r\nEpoch: [9][100/200]\tLoss 0.4735\t\r\nNMSE 0.8477 \u03c1 0.827 time 0.963\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [10][0/200]\tLoss 0.4208\t\r\nEpoch: [10][100/200]\tLoss 0.4186\t\r\nNMSE 0.4033 \u03c1 0.837 time 0.928\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [11][0/200]\tLoss 0.3055\t\r\nEpoch: [11][100/200]\tLoss 0.3709\t\r\nNMSE 0.3582 \u03c1 0.810 time 0.934\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [12][0/200]\tLoss 0.3048\t\r\nEpoch: [12][100/200]\tLoss 0.3369\t\r\nNMSE 0.3576 \u03c1 0.809 time 0.922\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [13][0/200]\tLoss 0.3596\t\r\nEpoch: [13][100/200]\tLoss 0.3335\t\r\nNMSE 0.3518 \u03c1 0.817 time 0.906\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [14][0/200]\tLoss 0.3324\t\r\nEpoch: [14][100/200]\tLoss 0.3278\t\r\nNMSE 0.3506 \u03c1 0.827 time 0.891\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [15][0/200]\tLoss 0.3422\t\r\nEpoch: [15][100/200]\tLoss 0.3615\t\r\nNMSE 0.3930 \u03c1 0.776 time 0.901\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [16][0/200]\tLoss 0.3087\t\r\nEpoch: [16][100/200]\tLoss 0.3223\t\r\nNMSE 0.4655 \u03c1 0.850 time 0.909\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [17][0/200]\tLoss 0.3008\t\r\nEpoch: [17][100/200]\tLoss 0.3295\t\r\nNMSE 0.3486 \u03c1 0.829 time 0.983\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [18][0/200]\tLoss 0.3196\t\r\nEpoch: [18][100/200]\tLoss 0.3202\t\r\nNMSE 0.3803 \u03c1 0.785 time 0.866\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [19][0/200]\tLoss 0.3409\t\r\nEpoch: [19][100/200]\tLoss 0.3137\t\r\nNMSE 0.3677 \u03c1 0.841 time 0.924\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [20][0/200]\tLoss 0.3369\t\r\nEpoch: [20][100/200]\tLoss 0.3256\t\r\nNMSE 0.3596 \u03c1 0.802 time 0.866\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [21][0/200]\tLoss 0.3814\t\r\nEpoch: [21][100/200]\tLoss 0.3145\t\r\nNMSE 0.3564 \u03c1 0.838 time 0.468\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [22][0/200]\tLoss 0.3593\t\r\nEpoch: [22][100/200]\tLoss 0.3371\t\r\nNMSE 0.3445 \u03c1 0.820 time 0.553\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [23][0/200]\tLoss 0.3151\t\r\nEpoch: [23][100/200]\tLoss 0.3410\t\r\nNMSE 0.3434 \u03c1 0.824 time 0.917\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [24][0/200]\tLoss 0.3600\t\r\nEpoch: [24][100/200]\tLoss 0.3348\t\r\nNMSE 0.5701 \u03c1 0.855 time 0.927\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [25][0/200]\tLoss 0.3392\t\r\nEpoch: [25][100/200]\tLoss 0.2966\t\r\nNMSE 0.3758 \u03c1 0.846 time 0.916\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [26][0/200]\tLoss 0.3510\t\r\nEpoch: [26][100/200]\tLoss 0.3204\t\r\nNMSE 0.3577 \u03c1 0.802 time 0.917\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [27][0/200]\tLoss 0.3344\t\r\nEpoch: [27][100/200]\tLoss 0.3121\t\r\nNMSE 0.4332 \u03c1 0.853 time 0.930\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [28][0/200]\tLoss 0.3257\t\r\nEpoch: [28][100/200]\tLoss 0.3527\t\r\nNMSE 0.3482 \u03c1 0.841 time 0.907\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [29][0/200]\tLoss 0.3031\t\r\nEpoch: [29][100/200]\tLoss 0.3429\t\r\nNMSE 0.3503 \u03c1 0.808 time 0.913\r\n========================\r\nlr:1.0000e-03\r\nEpoch: [30][0/200]\tLoss 0.3269\t\r\nEpoch: [30][100/200]\tLoss 0.3412\t\r\nNMSE 0.3313 \u03c1 0.830 time 0.920\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [31][0/200]\tLoss 0.3079\t\r\nEpoch: [31][100/200]\tLoss 0.3284\t\r\nNMSE 0.3356 \u03c1 0.841 time 0.924\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [32][0/200]\tLoss 0.3316\t\r\nEpoch: [32][100/200]\tLoss 0.3002\t\r\nNMSE 0.3334 \u03c1 0.824 time 0.919\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [33][0/200]\tLoss 0.3063\t\r\nEpoch: [33][100/200]\tLoss 0.2999\t\r\nNMSE 0.3316 \u03c1 0.838 time 0.920\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [34][0/200]\tLoss 0.3269\t\r\nEpoch: [34][100/200]\tLoss 0.3275\t\r\nNMSE 0.3298 \u03c1 0.831 time 0.918\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [35][0/200]\tLoss 0.3314\t\r\nEpoch: [35][100/200]\tLoss 0.3247\t\r\nNMSE 0.3295 \u03c1 0.834 time 0.921\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [36][0/200]\tLoss 0.2935\t\r\nEpoch: [36][100/200]\tLoss 0.3160\t\r\nNMSE 0.3294 \u03c1 0.835 time 0.922\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [37][0/200]\tLoss 0.2945\t\r\nEpoch: [37][100/200]\tLoss 0.3379\t\r\nNMSE 0.3291 \u03c1 0.835 time 0.917\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [38][0/200]\tLoss 0.2834\t\r\nEpoch: [38][100/200]\tLoss 0.3184\t\r\nNMSE 0.3293 \u03c1 0.830 time 0.907\r\n========================\r\nlr:1.0000e-04\r\nEpoch: [39][0/200]\tLoss 0.3403\t\r\nEpoch: [39][100/200]\tLoss 0.3393\t\r\nNMSE 0.3288 \u03c1 0.831 time 0.896\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:e(63844).A+"",width:"430",height:"427"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:e(93759).A+"",width:"430",height:"427"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Csi_CNN_Transformer_Net\r\n[1.7936574721336365, 1.3829833745956421, 1.283927230834961, 1.1894978594779968, 0.8790475904941559, 1.9164495015144347, 0.5816227388381958, 0.6243553447723389, 0.4789328819513321, 0.8476968479156494, 0.4033376920223236, 0.3581970351934433, 0.3575852298736572, 0.3518389576673508, 0.350602548122406, 0.3930432403087616, 0.46549117743968965, 0.3486467313766479, 0.3802634757757187, 0.36765024840831756, 0.3596446371078491, 0.35639182686805726, 0.34447405755519866, 0.34342944622039795, 0.5701418870687485, 0.3757738322019577, 0.35769821345806124, 0.4331714022159576, 0.34818855822086336, 0.35026067614555356, 0.3313265132904053, 0.3355947732925415, 0.33339386343955996, 0.33157447040081023, 0.3298351740837097, 0.3294666814804077, 0.32943468272686005, 0.32912233233451843, 0.3293388104438782, 0.3288260215520859]\r\naverage time used is: 0.850273048877716\r\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:e(89929).A+"",width:"547",height:"413"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Saving the entire model...\r\nModel saved to models/Csi_CNN_Transformer_Net_model_128.pth\r\nModel saved!\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import matplotlib.pyplot as plt\r\n\r\n# \u5047\u8bbe\u4f60\u6709\u4e09\u4e2a\u7f51\u7edc\u7684\u635f\u5931\u51fd\u6570\u7ed3\u679c\r\n\r\n# \u521b\u5efa\u4e00\u4e2a\u56fe\u5f62\r\nplt.figure(figsize=(8, 6))\r\n\r\n# \u7ed8\u5236\u7b2c\u4e00\u4e2a\u7f51\u7edc\u7684\u635f\u5931\u51fd\u6570\u66f2\u7ebf\r\nplt.plot(x1, agent1_NMSE, label="Csi_Transformer_Net", marker=\'o\')\r\n\r\n# \u7ed8\u5236\u7b2c\u4e8c\u4e2a\u7f51\u7edc\u7684\u635f\u5931\u51fd\u6570\u66f2\u7ebf\r\nplt.plot(x2, agent2_NMSE, label="Csi_CNN_Transformer_Net", marker=\'s\')\r\n\r\n# \u7ed8\u5236\u7b2c\u4e09\u4e2a\u7f51\u7edc\u7684\u635f\u5931\u51fd\u6570\u66f2\u7ebf\r\nplt.plot(x3, agent3_NMSE, label="Csi_Net", marker=\'^\')\r\n# plt.plot(x3, (np.array(agent3_NMSE) - 0.2) * 2 / 2.6 + 0.5, label="Csi_Net")\r\n# \u8bbe\u7f6e\u6807\u9898\u3001\u5750\u6807\u8f74\u6807\u7b7e\u548c\u56fe\u4f8b\r\nplt.xlabel("Epochs")\r\nplt.ylabel("NMSE")\r\nplt.legend()\r\n\r\n# \u663e\u793a\u56fe\u5f62\r\nplt.show()\r\n\n'})}),"\n",(0,s.jsxs)(n.p,{children:["\u200b",(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)(n.img,{alt:"png",src:e(12714).A+"",width:"700",height:"525"}),"\r\n\u200b"]}),"\n",(0,s.jsx)(n.h2,{id:"cs_net",children:"CS_Net"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# print("="*30)\r\n# print("Encoder: Random Projection; Decoder: CNN")\r\n# print("compressed codeword bits: {}".format(bits))\r\n# agent4 = model_trainer(epochs=40, net="CS_Net",feedbackbits=bits)\r\n# x4, agent4_NMSE= agent4.model_train()\r\n# print("CS_Net")\r\n# print(agent4_NMSE)\r\n# plt.plot(x4, agent4_NMSE, label="CS_Net")\r\n\r\n# # \u4fdd\u5b58\u6a21\u578b\r\n# model_path = f\'models/CS_Net_model_{bits}.pth\'\r\n# agent4.model_save(model_path)\n'})})]})}function p(r={}){const{wrapper:n}={...(0,o.R)(),...r.components};return n?(0,s.jsx)(n,{...r,children:(0,s.jsx)(d,{...r})}):d(r)}},38680:(r,n,e)=>{e.d(n,{A:()=>t});const t=e.p+"assets/images/main_11_1-3014337ea8968e4a00699b98a915ddd1.png"},34291:(r,n,e)=>{e.d(n,{A:()=>t});const t=e.p+"assets/images/main_11_2-c6eedac20c3cc2c9a4c6ee6d956f7164.png"},89157:(r,n,e)=>{e.d(n,{A:()=>t});const t=e.p+"assets/images/main_11_4-0889c4796fbec85f60624d0950022c21.png"},23910:(r,n,e)=>{e.d(n,{A:()=>t});const t=e.p+"assets/images/main_13_1-3014337ea8968e4a00699b98a915ddd1.png"},65149:(r,n,e)=>{e.d(n,{A:()=>t});const t="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAa4AAAGrCAYAAACYOHMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAmH0lEQVR4nO3de2zV9f3H8VcpPadA24Pl0ssoyEVBRDBDrI3KEDouSwgKf+AlGTgCgRUzqE7t4n0zZSzztlT4Yw5mIqIuotFEnFYpcSs4qgSvDZAqOGiZZO2BQk9L+/39YTi/VSied3u+nH5On4/kJPacj5/z+V7OeXF6Tl8nxfM8TwAAOKJfohcAAIAFwQUAcArBBQBwCsEFAHAKwQUAcArBBQBwCsEFAHAKwQUAcEqvCy7P8xQOh8XfRQMAzqd/ohfwfSdOnFAoFFJTU5OysrJM/+8f//hH0/j29vaYx/bv3+t2lS86OjpiHtvW1maa27K/razHxzLeum7LfunXz/Zvx7S0NNN4y3b6+Y9Fy3nlt5SUFF/Gdme8hfX4WMb7uW6r0tLSHxzj2yuuiooKXXrppUpPT1dhYaE+/PBDv+4KANCH+BJcL730kkpLS/Xwww/ro48+0pQpUzRnzhwdO3bMj7sDAPQhvgTXE088oeXLl+vOO+/UxIkTtXHjRg0cOFB/+ctf/Lg7AEAfEvfgam1tVU1NjYqLi///Tvr1U3Fxsaqrq+N9dwCAPibunzj49ttv1d7erpycnE7X5+Tk6MsvvzxnfCQSUSQSif4cDofjvSQAQBJJ+Mfhy8vLFQqFopeCgoJELwkA0IvFPbiGDh2q1NRUNTQ0dLq+oaFBubm554wvKytTU1NT9HL48OF4LwkAkETiHlyBQEBTp05VZWVl9LqOjg5VVlaqqKjonPHBYFBZWVmdLgAAdMWXv6otLS3VkiVLdM011+jaa6/VU089pebmZt15551+3B0AoA/xJbgWL16s//znP3rooYdUX1+vq6++Wtu3bz/nAxsAAFj51mO0evVqrV692q/pz6u1tdU03lK387+ffMR3rDUxftZmWetwLOeKta4oNTXVNN7CWrNlGd+b6oosa/FzbvScH/s74Z8qBADAguACADiF4AIAOIXgAgA4heACADiF4AIAOIXgAgA4heACADiF4AIAOIXgAgA4xb8OngRIS0szjbdU+Vjn7i0stVaSv1U7Vu3t7b7Nbdkv1gon6z63sO5zy3hrtZV1vIVlH/amOikrP+f3cx9a+DE3r7gAAE4huAAATiG4AABOIbgAAE4huAAATiG4AABOIbgAAE4huAAATiG4AABOIbgAAE4huAAATkmqrkI/u+3OnDljGt9bOsisLPswEomY5rZ0xElSenp6zGOtvXmW7bSuu62tLeax4XDYNLe1MzMzMzPmsf37254OLMffeq5YznHr8bFsp9/9jX53Ifol0X2PvOICADiF4AIAOIXgAgA4heACADiF4AIAOIXgAgA4heACADiF4AIAOIXgAgA4heACADglqSqfrNUiftbKWNbiZ+2LtYLGsp3W+iHrdlpqgvzch5YKJ8nfOinr+NbWVtN4C8s+T01NNc1t3U4LS32bdR3W7bSwnuO95TnIj4o6XnEBAJxCcAEAnEJwAQCcQnABAJxCcAEAnEJwAQCcQnABAJxCcAEAnEJwAQCcQnABAJxCcAEAnJJUXYVWln4ua1+dpbPM2uVl6R/sTR1k1vGWfejndvrZU+lnl6SVn114fvJzHda5rcfTMr+lA9PKz8eyH8eHV1wAAKfEPbgeeeQRpaSkdLpMmDAh3ncDAOijfPlV4ZVXXql33333/++kf5/+jSQAII58SZT+/fsrNzfXj6kBAH2cL+9x7d+/X/n5+RozZozuuOMOHTp0qMuxkUhE4XC40wUAgK7EPbgKCwu1efNmbd++XRs2bFBdXZ1uvPFGnThx4rzjy8vLFQqFopeCgoJ4LwkAkERSPJ8/09rY2KhRo0bpiSee0LJly865PRKJdPqK9nA4rIKCAjU1NSkrK8t0X+vWrTONt3y0mI/Dn8u67t70UW4Llz8O7+fHlnvLPu8tH8uX/D1X+srH4cvKyn5wjO+fmhg8eLAuv/xyHThw4Ly3B4NBBYNBv5cBAEgSvv8d18mTJ3Xw4EHl5eX5fVcAgD4g7sF1zz33qKqqSl999ZX++c9/6pZbblFqaqpuu+22eN8VAKAPivuvCr/55hvddtttOn78uIYNG6YbbrhBu3bt0rBhw+J9V+ew/p7W8rtXP3+d6ef7HJb32qxzW/8+r6sP6HQlPT3dl7HWtVjf3xwyZEjMYzMzM01znzlzxjS+ubk55rGtra2muS2PH+t5aHksW99Xsjze/Kxwkvx9L9zynpifz0F+/B1v3GfcunVrvKcEACCKrkIAgFMILgCAUwguAIBTCC4AgFMILgCAUwguAIBTCC4AgFMILgCAUwguAIBTCC4AgFN8/1qT3szSK9bS0mKa29LPZe1as/SbWTvILP1m//s9arEYMGCAabxl/kAgYJrbcjwff/xx09wbN26Meeydd95pmtvKUsH28ssvm+a29CxedtllprlDoVDMY63nuOXYW7sh09LSTOMtz0HW5wnL2q19nJZ1Wx+bseAVFwDAKQQXAMApBBcAwCkEFwDAKQQXAMApBBcAwCkEFwDAKQQXAMApBBcAwCkEFwDAKSmepbvjIgiHwwqFQmpqalJWVpbp/12/fr1Pq7JVnPitX7/Y/71hqYeyslbtWMdbWLfTsg+tdTitra0xj7WeV9bxlu20VvNYxlvriiznirWWyc/z0LqdlvGWY9mdtVj4+Xx49913/+AYXnEBAJxCcAEAnEJwAQCcQnABAJxCcAEAnEJwAQCcQnABAJxCcAEAnEJwAQCcQnABAJxCcAEAnNI/0QuIJ7973ywsPWHWdVi61qxz+7luP1nXYum387PzzTq3n52M1n3Y0tIS89iysjLT3JbeUWv3oGWf+9k9KNn2uZ8di67NzSsuAIBTCC4AgFMILgCAUwguAIBTCC4AgFMILgCAUwguAIBTCC4AgFMILgCAUwguAIBTCC4AgFOSqqvQ2uNm6dBqb283ze1nj59lbmt3mqXbrn9/2+lj3YeW45OWlmaau6mpKeaxbW1tprknTpwY89iCggLT3CdPnjSN/+qrr2Ie29jYaJrbYt26dabx1nPFL373cfaWvk8/nyesz8sx3X/cZwQAwEfm4Nq5c6fmz5+v/Px8paSk6LXXXut0u+d5euihh5SXl6cBAwaouLhY+/fvj9d6AQB9nDm4mpubNWXKFFVUVJz39vXr1+uZZ57Rxo0btXv3bg0aNEhz5swxfQUCAABdMb/HNW/ePM2bN++8t3mep6eeekoPPPCAFixYIEl6/vnnlZOTo9dee0233nprz1YLAOjz4voeV11dnerr61VcXBy9LhQKqbCwUNXV1ef9fyKRiMLhcKcLAABdiWtw1dfXS5JycnI6XZ+TkxO97fvKy8sVCoWiF+unrAAAfUvCP1VYVlampqam6OXw4cOJXhIAoBeLa3Dl5uZKkhoaGjpd39DQEL3t+4LBoLKysjpdAADoSlyDa/To0crNzVVlZWX0unA4rN27d6uoqCiedwUA6KPMnyo8efKkDhw4EP25rq5Oe/fuVXZ2tkaOHKk1a9bod7/7nS677DKNHj1aDz74oPLz83XzzTfHc90AgD7KHFx79uzRTTfdFP25tLRUkrRkyRJt3rxZ9957r5qbm7VixQo1Njbqhhtu0Pbt25Wenh6/VXfBUhEk2epWAoGAaW5LhYq13sYy3lrlYhlv3d/W6hdLjZN1LZa5MzIyTHMfOXLEl3VI0tdff20a/8UXX8Q81roPu/r1//l8/wNbP8RyHp45c8Y0t6XCy/r4sfJzfsvzm/XYW1jqoWJlDq4ZM2ZccIekpKToscce02OPPdajhQEAcD4J/1QhAAAWBBcAwCkEFwDAKQQXAMApBBcAwCkEFwDAKQQXAMApBBcAwCkEFwDAKQQXAMAp5sqn3szaiWXp57J2eVk6yKzrtnT+WbvQ/Owss2ptbfVlrGTvTbQYOHBgzGOPHz9umjsYDJrGX3nllTGPtXZmWs6V06dPm+a29A9a+x4tjzfreWJ9/FjGW7oHJdtj37oPLWvxo4+RV1wAAKcQXAAApxBcAACnEFwAAKcQXAAApxBcAACnEFwAAKcQXAAApxBcAACnEFwAAKckVeVTW1ubabyl+sVaW2Kpz7HObalbsdbEWPaJnxVbVoFAwDTesnbrPrTUG/ldyWU5D3vTuWI5nn5UCp1lfU7pTdVwfu4X67kSb7ziAgA4heACADiF4AIAOIXgAgA4heACADiF4AIAOIXgAgA4heACADiF4AIAOIXgAgA4heACADglqboK+/e3bY6ffVvWXjGL1NTUmMdat9HSb2bp5OvOWgYMGBDzWEsnnySdOXPGNL63zG059pJtn1vPWcvjzdqbZ+kItB57C+u6rcfHwroWP5+DLGvx43mWV1wAAKcQXAAApxBcAACnEFwAAKcQXAAApxBcAACnEFwAAKcQXAAApxBcAACnEFwAAKckVeWTpSZG8rc6yVK30tHRYZrbup0Wlhofa6WMtZqnpaUl5rHWui+LIUOG+DZ3dXW1afzhw4dN46dMmRLz2J/85CemuYPBYMxj6+vrTXMfOnQo5rGRSMQ0t6VKLBAImOa2nuOWx7K1TspyfKyPH0utmeVxHCtecQEAnEJwAQCcYg6unTt3av78+crPz1dKSopee+21TrcvXbpUKSkpnS5z586N13oBAH2cObiam5s1ZcoUVVRUdDlm7ty5Onr0aPTy4osv9miRAACcZX5He968eZo3b94FxwSDQeXm5nZ7UQAAdMWX97h27Nih4cOHa/z48Vq1apWOHz/e5dhIJKJwONzpAgBAV+IeXHPnztXzzz+vyspK/f73v1dVVZXmzZvX5cdEy8vLFQqFopeCgoJ4LwkAkETi/scvt956a/S/r7rqKk2ePFljx47Vjh07NGvWrHPGl5WVqbS0NPpzOBwmvAAAXfL94/BjxozR0KFDdeDAgfPeHgwGlZWV1ekCAEBXfA+ub775RsePH1deXp7fdwUA6APMvyo8efJkp1dPdXV12rt3r7Kzs5Wdna1HH31UixYtUm5urg4ePKh7771X48aN05w5c+K6cABA32QOrj179uimm26K/nz2/aklS5Zow4YN2rdvn/7617+qsbFR+fn5mj17tn7729+aerO6y9rllZKSEvNYa1ehpbPMzx5EK0sHmWWsZF+3pT/N2vdoWfuRI0dMc1uMGTPGNH7ChAmm8ZZ9/sUXX5jmtu5zC8uxtz7uLfvE2j1o5WfHZmtrqy9jrfx4vjLvtRkzZlzwifbtt9/u0YIAALgQugoBAE4huAAATiG4AABOIbgAAE4huAAATiG4AABOIbgAAE4huAAATiG4AABOIbgAAE7xrygrAay9YpaOM+vcfvYgWsb3lj45yd5ZZlm7dTv97Hv0c27reehnZ6afekuPaG/i5zlu2d9WfpxXvOICADiF4AIAOIXgAgA4heACADiF4AIAOIXgAgA4heACADiF4AIAOIXgAgA4heACADglqSqfrBVEFmlpaabxrlWodIefFU5WwWDQNN6yduu6T58+7dvc1vPQMt7Pc9a6nZa1WNft53noJ+t29pbaOT/OK15xAQCcQnABAJxCcAEAnEJwAQCcQnABAJxCcAEAnEJwAQCcQnABAJxCcAEAnEJwAQCcQnABAJySVF2FZ86cMY1PTU2NeWxra6tpbksXnp8dce3t7b7NHQgETOP97IizHp9IJBLz2JaWFtPc48aNi3nsxIkTTXM3Njaaxn/44Ycxj62vrzfNPXDgQF/GSlJ6erppvF96Ux+nqyzPs7HiFRcAwCkEFwDAKQQXAMApBBcAwCkEFwDAKQQXAMApBBcAwCkEFwDAKQQXAMApBBcAwClJVfnkJz9qS87yPM803s9aGUtF1OnTp01zW+tzLFVY1vqpzMxMX9YhSZ988knMY2tqakxzDxo0yDQ+IyMj5rEjRowwzW1hPfZ+Pt4s57iflWmS7dxKS0vzbR1tbW2+ze3HPuQVFwDAKabgKi8v17Rp05SZmanhw4fr5ptvVm1tbacxLS0tKikp0ZAhQ5SRkaFFixapoaEhrosGAPRdpuCqqqpSSUmJdu3apXfeeUdtbW2aPXu2mpubo2PWrl2rN954Q6+88oqqqqp05MgRLVy4MO4LBwD0Tab3uLZv397p582bN2v48OGqqanR9OnT1dTUpOeee05btmzRzJkzJUmbNm3SFVdcoV27dum6666L38oBAH1Sj97jampqkiRlZ2dL+u6N5ra2NhUXF0fHTJgwQSNHjlR1dXVP7goAAEk9+FRhR0eH1qxZo+uvv16TJk2S9N0X0QUCAQ0ePLjT2JycnC6/pC4SiXT6Ur9wONzdJQEA+oBuv+IqKSnRp59+qq1bt/ZoAeXl5QqFQtFLQUFBj+YDACS3bgXX6tWr9eabb+r999/v9Lcfubm5am1tPefrxRsaGpSbm3veucrKytTU1BS9HD58uDtLAgD0Eabg8jxPq1ev1rZt2/Tee+9p9OjRnW6fOnWq0tLSVFlZGb2utrZWhw4dUlFR0XnnDAaDysrK6nQBAKArpve4SkpKtGXLFr3++uvKzMyMvm8VCoU0YMAAhUIhLVu2TKWlpcrOzlZWVpbuuusuFRUV8YlCAEBcmIJrw4YNkqQZM2Z0un7Tpk1aunSpJOnJJ59Uv379tGjRIkUiEc2ZM0fPPvtsXBYLAIApuGLp1EtPT1dFRYUqKiq6vajusnbKWToCe1OfoLX3rbfM7WfvW2trq2m8pQvP2oM4ZMiQmMdazxNrh5/lMWE99pa1W7fTz8ePhZ/728r6+HF1H8aCrkIAgFMILgCAUwguAIBTCC4AgFMILgCAUwguAIBTCC4AgFMILgCAUwguAIBTCC4AgFO6/UWSycBScXPmzBkfV2JjqXKx1sRYxlurkPr3t51ulrVYK7mCwWDMY9va2kxzW74M1Xp8MjIyTOMHDBgQ81hr5ZOl3qg3Vab5qTdVPllYj71lvPXYx3T/cZ8RAAAfEVwAAKcQXAAApxBcAACnEFwAAKcQXAAApxBcAACnEFwAAKcQXAAApxBcAACnEFwAAKckVVehtSfM0qFl6XyTbF1r1l42S3deWlqaaW5LF56l70+SWlpaTOMt3YbWY2/Zh6mpqaa5rX2CFmPGjDGNHzRoUMxjjx07Zpr722+/jXmstWfP2mvpF+t5ZV23n32Pln1ufQ6yrNvagxjTnHGfEQAAHxFcAACnEFwAAKcQXAAApxBcAACnEFwAAKcQXAAApxBcAACnEFwAAKcQXAAAp/SOXpUE8bNuxc9KFEuNk3XuM2fO+DJWsu9DC2s1j2W8de5LLrkk5rHWfdjQ0GAab1m7tdoqMzMz5rHWyidrBZFfrMfe+nizzG+tWLOcW34+Nql8AgD0eQQXAMApBBcAwCkEFwDAKQQXAMApBBcAwCkEFwDAKQQXAMApBBcAwCkEFwDAKQQXAMApSdVVaO1as/RzWTvl/OzC6y0di37OLdk6zqxz+9nN1tra2ivWYZ3fz7VYuwetx7O3zG3dh5b94ufcvemxGdP9x31GAAB8ZAqu8vJyTZs2TZmZmRo+fLhuvvlm1dbWdhozY8YMpaSkdLqsXLkyrosGAPRdpuCqqqpSSUmJdu3apXfeeUdtbW2aPXu2mpubO41bvny5jh49Gr2sX78+rosGAPRdpve4tm/f3unnzZs3a/jw4aqpqdH06dOj1w8cOFC5ubnxWSEAAP+jR+9xNTU1SZKys7M7Xf/CCy9o6NChmjRpksrKynTq1Kku54hEIgqHw50uAAB0pdufKuzo6NCaNWt0/fXXa9KkSdHrb7/9do0aNUr5+fnat2+f7rvvPtXW1urVV1897zzl5eV69NFHu7sMAEAf0+3gKikp0aeffqoPPvig0/UrVqyI/vdVV12lvLw8zZo1SwcPHtTYsWPPmaesrEylpaXRn8PhsAoKCrq7LABAkutWcK1evVpvvvmmdu7cqREjRlxwbGFhoSTpwIED5w2uYDCoYDDYnWUAAPogU3B5nqe77rpL27Zt044dOzR69Ogf/H/27t0rScrLy+vWAgEA+F+m4CopKdGWLVv0+uuvKzMzU/X19ZKkUCikAQMG6ODBg9qyZYt+9rOfaciQIdq3b5/Wrl2r6dOna/Lkyb5sAACgbzEF14YNGyR990fG/2vTpk1aunSpAoGA3n33XT311FNqbm5WQUGBFi1apAceeCBuCwYA9G3mXxVeSEFBgaqqqnq0oJ7ws2vNz34zq/79Yz9s1n1iGe9nx6KVpR/QyvoerJ99j9bOTGtHoEV7e3vMY63baekdtc7tZ8+eZZ9YWY+ln88TFn6cg3QVAgCcQnABAJxCcAEAnEJwAQCcQnABAJxCcAEAnEJwAQCcQnABAJxCcAEAnEJwAQCc0u3v4+qNrNUiflYQtbW1xTzWz9ofP2uZrHNbanwk2/FMS0szzZ2RkRHz2GPHjpnmrqysjHlsJBIxzX3NNdeYxl999dUxj7XWSV3om82/r7m52TS35dyyVBtJtsem9TnFz1om6+PHejxdwisuAIBTCC4AgFMILgCAUwguAIBTCC4AgFMILgCAUwguAIBTCC4AgFMILgCAUwguAIBTCC4AgFOSqqvQ2llm6RWz9glau/Ms+vWL/d8b1nX72Z1m3SeWHr+TJ0+a5m5sbIx57KBBg0xz33bbbabxFk1NTabx//73v2Mea338BAKBmMcOGDDANLelZ8/SPWhl3SfW/k7Lc5C1e9DyPGEZK9m20/ocFAtecQEAnEJwAQCcQnABAJxCcAEAnEJwAQCcQnABAJxCcAEAnEJwAQCcQnABAJxCcAEAnJJUlU/WShQ/a0usFSoWflZVWWqWrPU2lrmtrHVSlroq6z601En5eZ5IUjAYjHmsdTstVUt+VgpZWbbT8ljrDj/qkLozt3Udfj4HxYJXXAAApxBcAACnEFwAAKcQXAAApxBcAACnEFwAAKcQXAAApxBcAACnEFwAAKcQXAAApxBcAACnJFVXobWvrr29Peaxlm47v1l63yx9cpK/HXEtLS2m8ZmZmTGP7d/fdir/97//jXmstWcvFArFPNZ6zjY3N5vGnzx5Muax1u1MT0+Peaz1vPKzC8/Pc9zabWgZb1235dyyPn4s+9zaIRsLXnEBAJxiCq4NGzZo8uTJysrKUlZWloqKivTWW29Fb29paVFJSYmGDBmijIwMLVq0SA0NDXFfNACg7zIF14gRI7Ru3TrV1NRoz549mjlzphYsWKDPPvtMkrR27Vq98cYbeuWVV1RVVaUjR45o4cKFviwcANA3mX6xOX/+/E4/P/7449qwYYN27dqlESNG6LnnntOWLVs0c+ZMSdKmTZt0xRVXaNeuXbruuuvit2oAQJ/V7fe42tvbtXXrVjU3N6uoqEg1NTVqa2tTcXFxdMyECRM0cuRIVVdXdzlPJBJROBzudAEAoCvm4Prkk0+UkZGhYDColStXatu2bZo4caLq6+sVCAQ0ePDgTuNzcnJUX1/f5Xzl5eUKhULRS0FBgXkjAAB9hzm4xo8fr71792r37t1atWqVlixZos8//7zbCygrK1NTU1P0cvjw4W7PBQBIfua/4woEAho3bpwkaerUqfrXv/6lp59+WosXL1Zra6saGxs7vepqaGhQbm5ul/MFg0EFg0H7ygEAfVKP/46ro6NDkUhEU6dOVVpamiorK6O31dbW6tChQyoqKurp3QAAIMn4iqusrEzz5s3TyJEjdeLECW3ZskU7duzQ22+/rVAopGXLlqm0tFTZ2dnKysrSXXfdpaKiIj5RCACIG1NwHTt2TD//+c919OhRhUIhTZ48WW+//bZ++tOfSpKefPJJ9evXT4sWLVIkEtGcOXP07LPP+rLw82ltbTWNt9Q4nT592jS3tcrHwlLNY63DsVTQWGuwrJVCluMZiURMc1vqc6xzT5s2Leaxl19+uWlu63vAW7dujXms9b3q7OzsmMde6O2C8xk6dGjMY63nuKV6zFILJ9mrkyyPCevclrVbz3E/q6piYdoTzz333AVvT09PV0VFhSoqKnq0KAAAukJXIQDAKQQXAMApBBcAwCkEFwDAKQQXAMApBBcAwCkEFwDAKQQXAMApBBcAwCkEFwDAKSmetejLZ+FwWKFQSE1NTcrKykr0cgAAvUyvCy7P83TixAllZmb6Us4IAHBbrwsuAAAuhPe4AABOIbgAAE4huAAATiG4AABOIbgAAE4huAAATiG4AABOIbgAAE4huAAATiG4AABO6Z/oBcTL2Y5DAIDbfqirNmmC68SJEwqFQoleBgCgh37o20GSpmS3q1dc4XBYBQUFOnz4cFJ/TQrbmTz6wjZKbGeyied29plXXCkpKRfcWVlZWUl90pzFdiaPvrCNEtuZbC7GdvLhDACAUwguAIBTkj64gsGgHn74YQWDwUQvxVdsZ/LoC9sosZ3J5mJuZ9J8OAMA0Dck/SsuAEByIbgAAE4huAAATiG4AABOSfrgqqio0KWXXqr09HQVFhbqww8/TPSS4uaRRx5RSkpKp8uECRMSvawe27lzp+bPn6/8/HylpKTotdde63S753l66KGHlJeXpwEDBqi4uFj79+9PzGJ74Ie2c+nSpecc37lz5yZmsd1UXl6uadOmKTMzU8OHD9fNN9+s2traTmNaWlpUUlKiIUOGKCMjQ4sWLVJDQ0OCVtw9sWznjBkzzjmeK1euTNCKu2fDhg2aPHly9I+Mi4qK9NZbb0Vvv1jHMqmD66WXXlJpaakefvhhffTRR5oyZYrmzJmjY8eOJXppcXPllVfq6NGj0csHH3yQ6CX1WHNzs6ZMmaKKiorz3r5+/Xo988wz2rhxo3bv3q1BgwZpzpw5amlpucgr7Zkf2k5Jmjt3bqfj++KLL17EFfZcVVWVSkpKtGvXLr3zzjtqa2vT7Nmz1dzcHB2zdu1avfHGG3rllVdUVVWlI0eOaOHChQlctV0s2ylJy5cv73Q8169fn6AVd8+IESO0bt061dTUaM+ePZo5c6YWLFigzz77TNJFPJZeErv22mu9kpKS6M/t7e1efn6+V15ensBVxc/DDz/sTZkyJdHL8JUkb9u2bdGfOzo6vNzcXO8Pf/hD9LrGxkYvGAx6L774YgJWGB/f307P87wlS5Z4CxYsSMh6/HLs2DFPkldVVeV53nfHLi0tzXvllVeiY7744gtPklddXZ2oZfbY97fT8zzvJz/5iferX/0qcYvyySWXXOL9+c9/vqjHMmlfcbW2tqqmpkbFxcXR6/r166fi4mJVV1cncGXxtX//fuXn52vMmDG64447dOjQoUQvyVd1dXWqr6/vdFxDoZAKCwuT6rietWPHDg0fPlzjx4/XqlWrdPz48UQvqUeampokSdnZ2ZKkmpoatbW1dTqeEyZM0MiRI50+nt/fzrNeeOEFDR06VJMmTVJZWZlOnTqViOXFRXt7u7Zu3arm5mYVFRVd1GOZNCW73/ftt9+qvb1dOTk5na7PycnRl19+maBVxVdhYaE2b96s8ePH6+jRo3r00Ud144036tNPP1VmZmail+eL+vp6STrvcT17W7KYO3euFi5cqNGjR+vgwYP6zW9+o3nz5qm6ulqpqamJXp5ZR0eH1qxZo+uvv16TJk2S9N3xDAQCGjx4cKexLh/P822nJN1+++0aNWqU8vPztW/fPt13332qra3Vq6++msDV2n3yyScqKipSS0uLMjIytG3bNk2cOFF79+69aMcyaYOrL5g3b170vydPnqzCwkKNGjVKL7/8spYtW5bAlSEebr311uh/X3XVVZo8ebLGjh2rHTt2aNasWQlcWfeUlJTo008/TYr3YS+kq+1csWJF9L+vuuoq5eXladasWTp48KDGjh17sZfZbePHj9fevXvV1NSkv/3tb1qyZImqqqou6hqS9leFQ4cOVWpq6jmfaGloaFBubm6CVuWvwYMH6/LLL9eBAwcSvRTfnD12fem4njVmzBgNHTrUyeO7evVqvfnmm3r//fc1YsSI6PW5ublqbW1VY2Njp/GuHs+utvN8CgsLJcm54xkIBDRu3DhNnTpV5eXlmjJlip5++umLeiyTNrgCgYCmTp2qysrK6HUdHR2qrKxUUVFRAlfmn5MnT+rgwYPKy8tL9FJ8M3r0aOXm5nY6ruFwWLt3707a43rWN998o+PHjzt1fD3P0+rVq7Vt2za99957Gj16dKfbp06dqrS0tE7Hs7a2VocOHXLqeP7Qdp7P3r17Jcmp43k+HR0dikQiF/dYxvWjHr3M1q1bvWAw6G3evNn7/PPPvRUrVniDBw/26uvrE720uLj77ru9HTt2eHV1dd4//vEPr7i42Bs6dKh37NixRC+tR06cOOF9/PHH3scff+xJ8p544gnv448/9r7++mvP8zxv3bp13uDBg73XX3/d27dvn7dgwQJv9OjR3unTpxO8cpsLbeeJEye8e+65x6uurvbq6uq8d9991/vxj3/sXXbZZV5LS0uilx6zVatWeaFQyNuxY4d39OjR6OXUqVPRMStXrvRGjhzpvffee96ePXu8oqIir6ioKIGrtvuh7Txw4ID32GOPeXv27PHq6uq8119/3RszZow3ffr0BK/c5v777/eqqqq8uro6b9++fd7999/vpaSkeH//+989z7t4xzKpg8vzPO9Pf/qTN3LkSC8QCHjXXnutt2vXrkQvKW4WL17s5eXleYFAwPvRj37kLV682Dtw4ECil9Vj77//vifpnMuSJUs8z/vuI/EPPvigl5OT4wWDQW/WrFlebW1tYhfdDRfazlOnTnmzZ8/2hg0b5qWlpXmjRo3yli9f7tw/us63fZK8TZs2RcecPn3a++Uvf+ldcskl3sCBA71bbrnFO3r0aOIW3Q0/tJ2HDh3ypk+f7mVnZ3vBYNAbN26c9+tf/9prampK7MKNfvGLX3ijRo3yAoGAN2zYMG/WrFnR0PK8i3cs+VoTAIBTkvY9LgBAciK4AABOIbgAAE4huAAATiG4AABOIbgAAE4huAAATiG4AABOIbgAAE4huAAATiG4AABOIbgAAE75P0XyZEUfd9GWAAAAAElFTkSuQmCC"},89931:(r,n,e)=>{e.d(n,{A:()=>t});const t=e.p+"assets/images/main_13_4-5399f794d5d7c1b9c630897fad7d5c35.png"},63844:(r,n,e)=>{e.d(n,{A:()=>t});const t=e.p+"assets/images/main_15_1-3014337ea8968e4a00699b98a915ddd1.png"},93759:(r,n,e)=>{e.d(n,{A:()=>t});const t=e.p+"assets/images/main_15_2-4958dd6c675697e763dc0d106e1c7ea0.png"},89929:(r,n,e)=>{e.d(n,{A:()=>t});const t=e.p+"assets/images/main_15_4-e2c3b2e15b67743de49c5fb4422461a8.png"},12714:(r,n,e)=>{e.d(n,{A:()=>t});const t=e.p+"assets/images/main_16_0-af52807ae412e19a41fcf46baeff2e68.png"},28453:(r,n,e)=>{e.d(n,{R:()=>i,x:()=>a});var t=e(96540);const s={},o=t.createContext(s);function i(r){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof r?r(n):{...n,...r}}),[n,r])}function a(r){let n;return n=r.disableParentContext?"function"==typeof r.components?r.components(s):r.components||s:i(r.components),t.createElement(o.Provider,{value:n},r.children)}}}]);